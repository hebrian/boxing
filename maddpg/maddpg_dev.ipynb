{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.atari import boxing_v2\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from supersuit import pad_observations_v0, pad_action_space_v0, resize_v1, normalize_obs_v0, frame_skip_v0, dtype_v0\n",
    "from pettingzoo.utils import aec_to_parallel\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic and Actor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, actions):\n",
    "        # Ensure observations are flattened\n",
    "        obs = obs.flatten(start_dim=1) if len(obs.shape) > 2 else obs\n",
    "        actions = actions.flatten(start_dim=1) if len(actions.shape) > 2 else actions\n",
    "\n",
    "        # Debugging shapes\n",
    "        #print(f\"obs shape: {obs.shape}, actions shape: {actions.shape}\")\n",
    "\n",
    "        x = torch.cat([obs, actions], dim=-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.fc(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(self, obs_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3):\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "        self.critic = Critic(obs_dim * 2, action_dim * 2)\n",
    "        self.actor_target = Actor(obs_dim, action_dim)\n",
    "        self.critic_target = Critic(obs_dim * 2, action_dim * 2)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.update_target(1.0)\n",
    "\n",
    "    def update_target(self, tau):\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, obs_dim, action_dim, n_agents, gamma=0.95, tau=0.01):\n",
    "        self.agents = [MADDPGAgent(obs_dim, action_dim) for _ in range(n_agents)]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def update(self, replay_buffer, batch_size):\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            obs, actions, rewards, next_obs, dones = replay_buffer.sample(batch_size)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).flatten(start_dim=2)  # Flatten observations\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).flatten(start_dim=2)  # Flatten actions\n",
    "\n",
    "            rewards = torch.tensor(rewards[:, agent_idx], dtype=torch.float32).unsqueeze(-1)\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32).flatten(start_dim=2)\n",
    "            dones = torch.tensor(dones[:, agent_idx], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "            # Critic update\n",
    "            with torch.no_grad():\n",
    "                next_actions = torch.cat([\n",
    "                    ag.actor_target(next_obs[:, i]) for i, ag in enumerate(self.agents)\n",
    "                ], dim=-1)\n",
    "                target_q = rewards + self.gamma * (1 - dones) * agent.critic_target(next_obs, next_actions)\n",
    "\n",
    "            q_value = agent.critic(obs, actions)\n",
    "            critic_loss = nn.MSELoss()(q_value, target_q)\n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            agent.critic_optimizer.step()\n",
    "            value_loss += critic_loss.item()\n",
    "\n",
    "            # Actor update\n",
    "            predicted_actions = torch.cat([\n",
    "                agent.actor(obs[:, i]) if i == agent_idx else actions[:, i] for i, _ in enumerate(self.agents)\n",
    "            ], dim=-1)\n",
    "            actor_loss = -agent.critic(obs, predicted_actions).mean()\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            agent.actor_optimizer.step()\n",
    "            policy_loss += actor_loss.item()\n",
    "\n",
    "            # Update targets\n",
    "            agent.update_target(self.tau)\n",
    "\n",
    "        return policy_loss / len(self.agents), value_loss / len(self.agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, action_dim, n_agents):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.obs = np.zeros((max_size, n_agents, obs_dim))\n",
    "        self.actions = np.zeros((max_size, n_agents, action_dim))\n",
    "        self.rewards = np.zeros((max_size, n_agents))\n",
    "        self.next_obs = np.zeros((max_size, n_agents, obs_dim))\n",
    "        self.dones = np.zeros((max_size, n_agents))\n",
    "\n",
    "    def store(self, obs, actions, rewards, next_obs, dones):\n",
    "        self.obs[self.ptr] = obs\n",
    "        self.actions[self.ptr] = actions\n",
    "        self.rewards[self.ptr] = rewards\n",
    "        self.next_obs[self.ptr] = next_obs\n",
    "        self.dones[self.ptr] = dones\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.obs[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.next_obs[idx],\n",
    "            self.dones[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = boxing_v2.env(render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "env = resize_v1(env, 84, 84)\n",
    "env = dtype_v0(env, dtype=\"float32\")\n",
    "env = normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "parallel_env = aec_to_parallel(env)\n",
    "\n",
    "obs_dim = 84 * 84\n",
    "action_dim = parallel_env.action_space(\"first_0\").n\n",
    "n_agents = 2\n",
    "maddpg = MADDPG(obs_dim, action_dim, n_agents)\n",
    "replay_buffer = ReplayBuffer(100000, obs_dim, action_dim, n_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 completed with total reward: 0.0\n",
      "Episode 2/10 completed with total reward: 0.0\n",
      "Episode 3/10 completed with total reward: 0.0\n",
      "Episode 4/10 completed with total reward: 0.0\n",
      "Episode 5/10 completed with total reward: 0.0\n",
      "Episode 6/10 completed with total reward: 0.0\n",
      "Episode 7/10 completed with total reward: 0.0\n",
      "Episode 8/10 completed with total reward: 0.0\n",
      "Episode 9/10 completed with total reward: 0.0\n",
      "Episode 10/10 completed with total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 10\n",
    "batch_size = 64\n",
    "\n",
    "max_steps_per_episode = 100  # Set the maximum steps per episode\n",
    "for episode in range(num_episodes):\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "\n",
    "            # Convert continuous action to discrete action\n",
    "            discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            #print(f\"Policy Loss: {policy_loss}, Value Loss: {value_loss}\")\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Print cumulative reward for the episode\n",
    "    total_reward = sum(episode_reward.values())\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed with total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs138_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
