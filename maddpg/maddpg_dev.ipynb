{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.atari import boxing_v2\n",
    "import numpy as np\n",
    "import random \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from supersuit import pad_observations_v0, pad_action_space_v0, resize_v1, normalize_obs_v0, frame_skip_v0, dtype_v0\n",
    "from pettingzoo.utils import aec_to_parallel\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic and Actor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, actions):\n",
    "        # Ensure observations are flattened\n",
    "        obs = obs.flatten(start_dim=1) if len(obs.shape) > 2 else obs\n",
    "        actions = actions.flatten(start_dim=1) if len(actions.shape) > 2 else actions\n",
    "\n",
    "        # Debugging shapes\n",
    "        #print(f\"obs shape: {obs.shape}, actions shape: {actions.shape}\")\n",
    "\n",
    "        x = torch.cat([obs, actions], dim=-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.fc(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(self, obs_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3):\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "        self.critic = Critic(obs_dim * 2, action_dim * 2)\n",
    "        self.actor_target = Actor(obs_dim, action_dim)\n",
    "        self.critic_target = Critic(obs_dim * 2, action_dim * 2)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.update_target(1.0)\n",
    "\n",
    "    def update_target(self, tau):\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MADDPG Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, obs_dim, action_dim, n_agents, gamma=0.95, tau=0.01):\n",
    "        self.agents = [MADDPGAgent(obs_dim, action_dim) for _ in range(n_agents)]\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def update(self, replay_buffer, batch_size):\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            obs, actions, rewards, next_obs, dones = replay_buffer.sample(batch_size)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).flatten(start_dim=2)  # Flatten observations\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).flatten(start_dim=2)  # Flatten actions\n",
    "\n",
    "            rewards = torch.tensor(rewards[:, agent_idx], dtype=torch.float32).unsqueeze(-1)\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32).flatten(start_dim=2)\n",
    "            dones = torch.tensor(dones[:, agent_idx], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "            # Critic update\n",
    "            with torch.no_grad():\n",
    "                next_actions = torch.cat([\n",
    "                    ag.actor_target(next_obs[:, i]) for i, ag in enumerate(self.agents)\n",
    "                ], dim=-1)\n",
    "                target_q = rewards + self.gamma * (1 - dones) * agent.critic_target(next_obs, next_actions)\n",
    "\n",
    "            q_value = agent.critic(obs, actions)\n",
    "            critic_loss = nn.MSELoss()(q_value, target_q)\n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            agent.critic_optimizer.step()\n",
    "            value_loss += critic_loss.item()\n",
    "\n",
    "            # Actor update\n",
    "            predicted_actions = torch.cat([\n",
    "                agent.actor(obs[:, i]) if i == agent_idx else actions[:, i] for i, _ in enumerate(self.agents)\n",
    "            ], dim=-1)\n",
    "            actor_loss = -agent.critic(obs, predicted_actions).mean()\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            agent.actor_optimizer.step()\n",
    "            policy_loss += actor_loss.item()\n",
    "\n",
    "            # Update targets\n",
    "            agent.update_target(self.tau)\n",
    "\n",
    "        return policy_loss / len(self.agents), value_loss / len(self.agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, action_dim, n_agents):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.obs = np.zeros((max_size, n_agents, obs_dim))\n",
    "        self.actions = np.zeros((max_size, n_agents, action_dim))\n",
    "        self.rewards = np.zeros((max_size, n_agents))\n",
    "        self.next_obs = np.zeros((max_size, n_agents, obs_dim))\n",
    "        self.dones = np.zeros((max_size, n_agents))\n",
    "\n",
    "    def store(self, obs, actions, rewards, next_obs, dones):\n",
    "        self.obs[self.ptr] = obs\n",
    "        self.actions[self.ptr] = actions\n",
    "        self.rewards[self.ptr] = rewards\n",
    "        self.next_obs[self.ptr] = next_obs\n",
    "        self.dones[self.ptr] = dones\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.obs[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.next_obs[idx],\n",
    "            self.dones[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = boxing_v2.env(render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "env = resize_v1(env, 84, 84)\n",
    "env = dtype_v0(env, dtype=\"float32\")\n",
    "env = normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "parallel_env = aec_to_parallel(env)\n",
    "\n",
    "obs_dim = 84 * 84\n",
    "action_dim = parallel_env.action_space(\"first_0\").n\n",
    "n_agents = 2\n",
    "maddpg = MADDPG(obs_dim, action_dim, n_agents)\n",
    "replay_buffer = ReplayBuffer(100000, obs_dim, action_dim, n_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Loss: -0.17359518678858876, Value Loss: 5.859720113221556e-05\n",
      "Policy Loss: -0.1754363370127976, Value Loss: 0.000234179893595865\n",
      "Policy Loss: -0.17963051283732057, Value Loss: 0.00025242687479476444\n",
      "Policy Loss: -0.1724323844537139, Value Loss: 7.117877248674631e-05\n",
      "Policy Loss: -0.16031982749700546, Value Loss: 7.12306318746414e-05\n",
      "Policy Loss: -0.16267995815724134, Value Loss: 0.00018309837469132617\n",
      "Policy Loss: -0.17731905914843082, Value Loss: 0.0001348505902569741\n",
      "Policy Loss: -0.18409174028784037, Value Loss: 2.7931535896641435e-05\n",
      "Policy Loss: -0.18013833835721016, Value Loss: 7.954859393066727e-05\n",
      "Policy Loss: -0.17830464569851756, Value Loss: 0.000130353873828426\n",
      "Policy Loss: -0.17940067034214735, Value Loss: 5.299672739056405e-05\n",
      "Policy Loss: -0.1751910988241434, Value Loss: 2.4324501282535493e-05\n",
      "Policy Loss: -0.1706725051626563, Value Loss: 7.271702816069592e-05\n",
      "Policy Loss: -0.17681840062141418, Value Loss: 7.935226494737435e-05\n",
      "Policy Loss: -0.18684775102883577, Value Loss: 1.0441269296279643e-05\n",
      "Policy Loss: -0.18779736896976829, Value Loss: 3.5224435123382136e-05\n",
      "Policy Loss: -0.18265760969370604, Value Loss: 5.4412814733950654e-05\n",
      "Policy Loss: -0.18167371209710836, Value Loss: 3.868376188620459e-05\n",
      "Policy Loss: -0.18277713656425476, Value Loss: 2.211844332578039e-06\n",
      "Policy Loss: -0.18105473555624485, Value Loss: 4.067328882229049e-05\n",
      "Policy Loss: -0.18185256514698267, Value Loss: 2.9428210012838463e-05\n",
      "Policy Loss: -0.18860004656016827, Value Loss: 1.4503794545817073e-05\n",
      "Policy Loss: -0.19257866544649005, Value Loss: 9.676141587533493e-06\n",
      "Policy Loss: -0.1889899712987244, Value Loss: 3.350537645019358e-05\n",
      "Policy Loss: -0.18609563913196325, Value Loss: 1.1733549854398007e-05\n",
      "Policy Loss: -0.18777021951973438, Value Loss: 6.740381820691255e-06\n",
      "Policy Loss: -0.18838346004486084, Value Loss: 1.7298862303505302e-05\n",
      "Policy Loss: -0.18851047102361917, Value Loss: 1.5010931747383438e-05\n",
      "Policy Loss: -0.19226406514644623, Value Loss: 5.28605750105271e-06\n",
      "Policy Loss: -0.1959184156730771, Value Loss: 6.034095918039384e-06\n",
      "Policy Loss: -0.19397448003292084, Value Loss: 1.6591683106526034e-05\n",
      "Policy Loss: -0.19130275771021843, Value Loss: 3.2874020092776846e-06\n",
      "Policy Loss: -0.1926412656903267, Value Loss: 5.135389415045211e-06\n",
      "Policy Loss: -0.1941965389996767, Value Loss: 8.360636229554075e-06\n",
      "Policy Loss: -0.19460878521203995, Value Loss: 6.374835720635019e-06\n",
      "Policy Loss: -0.1968632945790887, Value Loss: 1.37125465116128e-06\n",
      "Policy Loss: -0.19909811206161976, Value Loss: 4.924764596125897e-06\n",
      "Policy Loss: -0.1978757455945015, Value Loss: 6.44689089313033e-06\n",
      "Policy Loss: -0.1961080627515912, Value Loss: 5.623574423907485e-07\n",
      "Policy Loss: -0.19728044420480728, Value Loss: 3.1449095558855333e-06\n",
      "Policy Loss: -0.19889377802610397, Value Loss: 4.2477610691094014e-06\n",
      "Policy Loss: -0.19969422928988934, Value Loss: 1.8066930351778865e-06\n",
      "Policy Loss: -0.20109091512858868, Value Loss: 1.0470066342804785e-06\n",
      "Policy Loss: -0.202230135910213, Value Loss: 3.3313169183202263e-06\n",
      "Policy Loss: -0.20086510572582483, Value Loss: 2.75917909675627e-06\n",
      "Policy Loss: -0.1997220702469349, Value Loss: 2.0578274018134834e-07\n",
      "Policy Loss: -0.2013055458664894, Value Loss: 2.8879384217361803e-06\n",
      "Policy Loss: -0.2021949589252472, Value Loss: 1.9870587948389584e-06\n",
      "Policy Loss: -0.20232722535729408, Value Loss: 4.987885660057145e-07\n",
      "Policy Loss: -0.20336539391428232, Value Loss: 1.9301769498270005e-06\n",
      "Policy Loss: -0.20365384221076965, Value Loss: 1.355433248306781e-06\n",
      "Policy Loss: -0.20233931858092546, Value Loss: 7.06749744949775e-07\n",
      "Policy Loss: -0.20248864218592644, Value Loss: 7.521977352098475e-07\n",
      "Policy Loss: -0.20374526735395193, Value Loss: 1.1675110442865844e-06\n",
      "Policy Loss: -0.2041379176080227, Value Loss: 7.165297120081959e-07\n",
      "Policy Loss: -0.20436040684580803, Value Loss: 2.886284775627246e-07\n",
      "Policy Loss: -0.20511514693498611, Value Loss: 1.1688572953971743e-06\n",
      "Policy Loss: -0.204721013084054, Value Loss: 5.779870377864427e-07\n",
      "Policy Loss: -0.20396067388355732, Value Loss: 2.6742748104879865e-07\n",
      "Policy Loss: -0.20480606611818075, Value Loss: 9.453154916627682e-07\n",
      "Policy Loss: -0.20547439996153116, Value Loss: 4.753043754135433e-07\n",
      "Policy Loss: -0.20561696775257587, Value Loss: 1.891548819799027e-07\n",
      "Policy Loss: -0.20592224597930908, Value Loss: 5.03091996506555e-07\n",
      "Policy Loss: -0.2061610147356987, Value Loss: 4.0996815187099855e-07\n",
      "Policy Loss: -0.20557312108576298, Value Loss: 2.1409207917599815e-07\n",
      "Policy Loss: -0.2056078165769577, Value Loss: 3.02924064499166e-07\n",
      "Policy Loss: -0.20636110100895166, Value Loss: 4.268977420451847e-07\n",
      "Policy Loss: -0.20674352534115314, Value Loss: 1.9876193846357637e-07\n",
      "Policy Loss: -0.2067855503410101, Value Loss: 2.2367296992342744e-07\n",
      "Policy Loss: -0.20696409977972507, Value Loss: 3.4741280785510753e-07\n",
      "Policy Loss: -0.2068311758339405, Value Loss: 1.6810165348601913e-07\n",
      "Policy Loss: -0.20671408716589212, Value Loss: 2.5018294280698683e-07\n",
      "Policy Loss: -0.20715012960135937, Value Loss: 3.072142646942666e-07\n",
      "Policy Loss: -0.20746705681085587, Value Loss: 1.7960081422074836e-07\n",
      "Policy Loss: -0.2074027694761753, Value Loss: 1.700396410342364e-07\n",
      "Policy Loss: -0.20743317808955908, Value Loss: 1.8183215644285156e-07\n",
      "Policy Loss: -0.20733255706727505, Value Loss: 1.3089613659644783e-07\n",
      "Policy Loss: -0.20723406225442886, Value Loss: 1.710997850068452e-07\n",
      "Policy Loss: -0.2075346764177084, Value Loss: 1.4491234523461571e-07\n",
      "Policy Loss: -0.20767763629555702, Value Loss: 1.2749762845487567e-07\n",
      "Policy Loss: -0.2076476775109768, Value Loss: 1.0485028667517327e-07\n",
      "Policy Loss: -0.20777752809226513, Value Loss: 1.6967387495014918e-07\n",
      "Policy Loss: -0.2076893150806427, Value Loss: 1.0389409865751986e-07\n",
      "Policy Loss: -0.2075578635558486, Value Loss: 1.0399035410557644e-07\n",
      "Policy Loss: -0.20762675162404776, Value Loss: 1.345031535038288e-07\n",
      "Policy Loss: -0.20792132709175348, Value Loss: 1.0222396262804523e-07\n",
      "Policy Loss: -0.2080041840672493, Value Loss: 1.1386078568875746e-07\n",
      "Policy Loss: -0.20774043910205364, Value Loss: 1.4584458440936032e-07\n",
      "Policy Loss: -0.20786554459482431, Value Loss: 1.3169238322063848e-07\n",
      "Policy Loss: -0.20792559999972582, Value Loss: 1.478998967741063e-07\n",
      "Policy Loss: -0.20783829782158136, Value Loss: 1.636154962625369e-07\n",
      "Policy Loss: -0.20806236006319523, Value Loss: 1.4565135586508404e-07\n",
      "Policy Loss: -0.2080273786559701, Value Loss: 1.179656337058077e-07\n",
      "Policy Loss: -0.20761203207075596, Value Loss: 9.751985885486647e-08\n",
      "Policy Loss: -0.20795894972980022, Value Loss: 1.398404769759054e-07\n",
      "Policy Loss: -0.20802266336977482, Value Loss: 1.140575562885715e-07\n",
      "Policy Loss: -0.207590501755476, Value Loss: 2.2344295658172086e-07\n",
      "Policy Loss: -0.20803652703762054, Value Loss: 2.852413842902024e-07\n",
      "Policy Loss: -0.20804189890623093, Value Loss: 1.1648566200506139e-07\n",
      "Policy Loss: -0.2073302362114191, Value Loss: 2.1489970691845883e-07\n",
      "Episode 1/10 completed with total reward: 0.0\n",
      "Policy Loss: -0.2079095533117652, Value Loss: 4.6452136359675933e-07\n",
      "Policy Loss: -0.20792779233306646, Value Loss: 2.2899337182025192e-07\n",
      "Policy Loss: -0.2074006088078022, Value Loss: 1.1618882922448392e-07\n",
      "Policy Loss: -0.20807837322354317, Value Loss: 4.3101702118519825e-07\n",
      "Policy Loss: -0.20758996345102787, Value Loss: 4.324173517034069e-07\n",
      "Policy Loss: -0.20729056559503078, Value Loss: 1.0363307012539735e-07\n",
      "Policy Loss: -0.20787685364484787, Value Loss: 2.1979914777148224e-07\n",
      "Policy Loss: -0.20728681422770023, Value Loss: 4.0366011955228487e-07\n",
      "Policy Loss: -0.20734875835478306, Value Loss: 2.3129252468834238e-07\n",
      "Policy Loss: -0.20778961200267076, Value Loss: 1.2576568053646042e-07\n",
      "Policy Loss: -0.2069871574640274, Value Loss: 3.8630953369533927e-07\n",
      "Policy Loss: -0.2072149971500039, Value Loss: 2.559026563986322e-07\n",
      "Policy Loss: -0.2073262082412839, Value Loss: 7.66222640891101e-08\n",
      "Policy Loss: -0.20682419277727604, Value Loss: 2.1289714879912935e-07\n",
      "Policy Loss: -0.207380136474967, Value Loss: 3.4182862229670263e-07\n",
      "Policy Loss: -0.2069630492478609, Value Loss: 2.2187366610637582e-07\n",
      "Policy Loss: -0.206576244905591, Value Loss: 1.0710075670772312e-07\n",
      "Policy Loss: -0.20720962807536125, Value Loss: 2.556465421577059e-07\n",
      "Policy Loss: -0.20661795418709517, Value Loss: 3.9170663868048905e-07\n",
      "Policy Loss: -0.20665761269629002, Value Loss: 1.5061635849633603e-07\n",
      "Policy Loss: -0.20725168380886316, Value Loss: 2.1533053384814593e-07\n",
      "Policy Loss: -0.20611971896141768, Value Loss: 7.060451530094269e-07\n",
      "Policy Loss: -0.20645663607865572, Value Loss: 4.020301132001691e-07\n",
      "Policy Loss: -0.20693068206310272, Value Loss: 9.15377142973739e-08\n",
      "Policy Loss: -0.205905107781291, Value Loss: 4.005961402597791e-07\n",
      "Policy Loss: -0.20651268307119608, Value Loss: 4.677736296088142e-07\n",
      "Policy Loss: -0.20641301292926073, Value Loss: 1.6047238382554951e-07\n",
      "Policy Loss: -0.20563590247184038, Value Loss: 1.8441577509520357e-07\n",
      "Policy Loss: -0.20644110720604658, Value Loss: 4.971128326758389e-07\n",
      "Policy Loss: -0.2058801157400012, Value Loss: 3.4841881468850033e-07\n",
      "Policy Loss: -0.20586413331329823, Value Loss: 1.1966297996934827e-07\n",
      "Policy Loss: -0.2059868536889553, Value Loss: 8.900046566395758e-08\n",
      "Policy Loss: -0.20546079613268375, Value Loss: 1.389970964993381e-07\n",
      "Policy Loss: -0.20580989494919777, Value Loss: 2.0752698581816276e-07\n",
      "Policy Loss: -0.20557405799627304, Value Loss: 1.134725069462661e-07\n",
      "Policy Loss: -0.20509643200784922, Value Loss: 8.13841278812788e-08\n",
      "Policy Loss: -0.20567030925303698, Value Loss: 2.3035263296833364e-07\n",
      "Policy Loss: -0.20512224361300468, Value Loss: 2.2634260155030006e-07\n",
      "Policy Loss: -0.20502120349556208, Value Loss: 1.0446495757321372e-07\n",
      "Policy Loss: -0.20530620217323303, Value Loss: 7.840848148532586e-08\n",
      "Policy Loss: -0.20484534837305546, Value Loss: 8.966921782871395e-08\n",
      "Policy Loss: -0.20489615574479103, Value Loss: 9.089160535324936e-08\n",
      "Policy Loss: -0.20475961454212666, Value Loss: 5.134489100555584e-08\n",
      "Policy Loss: -0.20458767749369144, Value Loss: 7.216026531864372e-08\n",
      "Policy Loss: -0.20449659042060375, Value Loss: 9.28341989947512e-08\n",
      "Policy Loss: -0.20464957226067781, Value Loss: 9.482784335546057e-08\n",
      "Policy Loss: -0.20433467533439398, Value Loss: 1.1777012431934963e-07\n",
      "Policy Loss: -0.20420169178396463, Value Loss: 7.235445664832696e-08\n",
      "Policy Loss: -0.20434757322072983, Value Loss: 7.897992304606305e-08\n",
      "Policy Loss: -0.2037198767066002, Value Loss: 1.4144350934941485e-07\n",
      "Policy Loss: -0.20431892573833466, Value Loss: 3.35355878178234e-07\n",
      "Policy Loss: -0.20351248793303967, Value Loss: 3.216739887790254e-07\n",
      "Policy Loss: -0.2038836209103465, Value Loss: 2.4893550332194536e-07\n",
      "Policy Loss: -0.20367240346968174, Value Loss: 1.258520399005647e-07\n",
      "Policy Loss: -0.20329567603766918, Value Loss: 1.1979836855857684e-07\n",
      "Policy Loss: -0.20383604802191257, Value Loss: 2.5181882179481363e-07\n",
      "Policy Loss: -0.20283827185630798, Value Loss: 5.428715521560434e-07\n",
      "Policy Loss: -0.20352695602923632, Value Loss: 5.646250365032301e-07\n",
      "Policy Loss: -0.2029305100440979, Value Loss: 3.479599648414933e-07\n",
      "Policy Loss: -0.20298073440790176, Value Loss: 1.217888154769753e-07\n",
      "Policy Loss: -0.20312051847577095, Value Loss: 7.484305930560708e-08\n",
      "Policy Loss: -0.20249502453953028, Value Loss: 1.5917554208044749e-07\n",
      "Policy Loss: -0.2028204072266817, Value Loss: 1.5437696809783574e-07\n",
      "Policy Loss: -0.20270725898444653, Value Loss: 1.1366357455244724e-07\n",
      "Policy Loss: -0.20251311361789703, Value Loss: 7.57680851393161e-08\n",
      "Policy Loss: -0.20257668383419514, Value Loss: 1.0219338619776863e-07\n",
      "Policy Loss: -0.2023293375968933, Value Loss: 7.30737523824132e-08\n",
      "Policy Loss: -0.20249361917376518, Value Loss: 9.014060431411508e-08\n",
      "Policy Loss: -0.202215276658535, Value Loss: 1.236739564092204e-07\n",
      "Policy Loss: -0.20228444412350655, Value Loss: 1.0541592487811613e-07\n",
      "Policy Loss: -0.20214816834777594, Value Loss: 5.9204564806236704e-08\n",
      "Policy Loss: -0.20185962319374084, Value Loss: 6.912033967410025e-08\n",
      "Policy Loss: -0.2023257091641426, Value Loss: 1.4847329588008051e-07\n",
      "Policy Loss: -0.20177326258271933, Value Loss: 2.746891887284164e-07\n",
      "Policy Loss: -0.20225157588720322, Value Loss: 2.7590860263160266e-07\n",
      "Policy Loss: -0.20154429785907269, Value Loss: 3.386206035571604e-07\n",
      "Policy Loss: -0.20195274706929922, Value Loss: 2.389596520657733e-07\n",
      "Policy Loss: -0.2017087573185563, Value Loss: 1.1321253623464145e-07\n",
      "Policy Loss: -0.2015219321474433, Value Loss: 4.7908581102262815e-08\n",
      "Policy Loss: -0.20188823342323303, Value Loss: 1.4237695467045342e-07\n",
      "Policy Loss: -0.2011113427579403, Value Loss: 2.2467627758260278e-07\n",
      "Policy Loss: -0.20176089089363813, Value Loss: 3.5245375329395756e-07\n",
      "Policy Loss: -0.20119500439614058, Value Loss: 2.9620885388936813e-07\n",
      "Policy Loss: -0.20161495450884104, Value Loss: 1.6608983965937796e-07\n",
      "Policy Loss: -0.20137966610491276, Value Loss: 7.365610876775008e-08\n",
      "Policy Loss: -0.20123074017465115, Value Loss: 6.379820050028684e-08\n",
      "Policy Loss: -0.20121381524950266, Value Loss: 6.266580498959229e-08\n",
      "Policy Loss: -0.20122331753373146, Value Loss: 8.341840729286787e-08\n",
      "Policy Loss: -0.20087803341448307, Value Loss: 1.005346028648546e-07\n",
      "Policy Loss: -0.20140660740435123, Value Loss: 1.8101750676180473e-07\n",
      "Policy Loss: -0.20050243102014065, Value Loss: 3.0463528410962226e-07\n",
      "Policy Loss: -0.201224016956985, Value Loss: 3.5018821886723117e-07\n",
      "Policy Loss: -0.20076947286725044, Value Loss: 2.5560801031332403e-07\n",
      "Policy Loss: -0.2007201835513115, Value Loss: 7.198775087147169e-08\n",
      "Policy Loss: -0.20096580311655998, Value Loss: 5.981769568563777e-08\n",
      "Policy Loss: -0.20014648884534836, Value Loss: 2.1513142733908808e-07\n",
      "Policy Loss: -0.20114338025450706, Value Loss: 4.528409025539304e-07\n",
      "Policy Loss: -0.19974026549607515, Value Loss: 7.264532939643686e-07\n",
      "Policy Loss: -0.20099242124706507, Value Loss: 8.967728923892082e-07\n",
      "Policy Loss: -0.19987605139613152, Value Loss: 7.83222336764311e-07\n",
      "Episode 2/10 completed with total reward: 0.0\n",
      "Policy Loss: -0.20068861171603203, Value Loss: 5.590217266870923e-07\n",
      "Policy Loss: -0.20004045218229294, Value Loss: 3.4243235447206644e-07\n",
      "Policy Loss: -0.20019419491291046, Value Loss: 1.300062804077129e-07\n",
      "Policy Loss: -0.20038563292473555, Value Loss: 5.242259604187893e-08\n",
      "Policy Loss: -0.1998855322599411, Value Loss: 1.3862942660480826e-07\n",
      "Policy Loss: -0.2007242413237691, Value Loss: 3.6329648978039586e-07\n",
      "Policy Loss: -0.19935954920947552, Value Loss: 6.911703547274328e-07\n",
      "Policy Loss: -0.20085238572210073, Value Loss: 1.065676036304808e-06\n",
      "Policy Loss: -0.1987630957737565, Value Loss: 1.6720313693596722e-06\n",
      "Policy Loss: -0.20109229907393456, Value Loss: 2.253561063980669e-06\n",
      "Policy Loss: -0.19862097315490246, Value Loss: 2.68330494179736e-06\n",
      "Policy Loss: -0.2011276464909315, Value Loss: 3.1059304301805923e-06\n",
      "Policy Loss: -0.1979609215632081, Value Loss: 3.926000117893125e-06\n",
      "Policy Loss: -0.20140291936695576, Value Loss: 4.937401911320194e-06\n",
      "Policy Loss: -0.19730281550437212, Value Loss: 6.534840395033825e-06\n",
      "Policy Loss: -0.20231336262077093, Value Loss: 9.73369484214004e-06\n",
      "Policy Loss: -0.19566708337515593, Value Loss: 1.576306506034797e-05\n",
      "Policy Loss: -0.20398841984570026, Value Loss: 2.653539885777434e-05\n",
      "Policy Loss: -0.1925070881843567, Value Loss: 4.7195955774625986e-05\n",
      "Policy Loss: -0.20814891532063484, Value Loss: 8.653710872863485e-05\n",
      "Policy Loss: -0.18684166111052036, Value Loss: 0.00015894963785889615\n",
      "Policy Loss: -0.21563445404171944, Value Loss: 0.00029481621572990946\n",
      "Policy Loss: -0.1755656637251377, Value Loss: 0.0005556921404323845\n",
      "Policy Loss: -0.2309671388939023, Value Loss: 0.0010669713777184953\n",
      "Policy Loss: -0.15385408699512482, Value Loss: 0.0020522392331931982\n",
      "Policy Loss: -0.2600530106574297, Value Loss: 0.003985873885765656\n",
      "Policy Loss: -0.11627895198762417, Value Loss: 0.007449180713797077\n",
      "Policy Loss: -0.3007816905155778, Value Loss: 0.013410246786774493\n",
      "Policy Loss: -0.08309536427259445, Value Loss: 0.02073056151919772\n",
      "Policy Loss: -0.3024451741948724, Value Loss: 0.02636444476410915\n",
      "Policy Loss: -0.13302186131477356, Value Loss: 0.02144611018125886\n",
      "Policy Loss: -0.20533253252506256, Value Loss: 0.008516063532717055\n",
      "Policy Loss: -0.24621809367090464, Value Loss: 9.94366766455812e-05\n",
      "Policy Loss: -0.12206860166043043, Value Loss: 0.004536227122098779\n",
      "Policy Loss: -0.2627920936793089, Value Loss: 0.011511605247407353\n",
      "Policy Loss: -0.17712736316025257, Value Loss: 0.008241848889954717\n",
      "Policy Loss: -0.16942742094397545, Value Loss: 0.0009002396863095896\n",
      "Policy Loss: -0.2547103203833103, Value Loss: 0.0016219422988257293\n",
      "Policy Loss: -0.14995176531374454, Value Loss: 0.006337187037790848\n",
      "Policy Loss: -0.20846554916352034, Value Loss: 0.0046023371784755795\n",
      "Policy Loss: -0.22777452785521746, Value Loss: 0.00022195996476259694\n",
      "Policy Loss: -0.15166827011853456, Value Loss: 0.0017509555430343937\n",
      "Policy Loss: -0.2270730547606945, Value Loss: 0.004216694055260817\n",
      "Policy Loss: -0.2035896684974432, Value Loss: 0.0017028730366730471\n",
      "Policy Loss: -0.16425567399710417, Value Loss: 7.080601183950819e-05\n",
      "Policy Loss: -0.231531273573637, Value Loss: 0.0022081552236006274\n",
      "Policy Loss: -0.18754233047366142, Value Loss: 0.002280750722214009\n",
      "Policy Loss: -0.1784784495830536, Value Loss: 0.00019481846837887673\n",
      "Policy Loss: -0.22801178600639105, Value Loss: 0.0007097650628593577\n",
      "Policy Loss: -0.17857032269239426, Value Loss: 0.0018780959097153271\n",
      "Policy Loss: -0.19068296253681183, Value Loss: 0.0006822675074658058\n",
      "Policy Loss: -0.2207375792786479, Value Loss: 8.014195753602849e-05\n",
      "Policy Loss: -0.17504146602004766, Value Loss: 0.0011335905431213433\n",
      "Policy Loss: -0.20024280343204737, Value Loss: 0.000951367692012095\n",
      "Policy Loss: -0.21200721617788076, Value Loss: 2.7209660553140225e-05\n",
      "Policy Loss: -0.1758129708468914, Value Loss: 0.0004684886419892287\n",
      "Policy Loss: -0.20616326108574867, Value Loss: 0.0008569879306135064\n",
      "Policy Loss: -0.20366526395082474, Value Loss: 0.00019347919763479382\n",
      "Policy Loss: -0.1793194394558668, Value Loss: 0.00010546253612364609\n",
      "Policy Loss: -0.2085733087733388, Value Loss: 0.0005643454426973449\n",
      "Policy Loss: -0.1969219772145152, Value Loss: 0.0003114540516744313\n",
      "Policy Loss: -0.18402446806430817, Value Loss: 1.6450639703791126e-06\n",
      "Policy Loss: -0.2084293980151415, Value Loss: 0.0002772662327501507\n",
      "Policy Loss: -0.19176470208913088, Value Loss: 0.00032224902591693194\n",
      "Policy Loss: -0.18872311897575855, Value Loss: 3.1052186649560554e-05\n",
      "Policy Loss: -0.20729496888816357, Value Loss: 9.944753550783503e-05\n",
      "Policy Loss: -0.18837745301425457, Value Loss: 0.00027257841357197776\n",
      "Policy Loss: -0.19300365913659334, Value Loss: 0.00010055524585972364\n",
      "Policy Loss: -0.20410833694040775, Value Loss: 9.983599930762921e-06\n",
      "Policy Loss: -0.18712471798062325, Value Loss: 0.00015296187126345728\n",
      "Policy Loss: -0.1965616885572672, Value Loss: 0.00013068871273702598\n",
      "Policy Loss: -0.20026693027466536, Value Loss: 5.437752506054494e-06\n",
      "Policy Loss: -0.1872918540611863, Value Loss: 5.5696233646429505e-05\n",
      "Policy Loss: -0.19871520157903433, Value Loss: 0.00011257541353515421\n",
      "Policy Loss: -0.19693371653556824, Value Loss: 3.0384150983664426e-05\n",
      "Policy Loss: -0.18838412314653397, Value Loss: 1.0640829971819699e-05\n",
      "Policy Loss: -0.19960878789424896, Value Loss: 7.565374020934712e-05\n",
      "Policy Loss: -0.19383294694125652, Value Loss: 5.308384033675395e-05\n",
      "Policy Loss: -0.19041957892477512, Value Loss: 1.010060339368124e-06\n",
      "Policy Loss: -0.19900978449732065, Value Loss: 2.7249658314687508e-05\n",
      "Policy Loss: -0.19157776981592178, Value Loss: 4.714383676329703e-05\n",
      "Policy Loss: -0.1926964344456792, Value Loss: 1.2836754201828171e-05\n",
      "Policy Loss: -0.19735439587384462, Value Loss: 2.729830137049305e-06\n",
      "Policy Loss: -0.19063318893313408, Value Loss: 2.4078521532189257e-05\n",
      "Policy Loss: -0.19429121166467667, Value Loss: 1.925077780629003e-05\n",
      "Policy Loss: -0.19559679366648197, Value Loss: 9.180426836508104e-07\n",
      "Policy Loss: -0.1904859719797969, Value Loss: 8.204610183248917e-06\n",
      "Policy Loss: -0.19532077945768833, Value Loss: 1.8421741529994762e-05\n",
      "Policy Loss: -0.19353479892015457, Value Loss: 7.770907792803428e-06\n",
      "Policy Loss: -0.19102523382753134, Value Loss: 2.5855202068214567e-07\n",
      "Policy Loss: -0.19545461051166058, Value Loss: 9.2424339044328e-06\n",
      "Policy Loss: -0.19220109563320875, Value Loss: 1.0408961451879861e-05\n",
      "Policy Loss: -0.19176859594881535, Value Loss: 1.28576419200499e-06\n",
      "Policy Loss: -0.1949561256915331, Value Loss: 2.4640030216183106e-06\n",
      "Policy Loss: -0.19098135363310575, Value Loss: 9.241264245396508e-06\n",
      "Policy Loss: -0.19305779971182346, Value Loss: 6.6001376515245624e-06\n",
      "Policy Loss: -0.19346459489315748, Value Loss: 5.1436963488527e-07\n",
      "Policy Loss: -0.19066458940505981, Value Loss: 2.0721739932838545e-06\n",
      "Policy Loss: -0.1938777407631278, Value Loss: 6.646938643939393e-06\n",
      "Policy Loss: -0.19203187711536884, Value Loss: 4.576505518549823e-06\n",
      "Episode 3/10 completed with total reward: 0.0\n",
      "Policy Loss: -0.191171838901937, Value Loss: 2.0491887831042277e-07\n",
      "Policy Loss: -0.19355388917028904, Value Loss: 1.9454199216006884e-06\n",
      "Policy Loss: -0.19098455738276243, Value Loss: 4.4202847724328365e-06\n",
      "Policy Loss: -0.19195833429694176, Value Loss: 2.173544754846546e-06\n",
      "Policy Loss: -0.19264406338334084, Value Loss: 7.431352599951424e-08\n",
      "Policy Loss: -0.19072142243385315, Value Loss: 1.227405922676894e-06\n",
      "Policy Loss: -0.1922512762248516, Value Loss: 2.1817009088209716e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replay_buffer\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m---> 72\u001b[0m     policy_loss, value_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmaddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Value Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Increment step count\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mMADDPG.update\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     next_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m     23\u001b[0m         ag\u001b[38;5;241m.\u001b[39mactor_target(next_obs[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, ag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents)\n\u001b[1;32m     24\u001b[0m     ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     target_q \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m q_value \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcritic(obs, actions)\n\u001b[1;32m     28\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_value, target_q)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Debugging shapes\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print(f\"obs shape: {obs.shape}, actions shape: {actions.shape}\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([obs, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs138_project/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 10\n",
    "batch_size = 64\n",
    "\n",
    "max_steps_per_episode = 100  # Set the maximum steps per episode\n",
    "for episode in range(num_episodes):\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "\n",
    "            # Convert continuous action to discrete action\n",
    "            discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            print(f\"Policy Loss: {policy_loss}, Value Loss: {value_loss}\")\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Print cumulative reward for the episode\n",
    "    total_reward = sum(episode_reward.values())\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed with total reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/30 completed with total reward: 0.0, Total Loss: -9.472727838390162\n",
      "Episode 2/30 completed with total reward: 0.0, Total Loss: -9.272694775922105\n",
      "Episode 3/30 completed with total reward: 0.0, Total Loss: -8.988100596469101\n",
      "Episode 4/30 completed with total reward: 0.0, Total Loss: -8.847783310707563\n",
      "Episode 5/30 completed with total reward: 0.0, Total Loss: -8.654771053341438\n",
      "Episode 6/30 completed with total reward: 0.0, Total Loss: -8.466184665947482\n",
      "Episode 7/30 completed with total reward: 0.0, Total Loss: -8.291770481918148\n",
      "Episode 8/30 completed with total reward: 0.0, Total Loss: -7.985007055572987\n",
      "Episode 9/30 completed with total reward: 0.0, Total Loss: -7.833614682203307\n",
      "Episode 10/30 completed with total reward: 0.0, Total Loss: -7.668229397509033\n",
      "Episode 11/30 completed with total reward: 0.0, Total Loss: -7.508503085138606\n",
      "Episode 12/30 completed with total reward: 0.0, Total Loss: -7.251800697686379\n",
      "Episode 13/30 completed with total reward: 0.0, Total Loss: -7.170812818471603\n",
      "Episode 14/30 completed with total reward: 0.0, Total Loss: -7.00702425115508\n",
      "Episode 15/30 completed with total reward: 0.0, Total Loss: -6.849240025434648\n",
      "Episode 16/30 completed with total reward: 0.0, Total Loss: -6.698211987868885\n",
      "Episode 17/30 completed with total reward: 0.0, Total Loss: -6.55137251240416\n",
      "Episode 18/30 completed with total reward: 0.0, Total Loss: -6.304507550495169\n",
      "Episode 19/30 completed with total reward: 0.0, Total Loss: -6.251951865457547\n",
      "Episode 20/30 completed with total reward: 0.0, Total Loss: -5.72041965949242\n",
      "Episode 21/30 completed with total reward: 0.0, Total Loss: -5.860259345908527\n",
      "Episode 22/30 completed with total reward: 0.0, Total Loss: -5.6226399377508285\n",
      "Episode 23/30 completed with total reward: 0.0, Total Loss: -5.5442808742536975\n",
      "Episode 24/30 completed with total reward: 0.0, Total Loss: -5.463128952043212\n",
      "Episode 25/30 completed with total reward: 0.0, Total Loss: -5.264758291526833\n",
      "Episode 26/30 completed with total reward: 0.0, Total Loss: -5.123747212752561\n",
      "Episode 27/30 completed with total reward: 0.0, Total Loss: -4.956289045733046\n",
      "Episode 28/30 completed with total reward: 0.0, Total Loss: -4.8580086510892055\n",
      "Episode 29/30 completed with total reward: 0.0, Total Loss: -4.754231925069562\n",
      "Episode 30/30 completed with total reward: 0.0, Total Loss: -4.641514845234284\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 30\n",
    "batch_size = 64\n",
    "\n",
    "max_steps_per_episode = 50  # Set the maximum steps per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Track total loss per episode\n",
    "total_loss_per_episode = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    episode_policy_loss = 0  # Accumulate policy loss\n",
    "    episode_value_loss = 0  # Accumulate value loss\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "\n",
    "            # Convert continuous action to discrete action\n",
    "            discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            episode_policy_loss += policy_loss\n",
    "            episode_value_loss += value_loss\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Record total loss for this episode\n",
    "    total_loss = episode_policy_loss + episode_value_loss\n",
    "    total_loss_per_episode.append(total_loss)\n",
    "\n",
    "    # Print cumulative reward and total loss for the episode\n",
    "    total_reward = sum(episode_reward.values())\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed with total reward: {total_reward}, Total Loss: {total_loss}\")\n",
    "\n",
    "# Plot total loss per episode\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_loss_per_episode, label='Total Loss')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss Per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to track agent rewards separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 157.77606607041687\n",
      "Episode 1/6 completed in 351.40 seconds\n",
      "Episode 2/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 86.5786312672517\n",
      "Episode 2/6 completed in 374.47 seconds\n",
      "Episode 3/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 43.49052276886123\n",
      "Episode 3/6 completed in 374.27 seconds\n",
      "Episode 4/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 19.464231006591895\n",
      "Episode 4/6 completed in 374.08 seconds\n",
      "Episode 5/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 5.522323497176852\n",
      "Episode 5/6 completed in 373.26 seconds\n",
      "Episode 6/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 6.522291655597713\n",
      "Episode 6/6 completed in 372.90 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 6\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 1000 # maybe 30-45 seconds per 100 steps\n",
    "\n",
    "# Track total loss and rewards per episode\n",
    "total_loss_per_episode = []\n",
    "agent1_rewards = []  # Rewards for the first agent\n",
    "agent2_rewards = []  # Rewards for the second agent\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Start timing the episode\n",
    "    start_time = time.time()\n",
    "\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    episode_policy_loss = 0  # Accumulate policy loss\n",
    "    episode_value_loss = 0  # Accumulate value loss\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "\n",
    "            # Convert continuous action to discrete action\n",
    "            discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards for each agent\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            episode_policy_loss += policy_loss\n",
    "            episode_value_loss += value_loss\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Record total loss for this episode\n",
    "    total_loss = episode_policy_loss + episode_value_loss\n",
    "    total_loss_per_episode.append(total_loss)\n",
    "\n",
    "    # Separate rewards for each agent\n",
    "    agent1_reward = episode_reward[parallel_env.agents[0]]\n",
    "    agent2_reward = episode_reward[parallel_env.agents[1]]\n",
    "\n",
    "    # Append rewards to lists\n",
    "    agent1_rewards.append(agent1_reward)\n",
    "    agent2_rewards.append(agent2_reward)\n",
    "\n",
    "    # Print cumulative reward and total loss for the episode\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "    print(f\"  {parallel_env.agents[0]} Reward: {agent1_reward}\")\n",
    "    print(f\"  {parallel_env.agents[1]} Reward: {agent2_reward}\")\n",
    "    print(f\"  Total Loss: {total_loss}\")\n",
    "\n",
    "    # End timing the episode\n",
    "    end_time = time.time()\n",
    "    episode_duration = end_time - start_time\n",
    "\n",
    "    # Print the duration\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed in {episode_duration:.2f} seconds\")\n",
    "\n",
    "# Plot total loss per episode\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_loss_per_episode, label='Total Loss', color='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss Per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot rewards for both agents\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(agent1_rewards, label=parallel_env.agents[0], color='green')\n",
    "plt.plot(agent2_rewards, label=parallel_env.agents[1], color='orange')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward Per Episode for Each Agent')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'first_0': 0, 'second_0': 0})\n"
     ]
    }
   ],
   "source": [
    "print(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions taken by agents: {'first_0': 0, 'second_0': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actions taken by agents: {actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print actions in a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 1/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 0\n",
      "Episode 1/6 completed in 0.33 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 11\n",
      "  second_0: Action 11\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 2/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 28.479087879345798\n",
      "Episode 2/6 completed in 14.19 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 3/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 3.8051739525641324\n",
      "Episode 3/6 completed in 19.10 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 4/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 3.8576771328978836\n",
      "Episode 4/6 completed in 18.93 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 5/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 4.000363493795987\n",
      "Episode 5/6 completed in 18.90 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 3\n",
      "  second_0: Action 3\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 6/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 4.106419457061456\n",
      "Episode 6/6 completed in 19.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 6\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 50  # maybe 30-45 seconds per 100 steps\n",
    "\n",
    "# Track total loss and rewards per episode\n",
    "total_loss_per_episode = []\n",
    "agent1_rewards = []  # Rewards for the first agent\n",
    "agent2_rewards = []  # Rewards for the second agent\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Start timing the episode\n",
    "    start_time = time.time()\n",
    "\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    episode_policy_loss = 0  # Accumulate policy loss\n",
    "    episode_value_loss = 0  # Accumulate value loss\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "\n",
    "            # Convert continuous action to discrete action\n",
    "            discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "        # Print actions for debugging\n",
    "        print(f\"Step {step_count + 1} Actions:\")\n",
    "        for agent, action in actions.items():\n",
    "            print(f\"  {agent}: Action {action}\")\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards for each agent\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Print rewards for debugging\n",
    "        print(f\"Step {step_count + 1} Rewards:\")\n",
    "        for agent, reward in rewards.items():\n",
    "            print(f\"  {agent}: Reward {reward}\")\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            episode_policy_loss += policy_loss\n",
    "            episode_value_loss += value_loss\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Record total loss for this episode\n",
    "    total_loss = episode_policy_loss + episode_value_loss\n",
    "    total_loss_per_episode.append(total_loss)\n",
    "\n",
    "    # Separate rewards for each agent\n",
    "    agent1_reward = episode_reward[parallel_env.agents[0]]\n",
    "    agent2_reward = episode_reward[parallel_env.agents[1]]\n",
    "\n",
    "    # Append rewards to lists\n",
    "    agent1_rewards.append(agent1_reward)\n",
    "    agent2_rewards.append(agent2_reward)\n",
    "\n",
    "    # Print cumulative reward and total loss for the episode\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "    print(f\"  {parallel_env.agents[0]} Reward: {agent1_reward}\")\n",
    "    print(f\"  {parallel_env.agents[1]} Reward: {agent2_reward}\")\n",
    "    print(f\"  Total Loss: {total_loss}\")\n",
    "\n",
    "    # End timing the episode\n",
    "    end_time = time.time()\n",
    "    episode_duration = end_time - start_time\n",
    "\n",
    "    # Print the duration\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed in {episode_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render using pettingzoo Atari human\n",
    "### 1000 steps is only about 17 seconds in gametime!!!\n",
    "### 3000 steps is about 51 seconds in gametime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 3:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 4:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 5:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 6:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 7:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 8:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 9:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 10:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 11:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 12:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 13:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 14:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 15:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 16:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 17:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 18:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 19:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 20:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 21:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 22:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 23:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 24:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 25:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 26:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 27:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 28:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 29:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 30:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 31:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 32:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 33:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 34:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 35:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 36:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 37:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 38:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 39:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 40:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 41:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 42:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 43:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 44:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 45:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 46:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 47:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 48:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 49:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 50:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 51:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 52:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 53:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 54:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 55:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 56:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 57:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 58:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 59:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 60:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 61:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 62:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 63:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 64:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 65:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 66:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 67:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 68:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 69:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 70:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 71:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 72:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 73:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 74:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 75:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 76:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 77:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 78:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 79:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 80:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 81:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 82:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 83:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 84:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 85:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 86:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 87:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 88:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 89:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 90:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 91:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 92:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 93:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 94:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 95:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 96:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 97:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 98:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 99:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 100:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 101:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 102:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 103:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 104:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 105:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 106:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 107:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 108:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 109:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 110:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 111:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 112:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 113:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 114:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 115:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 116:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 117:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 118:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 119:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 120:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 121:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 122:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 123:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 124:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 125:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 126:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 127:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 128:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 129:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 130:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 131:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 132:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 133:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 134:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 135:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 136:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 137:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 138:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 139:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 140:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 141:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 142:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 143:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 144:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 145:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 146:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 147:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 148:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 149:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 150:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 151:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 152:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 153:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 154:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 155:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 156:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 157:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 158:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 159:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 160:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 161:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 162:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 163:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 164:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 165:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 166:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 167:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 168:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 169:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 170:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 171:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 172:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 173:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 174:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 175:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 176:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 177:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 178:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 179:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 180:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 181:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 182:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 183:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 184:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 185:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 186:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 187:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 188:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 189:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 190:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 191:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 192:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 193:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 194:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 195:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 196:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 197:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 198:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 199:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 200:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 201:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 202:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 203:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 204:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 205:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 206:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 207:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 208:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 209:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 210:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 211:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 212:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 213:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 214:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 215:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 216:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 217:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 218:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 219:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 220:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 221:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 222:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 223:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 224:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 225:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 226:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 227:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 228:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 229:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 230:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 231:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 232:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 233:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 234:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 235:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 236:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 237:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 238:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 239:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 240:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 241:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 242:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 243:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 244:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 245:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 246:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 247:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 248:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 249:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 250:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 251:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 252:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 253:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 254:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 255:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 256:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 257:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 258:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 259:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 260:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 261:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 262:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 263:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 264:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 265:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 266:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 267:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 268:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 269:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 270:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 271:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 272:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 273:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 274:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 275:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 276:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 277:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 278:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 279:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 280:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 281:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 282:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 283:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 284:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 285:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 286:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 287:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 288:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 289:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 290:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 291:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 292:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 293:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 294:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 295:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 296:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 297:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 298:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 299:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 300:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 301:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 302:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 303:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 304:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 305:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 306:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 307:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 308:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 309:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 310:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 311:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 312:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 313:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 314:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 315:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 316:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 317:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 318:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 319:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 320:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 321:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 322:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 323:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 324:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 325:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 326:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 327:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 328:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 329:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 330:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 331:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 332:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 333:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 334:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 335:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 336:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 337:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 338:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 339:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 340:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 341:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 342:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 343:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 344:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 345:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 346:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 347:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 348:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 349:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 350:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 351:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 352:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 353:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 354:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 355:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 356:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 357:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 358:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 359:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 360:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 361:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 362:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 363:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 364:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 365:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 366:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 367:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 368:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 369:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 370:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 371:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 372:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 373:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 374:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 375:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 376:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 377:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 378:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 379:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 380:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 381:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 382:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 383:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 384:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 385:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 386:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 387:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 388:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 389:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 390:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 391:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 392:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 393:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 394:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 395:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 396:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 397:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 398:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 399:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 400:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 401:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 402:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 403:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 404:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 405:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 406:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 407:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 408:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 409:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 410:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 411:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 412:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 413:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 414:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 415:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 416:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 417:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 418:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 419:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 420:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 421:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 422:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 423:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 424:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 425:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 426:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 427:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 428:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 429:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 430:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 431:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 432:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 433:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 434:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 435:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 436:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 437:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 438:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 439:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 440:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 441:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 442:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 443:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 444:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 445:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 446:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 447:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 448:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 449:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 450:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 451:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 452:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 453:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 454:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 455:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 456:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 457:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 458:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 459:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 460:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 461:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 462:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 463:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 464:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 465:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 466:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 467:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 468:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 469:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 470:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 471:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 472:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 473:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 474:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 475:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 476:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 477:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 478:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 479:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 480:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 481:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 482:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 483:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 484:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 485:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 486:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 487:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 488:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 489:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 490:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 491:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 492:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 493:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 494:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 495:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 496:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 497:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 498:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 499:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 500:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 501:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 502:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 503:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 504:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 505:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 506:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 507:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 508:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 509:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 510:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 511:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 512:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 513:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 514:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 515:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 516:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 517:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 518:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 519:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 520:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 521:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 522:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 523:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 524:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 525:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 526:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 527:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 528:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 529:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 530:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 531:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 532:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 533:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 534:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 535:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 536:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 537:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 538:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 539:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 540:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 541:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 542:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 543:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 544:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 545:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 546:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 547:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 548:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 549:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 550:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 551:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 552:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 553:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 554:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 555:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 556:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 557:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 558:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 559:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 560:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 561:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 562:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 563:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 564:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 565:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 566:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 567:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 568:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 569:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 570:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 571:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 572:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 573:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 574:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 575:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 576:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 577:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 578:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 579:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 580:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 581:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 582:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 583:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 584:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 585:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 586:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 587:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 588:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 589:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 590:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 591:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 592:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 593:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 594:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 595:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 596:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 597:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 598:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 599:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 600:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 601:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 602:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 603:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 604:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 605:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 606:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 607:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 608:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 609:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 610:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 611:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 612:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 613:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 614:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 615:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 616:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 617:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 618:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 619:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 620:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 621:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 622:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 623:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 624:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 625:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 626:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 627:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 628:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 629:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 630:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 631:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 632:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 633:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 634:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 635:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 636:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 637:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 638:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 639:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 640:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 641:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 642:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 643:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 644:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 645:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 646:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 647:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 648:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 649:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 650:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 651:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 652:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 653:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 654:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 655:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 656:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 657:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 658:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 659:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 660:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 661:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 662:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 663:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 664:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 665:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 666:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 667:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 668:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 669:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 670:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 671:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 672:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 673:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 674:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 675:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 676:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 677:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 678:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 679:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 680:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 681:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 682:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 683:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 684:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 685:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 686:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 687:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 688:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 689:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 690:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 691:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 692:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 693:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 694:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 695:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 696:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 697:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 698:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 699:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 700:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 701:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 702:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 703:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 704:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 705:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 706:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 707:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 708:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 709:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 710:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 711:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 712:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 713:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 714:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 715:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 716:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 717:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 718:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 719:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 720:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 721:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 722:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 723:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 724:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 725:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 726:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 727:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 728:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 729:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 730:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 731:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 732:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 733:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 734:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 735:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 736:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 737:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 738:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 739:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 740:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 741:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 742:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 743:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 744:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 745:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 746:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 747:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 748:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 749:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 750:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 751:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 752:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 753:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 754:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 755:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 756:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 757:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 758:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 759:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 760:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 761:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 762:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 763:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 764:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 765:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 766:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 767:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 768:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 769:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 770:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 771:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 772:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 773:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 774:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 775:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 776:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 777:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 778:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 779:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 780:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 781:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 782:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 783:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 784:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 785:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 786:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 787:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 788:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 789:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 790:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 791:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 792:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 793:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 794:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 795:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 796:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 797:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 798:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 799:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 800:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 801:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 802:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 803:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 804:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 805:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 806:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 807:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 808:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 809:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 810:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 811:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 812:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 813:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 814:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 815:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 816:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 817:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 818:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 819:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 820:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 821:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 822:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 823:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 824:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 825:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 826:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 827:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 828:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 829:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 830:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 831:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 832:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 833:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 834:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 835:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 836:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 837:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 838:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 839:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 840:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 841:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 842:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 843:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 844:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 845:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 846:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 847:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 848:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 849:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 850:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 851:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 852:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 853:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 854:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 855:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 856:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 857:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 858:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 859:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 860:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 861:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 862:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 863:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 864:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 865:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 866:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 867:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 868:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 869:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 870:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 871:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 872:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 873:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 874:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 875:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 876:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 877:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 878:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 879:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 880:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 881:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 882:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 883:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 884:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 885:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 886:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 887:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 888:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 889:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 890:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 891:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 892:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 893:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 894:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 895:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 896:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 897:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 898:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 899:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 900:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 901:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 902:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 903:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 904:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 905:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 906:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 907:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 908:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 909:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 910:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 911:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 912:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 913:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 914:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 915:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 916:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 917:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 918:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 919:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 920:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 921:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 922:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 923:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 924:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 925:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 926:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 927:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 928:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 929:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 930:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 931:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 932:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 933:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 934:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 935:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 936:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 937:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 938:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 939:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 940:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 941:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 942:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 943:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 944:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 945:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 946:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 947:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 948:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 949:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 950:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 951:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 952:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 953:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 954:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 955:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 956:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 957:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 958:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 959:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 960:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 961:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 962:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 963:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 964:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 965:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 966:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 967:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 968:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 969:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 970:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 971:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 972:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 973:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 974:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 975:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 976:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 977:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 978:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 979:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 980:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 981:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 982:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 983:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 984:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 985:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 986:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 987:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 988:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 989:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 990:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 991:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 992:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 993:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 994:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 995:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 996:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 997:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 998:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 999:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1000:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1001:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1002:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1003:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1004:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1005:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1006:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1007:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1008:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1009:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1010:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1011:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1012:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1013:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1014:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1015:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1016:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1017:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1018:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1019:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1020:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1021:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1022:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1023:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1024:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1025:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1026:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1027:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1028:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1029:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1030:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1031:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1032:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1033:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1034:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1035:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1036:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1037:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1038:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1039:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1040:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1041:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1042:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1043:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1044:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1045:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1046:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1047:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1048:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1049:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1050:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1051:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1052:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1053:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1054:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1055:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1056:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1057:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1058:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1059:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1060:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1061:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1062:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1063:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1064:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1065:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1066:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1067:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1068:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1069:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1070:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1071:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1072:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1073:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1074:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1075:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1076:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1077:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1078:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1079:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1080:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1081:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1082:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1083:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1084:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1085:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1086:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1087:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1088:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1089:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1090:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1091:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1092:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1093:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1094:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1095:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1096:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1097:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1098:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1099:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1100:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1101:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1102:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1103:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1104:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1105:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1106:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1107:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1108:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1109:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1110:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1111:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1112:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1113:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1114:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1115:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1116:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1117:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1118:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1119:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1120:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1121:\n",
      "  first_0 Reward: 1\n",
      "  second_0 Reward: -1\n",
      "Step 1122:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1123:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1124:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1125:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1126:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1127:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1128:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1129:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1130:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1131:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1132:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1133:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1134:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1135:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1136:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1137:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1138:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1139:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1140:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1141:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1142:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1143:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1144:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1145:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1146:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1147:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1148:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1149:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1150:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1151:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1152:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1153:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1154:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1155:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1156:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1157:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1158:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1159:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1160:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1161:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1162:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1163:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1164:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1165:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1166:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1167:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1168:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1169:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1170:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1171:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1172:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1173:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1174:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1175:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1176:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1177:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1178:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1179:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1180:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1181:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1182:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1183:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1184:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1185:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1186:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1187:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1188:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1189:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1190:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1191:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1192:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1193:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1194:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1195:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1196:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1197:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1198:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1199:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1200:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1201:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1202:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1203:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1204:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1205:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1206:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1207:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1208:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1209:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1210:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1211:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1212:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1213:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1214:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1215:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1216:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1217:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1218:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1219:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1220:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1221:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1222:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1223:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1224:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1225:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1226:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1227:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1228:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1229:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1230:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1231:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1232:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1233:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1234:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1235:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1236:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1237:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1238:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1239:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1240:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1241:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1242:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1243:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1244:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1245:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1246:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1247:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1248:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1249:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1250:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1251:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1252:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1253:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1254:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1255:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1256:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1257:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1258:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1259:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1260:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1261:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1262:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1263:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1264:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1265:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1266:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1267:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1268:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1269:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1270:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1271:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1272:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1273:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1274:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1275:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1276:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1277:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1278:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1279:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1280:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1281:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1282:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1283:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1284:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1285:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1286:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1287:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1288:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1289:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1290:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1291:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1292:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1293:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1294:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1295:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1296:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1297:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1298:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1299:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1300:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1301:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1302:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1303:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1304:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1305:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1306:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1307:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1308:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1309:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1310:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1311:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1312:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1313:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1314:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1315:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1316:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1317:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1318:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1319:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1320:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1321:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1322:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1323:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1324:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1325:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1326:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1327:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1328:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1329:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1330:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1331:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1332:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1333:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1334:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1335:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1336:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1337:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1338:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1339:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1340:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1341:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1342:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1343:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1344:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1345:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1346:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1347:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1348:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1349:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1350:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1351:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1352:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1353:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1354:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1355:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1356:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1357:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1358:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1359:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1360:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1361:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1362:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1363:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1364:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1365:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1366:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1367:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1368:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1369:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1370:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1371:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1372:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1373:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1374:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1375:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1376:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1377:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1378:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1379:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1380:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1381:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1382:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1383:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1384:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1385:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1386:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1387:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1388:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1389:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1390:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1391:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1392:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1393:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1394:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1395:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1396:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1397:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1398:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1399:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1400:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1401:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1402:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1403:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1404:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1405:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1406:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1407:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1408:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1409:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1410:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1411:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1412:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1413:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1414:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1415:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1416:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1417:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1418:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1419:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1420:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1421:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1422:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1423:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1424:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1425:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1426:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1427:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1428:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1429:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1430:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1431:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1432:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1433:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1434:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1435:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1436:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1437:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1438:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1439:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1440:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1441:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1442:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1443:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1444:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1445:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1446:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1447:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1448:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1449:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1450:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1451:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1452:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1453:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1454:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1455:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1456:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1457:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1458:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1459:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1460:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1461:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1462:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1463:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1464:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1465:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1466:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1467:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1468:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1469:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1470:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1471:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1472:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1473:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1474:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1475:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1476:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1477:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1478:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1479:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1480:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1481:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1482:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1483:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1484:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1485:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1486:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1487:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1488:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1489:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1490:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1491:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1492:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1493:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1494:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1495:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1496:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1497:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1498:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1499:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1500:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1501:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1502:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1503:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1504:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1505:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1506:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1507:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1508:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1509:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1510:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1511:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1512:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1513:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1514:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1515:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1516:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1517:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1518:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1519:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1520:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1521:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1522:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1523:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1524:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1525:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1526:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1527:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1528:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1529:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1530:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1531:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1532:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1533:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1534:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1535:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1536:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1537:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1538:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1539:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1540:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1541:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1542:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1543:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1544:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1545:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1546:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1547:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1548:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1549:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1550:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1551:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1552:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1553:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1554:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1555:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1556:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1557:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1558:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1559:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1560:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1561:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1562:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1563:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1564:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1565:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1566:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1567:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1568:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1569:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1570:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1571:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1572:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1573:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1574:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1575:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1576:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1577:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1578:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1579:\n",
      "  first_0 Reward: -1\n",
      "  second_0 Reward: 1\n",
      "Step 1580:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1581:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1582:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1583:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1584:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1585:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1586:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1587:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1588:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1589:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1590:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1591:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1592:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1593:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1594:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1595:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1596:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1597:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1598:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1599:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1600:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1601:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1602:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1603:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1604:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1605:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1606:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1607:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1608:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1609:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1610:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1611:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1612:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1613:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1614:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1615:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1616:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1617:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1618:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1619:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1620:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1621:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1622:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1623:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1624:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1625:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1626:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1627:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1628:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1629:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1630:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1631:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1632:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1633:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1634:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1635:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1636:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1637:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1638:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1639:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1640:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1641:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1642:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1643:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1644:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1645:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1646:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1647:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1648:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1649:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1650:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1651:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1652:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1653:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1654:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1655:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1656:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1657:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1658:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1659:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1660:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1661:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1662:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1663:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1664:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1665:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1666:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1667:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1668:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1669:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1670:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1671:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1672:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1673:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1674:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1675:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1676:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1677:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1678:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1679:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1680:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1681:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1682:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1683:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1684:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1685:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1686:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1687:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1688:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1689:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1690:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1691:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1692:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1693:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1694:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1695:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1696:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1697:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1698:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1699:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1700:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1701:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1702:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1703:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1704:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1705:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1706:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1707:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1708:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1709:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1710:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1711:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1712:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1713:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1714:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1715:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1716:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1717:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1718:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1719:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1720:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1721:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1722:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1723:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1724:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1725:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1726:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1727:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1728:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1729:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1730:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1731:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1732:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1733:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1734:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1735:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1736:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1737:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1738:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1739:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1740:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1741:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1742:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1743:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1744:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1745:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1746:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1747:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1748:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1749:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1750:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1751:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1752:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1753:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1754:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1755:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1756:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1757:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1758:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1759:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1760:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1761:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1762:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1763:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1764:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1765:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1766:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1767:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1768:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1769:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1770:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1771:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1772:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1773:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1774:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1775:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1776:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1777:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1778:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1779:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1780:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1781:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1782:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1783:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1784:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1785:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1786:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1787:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1788:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1789:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1790:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1791:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1792:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1793:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1794:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1795:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1796:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1797:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1798:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1799:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1800:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1801:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1802:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1803:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1804:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1805:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1806:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1807:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1808:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1809:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1810:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1811:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1812:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1813:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1814:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1815:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1816:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1817:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1818:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1819:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1820:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1821:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1822:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1823:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1824:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1825:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1826:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1827:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1828:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1829:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1830:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1831:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1832:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1833:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1834:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1835:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1836:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1837:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1838:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1839:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1840:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1841:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1842:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1843:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1844:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1845:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1846:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1847:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1848:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1849:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1850:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1851:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1852:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1853:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1854:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1855:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1856:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1857:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1858:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1859:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1860:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1861:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1862:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1863:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1864:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1865:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1866:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1867:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1868:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1869:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1870:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1871:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1872:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1873:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1874:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1875:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1876:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1877:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1878:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1879:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1880:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1881:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1882:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1883:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1884:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1885:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1886:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1887:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1888:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1889:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1890:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1891:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1892:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1893:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1894:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1895:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1896:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1897:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1898:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1899:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1900:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1901:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1902:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1903:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1904:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1905:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1906:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1907:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1908:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1909:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1910:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1911:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1912:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1913:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1914:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1915:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1916:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1917:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1918:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1919:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1920:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1921:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1922:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1923:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1924:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1925:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1926:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1927:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1928:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1929:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1930:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1931:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1932:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1933:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1934:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1935:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1936:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1937:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1938:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1939:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1940:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1941:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1942:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1943:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1944:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1945:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1946:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1947:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1948:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1949:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1950:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1951:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1952:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1953:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1954:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1955:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1956:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1957:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1958:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1959:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1960:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1961:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1962:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1963:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1964:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1965:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1966:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1967:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1968:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1969:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1970:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1971:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1972:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1973:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1974:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1975:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1976:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1977:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1978:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1979:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1980:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1981:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1982:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1983:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1984:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1985:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1986:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1987:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1988:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1989:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1990:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1991:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1992:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1993:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1994:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1995:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1996:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1997:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1998:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 1999:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2000:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2001:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2002:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2003:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2004:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2005:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2006:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2007:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2008:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2009:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2010:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2011:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2012:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2013:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2014:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2015:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2016:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2017:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2018:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2019:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2020:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2021:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2022:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2023:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2024:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2025:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2026:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2027:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2028:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2029:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2030:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2031:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2032:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2033:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2034:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2035:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2036:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2037:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2038:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2039:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2040:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2041:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2042:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2043:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2044:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2045:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2046:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2047:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2048:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2049:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2050:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2051:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2052:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2053:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2054:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2055:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2056:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2057:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2058:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2059:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2060:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2061:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2062:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2063:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2064:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2065:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2066:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2067:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2068:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2069:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2070:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2071:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2072:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2073:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2074:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2075:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2076:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2077:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2078:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2079:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2080:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2081:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2082:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2083:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2084:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2085:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2086:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2087:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2088:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2089:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2090:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2091:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2092:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2093:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2094:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2095:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2096:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2097:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2098:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2099:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2100:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2101:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2102:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2103:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2104:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2105:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2106:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2107:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2108:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2109:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2110:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2111:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2112:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2113:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2114:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2115:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2116:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2117:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2118:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2119:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2120:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2121:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2122:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2123:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2124:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2125:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2126:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2127:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2128:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2129:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2130:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2131:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2132:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2133:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2134:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2135:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2136:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2137:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2138:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2139:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2140:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2141:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2142:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2143:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2144:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2145:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2146:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2147:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2148:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2149:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2150:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2151:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2152:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2153:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2154:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2155:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2156:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2157:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2158:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2159:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2160:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2161:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2162:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2163:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2164:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2165:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2166:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2167:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2168:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2169:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2170:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2171:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2172:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2173:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2174:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2175:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2176:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2177:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2178:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2179:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2180:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2181:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2182:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2183:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2184:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2185:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2186:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2187:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2188:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2189:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2190:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2191:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2192:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2193:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2194:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2195:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2196:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2197:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2198:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2199:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2200:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2201:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2202:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2203:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2204:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2205:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2206:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2207:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2208:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2209:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2210:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2211:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2212:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2213:\n",
      "  first_0 Reward: 1\n",
      "  second_0 Reward: -1\n",
      "Step 2214:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2215:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2216:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2217:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2218:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2219:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2220:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2221:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2222:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2223:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2224:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2225:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2226:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2227:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2228:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2229:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2230:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2231:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2232:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2233:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2234:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2235:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2236:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2237:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2238:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2239:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2240:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2241:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2242:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2243:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2244:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2245:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2246:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2247:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2248:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2249:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2250:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2251:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2252:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2253:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2254:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2255:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2256:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2257:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2258:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2259:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2260:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2261:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2262:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2263:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2264:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2265:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2266:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2267:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2268:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2269:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2270:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2271:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2272:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2273:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2274:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2275:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2276:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2277:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2278:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2279:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2280:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2281:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2282:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2283:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2284:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2285:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2286:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2287:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2288:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2289:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2290:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2291:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2292:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2293:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2294:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2295:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2296:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2297:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2298:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2299:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2300:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2301:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2302:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2303:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2304:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2305:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2306:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2307:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2308:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2309:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2310:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2311:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2312:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2313:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2314:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2315:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2316:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2317:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2318:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2319:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2320:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2321:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2322:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2323:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2324:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2325:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2326:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2327:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2328:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2329:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2330:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2331:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2332:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2333:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2334:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2335:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2336:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2337:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2338:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2339:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2340:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2341:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2342:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2343:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2344:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2345:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2346:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2347:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2348:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2349:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2350:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2351:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2352:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2353:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2354:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2355:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2356:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2357:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2358:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2359:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2360:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2361:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2362:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2363:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2364:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2365:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2366:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2367:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2368:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2369:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2370:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2371:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2372:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2373:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2374:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2375:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2376:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2377:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2378:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2379:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2380:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2381:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2382:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2383:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2384:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2385:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2386:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2387:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2388:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2389:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2390:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2391:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2392:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2393:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2394:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2395:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2396:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2397:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2398:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2399:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2400:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2401:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2402:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2403:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2404:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2405:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2406:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2407:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2408:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2409:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2410:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2411:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2412:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2413:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2414:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2415:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2416:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2417:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2418:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2419:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2420:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2421:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2422:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2423:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2424:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2425:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2426:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2427:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2428:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2429:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2430:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2431:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2432:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2433:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2434:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2435:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2436:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2437:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2438:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2439:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2440:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2441:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2442:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2443:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2444:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2445:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2446:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2447:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2448:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2449:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2450:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2451:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2452:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2453:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2454:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2455:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2456:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2457:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2458:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2459:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2460:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2461:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2462:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2463:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2464:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2465:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2466:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2467:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2468:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2469:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2470:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2471:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2472:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2473:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2474:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2475:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2476:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2477:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2478:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2479:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2480:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2481:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2482:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2483:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2484:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2485:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2486:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2487:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2488:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2489:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2490:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2491:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2492:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2493:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2494:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2495:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2496:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2497:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2498:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2499:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2500:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2501:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2502:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2503:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2504:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2505:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2506:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2507:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2508:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2509:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2510:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2511:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2512:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2513:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2514:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2515:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2516:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2517:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2518:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2519:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2520:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2521:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2522:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2523:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2524:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2525:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2526:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2527:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2528:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2529:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2530:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2531:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2532:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2533:\n",
      "  first_0 Reward: -1\n",
      "  second_0 Reward: 1\n",
      "Step 2534:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2535:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2536:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2537:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2538:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2539:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2540:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2541:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2542:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2543:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2544:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2545:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2546:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2547:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2548:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2549:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2550:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2551:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2552:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2553:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2554:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2555:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2556:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2557:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2558:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2559:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2560:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2561:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2562:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2563:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2564:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2565:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2566:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2567:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2568:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2569:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2570:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2571:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2572:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2573:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2574:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2575:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2576:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2577:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2578:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2579:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2580:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2581:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2582:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2583:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2584:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2585:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2586:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2587:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2588:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2589:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2590:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2591:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2592:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2593:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2594:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2595:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2596:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2597:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2598:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2599:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2600:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2601:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2602:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2603:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2604:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2605:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2606:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2607:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2608:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2609:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2610:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2611:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2612:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2613:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2614:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2615:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2616:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2617:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2618:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2619:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2620:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2621:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2622:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2623:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2624:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2625:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2626:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2627:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2628:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2629:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2630:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2631:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2632:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2633:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2634:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2635:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2636:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2637:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2638:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2639:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2640:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2641:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2642:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2643:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2644:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2645:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2646:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2647:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2648:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2649:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2650:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2651:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2652:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2653:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2654:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2655:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2656:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2657:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2658:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2659:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2660:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2661:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2662:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2663:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2664:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2665:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2666:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2667:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2668:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2669:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2670:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2671:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2672:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2673:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2674:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2675:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2676:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2677:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2678:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2679:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2680:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2681:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2682:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2683:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2684:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2685:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2686:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2687:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2688:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2689:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2690:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2691:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2692:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2693:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2694:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2695:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2696:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2697:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2698:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2699:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2700:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2701:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2702:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2703:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2704:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2705:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2706:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2707:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2708:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2709:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2710:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2711:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2712:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2713:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2714:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2715:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2716:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2717:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2718:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2719:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2720:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2721:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2722:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2723:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2724:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2725:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2726:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2727:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2728:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2729:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2730:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2731:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2732:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2733:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2734:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2735:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2736:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2737:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2738:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2739:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2740:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2741:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2742:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2743:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2744:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2745:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2746:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2747:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2748:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2749:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2750:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2751:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2752:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2753:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2754:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2755:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2756:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2757:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2758:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2759:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2760:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2761:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2762:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2763:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2764:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2765:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2766:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2767:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2768:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2769:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2770:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2771:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2772:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2773:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2774:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2775:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2776:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2777:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2778:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2779:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2780:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2781:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2782:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2783:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2784:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2785:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2786:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2787:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2788:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2789:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2790:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2791:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2792:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2793:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2794:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2795:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2796:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2797:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2798:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2799:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2800:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2801:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2802:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2803:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2804:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2805:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2806:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2807:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2808:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2809:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2810:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2811:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2812:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2813:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2814:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2815:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2816:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2817:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2818:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2819:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2820:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2821:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2822:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2823:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2824:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2825:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2826:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2827:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2828:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2829:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2830:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2831:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2832:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2833:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2834:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2835:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2836:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2837:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2838:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2839:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2840:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2841:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2842:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2843:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2844:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2845:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2846:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2847:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2848:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2849:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2850:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2851:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2852:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2853:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2854:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2855:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2856:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2857:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2858:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2859:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2860:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2861:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2862:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2863:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2864:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2865:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2866:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2867:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2868:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2869:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2870:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2871:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2872:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2873:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2874:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2875:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2876:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2877:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2878:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2879:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2880:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2881:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2882:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2883:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2884:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2885:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2886:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2887:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2888:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2889:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2890:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2891:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2892:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2893:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2894:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2895:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2896:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2897:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2898:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2899:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2900:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2901:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2902:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2903:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2904:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2905:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2906:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2907:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2908:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2909:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2910:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2911:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2912:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2913:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2914:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2915:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2916:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2917:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2918:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2919:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2920:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2921:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2922:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2923:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2924:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2925:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2926:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2927:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2928:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2929:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2930:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2931:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2932:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2933:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2934:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2935:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2936:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2937:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2938:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2939:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2940:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2941:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2942:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2943:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2944:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2945:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2946:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2947:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2948:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2949:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2950:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2951:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2952:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2953:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2954:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2955:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2956:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2957:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2958:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2959:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2960:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2961:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2962:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2963:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2964:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2965:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2966:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2967:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2968:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2969:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2970:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2971:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2972:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2973:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2974:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2975:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2976:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2977:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2978:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2979:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2980:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2981:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2982:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2983:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2984:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2985:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2986:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2987:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2988:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2989:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2990:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2991:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2992:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2993:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2994:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2995:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2996:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2997:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2998:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 2999:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Step 3000:\n",
      "  first_0 Reward: 0\n",
      "  second_0 Reward: 0\n",
      "Testing complete.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.atari import boxing_v2\n",
    "from supersuit import pad_observations_v0, pad_action_space_v0, resize_v1, normalize_obs_v0, dtype_v0\n",
    "\n",
    "# Initialize the environment with human rendering\n",
    "parallel_env = boxing_v2.parallel_env(render_mode=\"human\")\n",
    "\n",
    "# Preprocess the environment for compatibility with agents\n",
    "parallel_env = pad_observations_v0(parallel_env)\n",
    "parallel_env = pad_action_space_v0(parallel_env)\n",
    "parallel_env = resize_v1(parallel_env, 84, 84)\n",
    "parallel_env = dtype_v0(parallel_env, dtype=\"float32\")\n",
    "parallel_env = normalize_obs_v0(parallel_env, env_min=0, env_max=1)\n",
    "\n",
    "# Reset the environment\n",
    "obs = parallel_env.reset()\n",
    "\n",
    "# Run for one episode, max 100 steps\n",
    "max_steps = 3000\n",
    "step_count = 0\n",
    "done = {agent: False for agent in parallel_env.agents}  # Track done state for each agent\n",
    "\n",
    "while not all(done.values()) and step_count < max_steps:\n",
    "    actions = {}\n",
    "\n",
    "    # Sample random actions for each agent (replace with learned actions if available)\n",
    "    for agent in parallel_env.agents:\n",
    "        actions[agent] = parallel_env.action_space(agent).sample()  # Replace with actual agent's action if needed\n",
    "\n",
    "    # Step the environment\n",
    "    step_output = parallel_env.step(actions)\n",
    "\n",
    "    # Render the game\n",
    "    parallel_env.render()  # This should pop up the Atari game window\n",
    "\n",
    "    # Print rewards for debugging purposes\n",
    "    if isinstance(step_output, tuple):  # Handle if step output is a tuple\n",
    "        obs, rewards, dones, truncations, infos = step_output\n",
    "        done = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "    else:\n",
    "        obs, rewards, dones, infos = step_output\n",
    "\n",
    "    print(f\"Step {step_count + 1}:\")\n",
    "    for agent, reward in rewards.items():\n",
    "        print(f\"  {agent} Reward: {reward}\")\n",
    "\n",
    "    step_count += 1  # Increment step counter\n",
    "\n",
    "print(\"Testing complete.\")\n",
    "parallel_env.close()  # Close the rendering window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human rendering MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for human rendering\n",
    "env = boxing_v2.env(render_mode=\"human\")  # Set render_mode to \"human\" for faster real-time rendering\n",
    "env.reset(seed=42)\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "env = resize_v1(env, 84, 84)\n",
    "env = dtype_v0(env, dtype=\"float32\")\n",
    "env = normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "parallel_env = aec_to_parallel(env)\n",
    "\n",
    "obs_dim = 84 * 84\n",
    "action_dim = parallel_env.action_space(\"first_0\").n\n",
    "n_agents = 2\n",
    "maddpg = MADDPG(obs_dim, action_dim, n_agents)\n",
    "replay_buffer = ReplayBuffer(100000, obs_dim, action_dim, n_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06961506 -0.047357   -0.07245055  0.0267143   0.01113076 -0.00296758\n",
      "  -0.05201489 -0.05218387 -0.00808928 -0.04564978  0.03491726  0.00805301\n",
      "   0.0322105   0.04356951 -0.01945618 -0.04225711 -0.03891763 -0.03505651]]\n",
      "  Discrete Action: 13\n",
      "Step 1, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06961506 -0.047357   -0.07245055  0.0267143   0.01113076 -0.00296758\n",
      "  -0.05201489 -0.05218387 -0.00808928 -0.04564978  0.03491726  0.00805301\n",
      "   0.0322105   0.04356951 -0.01945618 -0.04225711 -0.03891763 -0.03505651]]\n",
      "  Discrete Action: 13\n",
      "Step 2, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06961506 -0.047357   -0.07245055  0.0267143   0.01113076 -0.00296758\n",
      "  -0.05201489 -0.05218387 -0.00808928 -0.04564978  0.03491726  0.00805301\n",
      "   0.0322105   0.04356951 -0.01945618 -0.04225711 -0.03891763 -0.03505651]]\n",
      "  Discrete Action: 13\n",
      "Step 2, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06961506 -0.047357   -0.07245055  0.0267143   0.01113076 -0.00296758\n",
      "  -0.05201489 -0.05218387 -0.00808928 -0.04564978  0.03491726  0.00805301\n",
      "   0.0322105   0.04356951 -0.01945618 -0.04225711 -0.03891763 -0.03505651]]\n",
      "  Discrete Action: 13\n",
      "Step 3, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07144735 -0.04567983 -0.07071189  0.02728899  0.01074657 -0.00292703\n",
      "  -0.05313633 -0.05237443 -0.01112458 -0.04225681  0.03419211  0.00909316\n",
      "   0.03273188  0.04542268 -0.01685477 -0.04210474 -0.03867995 -0.03546961]]\n",
      "  Discrete Action: 13\n",
      "Step 3, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07144735 -0.04567983 -0.07071189  0.02728899  0.01074657 -0.00292703\n",
      "  -0.05313633 -0.05237443 -0.01112458 -0.04225681  0.03419211  0.00909316\n",
      "   0.03273188  0.04542268 -0.01685477 -0.04210474 -0.03867995 -0.03546961]]\n",
      "  Discrete Action: 13\n",
      "Step 4, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07438145 -0.04756457 -0.07239563  0.02827621  0.01063319 -0.0008937\n",
      "  -0.05290464 -0.05704713 -0.01131945 -0.04236632  0.03737836  0.00833851\n",
      "   0.03301495  0.04643994 -0.01664281 -0.04205814 -0.03708383 -0.03400183]]\n",
      "  Discrete Action: 13\n",
      "Step 4, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07438145 -0.04756457 -0.07239563  0.02827621  0.01063319 -0.0008937\n",
      "  -0.05290464 -0.05704713 -0.01131945 -0.04236632  0.03737836  0.00833851\n",
      "   0.03301495  0.04643994 -0.01664281 -0.04205814 -0.03708383 -0.03400183]]\n",
      "  Discrete Action: 13\n",
      "Step 5, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0724528  -0.04679791 -0.07325179  0.02980675  0.01071883 -0.00015531\n",
      "  -0.05097797 -0.05633964 -0.01176028 -0.04296833  0.03553342  0.00914625\n",
      "   0.03404192  0.04789741 -0.0186907  -0.04140336 -0.03784606 -0.03233319]]\n",
      "  Discrete Action: 13\n",
      "Step 5, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0724528  -0.04679791 -0.07325179  0.02980675  0.01071883 -0.00015531\n",
      "  -0.05097797 -0.05633964 -0.01176028 -0.04296833  0.03553342  0.00914625\n",
      "   0.03404192  0.04789741 -0.0186907  -0.04140336 -0.03784606 -0.03233319]]\n",
      "  Discrete Action: 13\n",
      "Step 6, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07139824 -0.04621003 -0.07479752  0.02968065  0.00984267  0.00025037\n",
      "  -0.04953666 -0.05682722 -0.01006099 -0.04262163  0.03474553  0.01010849\n",
      "   0.03411308  0.0493696  -0.01902161 -0.04124719 -0.03830266 -0.03181067]]\n",
      "  Discrete Action: 13\n",
      "Step 6, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07139824 -0.04621003 -0.07479752  0.02968065  0.00984267  0.00025037\n",
      "  -0.04953666 -0.05682722 -0.01006099 -0.04262163  0.03474553  0.01010849\n",
      "   0.03411308  0.0493696  -0.01902161 -0.04124719 -0.03830266 -0.03181067]]\n",
      "  Discrete Action: 13\n",
      "Step 7, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0706041  -0.04285908 -0.07348143  0.03202886  0.01297809 -0.00188327\n",
      "  -0.05354417 -0.05057517 -0.00993141 -0.04059584  0.03300039  0.00940288\n",
      "   0.03583617  0.04669879 -0.01814242 -0.04264656 -0.03900213 -0.02922963]]\n",
      "  Discrete Action: 13\n",
      "Step 7, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0706041  -0.04285908 -0.07348143  0.03202886  0.01297809 -0.00188327\n",
      "  -0.05354417 -0.05057517 -0.00993141 -0.04059584  0.03300039  0.00940288\n",
      "   0.03583617  0.04669879 -0.01814242 -0.04264656 -0.03900213 -0.02922963]]\n",
      "  Discrete Action: 13\n",
      "Step 8, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07053563 -0.04015502 -0.07435396  0.03241976  0.01441288 -0.00321301\n",
      "  -0.05356174 -0.04904921 -0.00983167 -0.03847628  0.03376149  0.00927199\n",
      "   0.03431831  0.04614696 -0.01709002 -0.04177845 -0.03805882 -0.02893233]]\n",
      "  Discrete Action: 13\n",
      "Step 8, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.07053563 -0.04015502 -0.07435396  0.03241976  0.01441288 -0.00321301\n",
      "  -0.05356174 -0.04904921 -0.00983167 -0.03847628  0.03376149  0.00927199\n",
      "   0.03431831  0.04614696 -0.01709002 -0.04177845 -0.03805882 -0.02893233]]\n",
      "  Discrete Action: 13\n",
      "Step 9, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06546257 -0.03766733 -0.08025354  0.03651357  0.01215428  0.00112933\n",
      "  -0.05038535 -0.04922847 -0.00772899 -0.03873784  0.03459438  0.01140413\n",
      "   0.03322601  0.04594563 -0.01774178 -0.03919916 -0.03981799 -0.03166928]]\n",
      "  Discrete Action: 13\n",
      "Step 9, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06546257 -0.03766733 -0.08025354  0.03651357  0.01215428  0.00112933\n",
      "  -0.05038535 -0.04922847 -0.00772899 -0.03873784  0.03459438  0.01140413\n",
      "   0.03322601  0.04594563 -0.01774178 -0.03919916 -0.03981799 -0.03166928]]\n",
      "  Discrete Action: 13\n",
      "Step 10, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06712858 -0.03842551 -0.07662596  0.03421705  0.00979454 -0.00096518\n",
      "  -0.04961039 -0.0476722  -0.00908473 -0.0392539   0.03330468  0.01119073\n",
      "   0.03136779  0.04539753 -0.01727657 -0.03914364 -0.04106288 -0.03185641]]\n",
      "  Discrete Action: 13\n",
      "Step 10, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06712858 -0.03842551 -0.07662596  0.03421705  0.00979454 -0.00096518\n",
      "  -0.04961039 -0.0476722  -0.00908473 -0.0392539   0.03330468  0.01119073\n",
      "   0.03136779  0.04539753 -0.01727657 -0.03914364 -0.04106288 -0.03185641]]\n",
      "  Discrete Action: 13\n",
      "Step 11, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 11, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 12, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 12, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 13, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 13, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 14, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 14, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 15, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 15, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 16, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 16, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 17, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 17, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 18, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 18, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 19, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 19, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 20, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 20, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 21, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 21, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 22, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 22, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 23, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 23, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 24, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 24, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 25, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 25, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 26, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 26, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 27, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 27, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 28, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 28, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 29, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 29, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 30, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 30, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 31, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 31, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 32, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 32, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 33, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 33, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 34, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 34, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 35, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 35, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 36, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 36, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 37, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 37, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 38, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 38, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 39, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 39, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 40, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 40, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 41, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 41, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 42, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 42, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 43, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 43, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 44, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 44, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 45, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 45, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 46, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 46, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 47, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 47, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 48, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 48, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 49, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 49, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 50, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 50, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 51, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 51, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 52, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 52, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 53, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 53, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 54, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 54, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 55, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 55, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 56, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 56, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 57, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 57, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 58, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 58, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 59, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 59, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 60, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 60, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.0639024  -0.0379018  -0.07218461  0.03375131  0.00930673 -0.00674749\n",
      "  -0.05207377 -0.04769945 -0.01310553 -0.03749472  0.02939899  0.00901826\n",
      "   0.03382375  0.04597804 -0.01534732 -0.04738772 -0.03997899 -0.03132266]]\n",
      "  Discrete Action: 13\n",
      "Step 61, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 61, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 62, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 62, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 63, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 63, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 64, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 64, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 65, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 65, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 66, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 66, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 67, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 67, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 68, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 68, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 69, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 69, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 70, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 70, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 71, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 71, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 72, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 72, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 73, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 73, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 74, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 74, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 75, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 75, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 76, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 76, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 77, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 77, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 78, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 78, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 79, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 79, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 80, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 80, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 81, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 81, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 82, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 82, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 83, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 83, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 84, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 84, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 85, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 85, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 86, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 86, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 87, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 87, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 88, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 88, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 89, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 89, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 90, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 90, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 91, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 91, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 92, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 92, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 93, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 93, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 94, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 94, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 95, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 95, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 96, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 96, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 97, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 97, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 98, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 98, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 99, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 99, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 100, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 100, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 101, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 101, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 102, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 102, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 103, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 103, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 104, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 104, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 105, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 105, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 106, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 106, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 107, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 107, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 108, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 108, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 109, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 109, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 110, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 110, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 111, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 111, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 112, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 112, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 113, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 113, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 114, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 114, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 115, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 115, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 116, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 116, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 117, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 117, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 118, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 118, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 119, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 119, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 120, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 120, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 121, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 121, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 122, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 122, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 123, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 123, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 124, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 124, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 125, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 125, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 126, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 126, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 127, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 127, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 128, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 128, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 129, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 129, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 130, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 130, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 131, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 131, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 132, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 132, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 133, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 133, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 134, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 134, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 135, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 135, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 136, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 136, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 137, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 137, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 138, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 138, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 139, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 139, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 140, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 140, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 141, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 141, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 142, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 142, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 143, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 143, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 144, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 144, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 145, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 145, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 146, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 146, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 147, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 147, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 148, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 148, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 149, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 149, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 150, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 150, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 151, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 151, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 152, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 152, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 153, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 153, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 154, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 154, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 155, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 155, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 156, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 156, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 157, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 157, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 158, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 158, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 159, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 159, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 160, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 160, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 161, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 161, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 162, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 162, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 163, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 163, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 164, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 164, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 165, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 165, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 166, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 166, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 167, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 167, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 168, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 168, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 169, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 169, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 170, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 170, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 171, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 171, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 172, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 172, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 173, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 173, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 174, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 174, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 175, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 175, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 176, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 176, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 177, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 177, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 178, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 178, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 179, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 179, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 180, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 180, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06322255 -0.03789078 -0.07236872  0.03481968  0.00914542 -0.00627199\n",
      "  -0.0529319  -0.04791969 -0.0128728  -0.03717574  0.02888695  0.00814244\n",
      "   0.03266234  0.04622678 -0.01629566 -0.04697837 -0.03992938 -0.03131898]]\n",
      "  Discrete Action: 13\n",
      "Step 181, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 181, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 182, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 182, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 183, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 183, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 184, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 184, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 185, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 185, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 186, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 186, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 187, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 187, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 188, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 188, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 189, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 189, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 190, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 190, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 191, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 191, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 192, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 192, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 193, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 193, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 194, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 194, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 195, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 195, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 196, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 196, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 197, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 197, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 198, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 198, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 199, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 199, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 200, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 200, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 201, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 201, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 202, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 202, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 203, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 203, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 204, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 204, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 205, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 205, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 206, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 206, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 207, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 207, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 208, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 208, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 209, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 209, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 210, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 210, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 211, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 211, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 212, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 212, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 213, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 213, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 214, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 214, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 215, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 215, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 216, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 216, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 217, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 217, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 218, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 218, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 219, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 219, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 220, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 220, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 221, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 221, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 222, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 222, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 223, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 223, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 224, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 224, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 225, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 225, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 226, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 226, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 227, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 227, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 228, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 228, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 229, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 229, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 230, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 230, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 231, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 231, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 232, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 232, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 233, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 233, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 234, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 234, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 235, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 235, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 236, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 236, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 237, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 237, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 238, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 238, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 239, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 239, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 240, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 240, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06450234 -0.03749095 -0.07187153  0.03373417  0.0096268  -0.00678042\n",
      "  -0.05300439 -0.04777502 -0.01341263 -0.03747446  0.03011398  0.00850062\n",
      "   0.03372604  0.0459131  -0.0150741  -0.0471925  -0.04059105 -0.03056177]]\n",
      "  Discrete Action: 13\n",
      "Step 241, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 241, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 242, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 242, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 243, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 243, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 244, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 244, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 245, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 245, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 246, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 246, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 247, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 247, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 248, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 248, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 249, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 249, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 250, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 250, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 251, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 251, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 252, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 252, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 253, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 253, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 254, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 254, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 255, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 255, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 256, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 256, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 257, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 257, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 258, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 258, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 259, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 259, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 260, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 260, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 261, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 261, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 262, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 262, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 263, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 263, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 264, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 264, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 265, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 265, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 266, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 266, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 267, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 267, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 268, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 268, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 269, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 269, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 270, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 270, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 271, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 271, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 272, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 272, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 273, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 273, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 274, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 274, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 275, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 275, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 276, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 276, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 277, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 277, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 278, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 278, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 279, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 279, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 280, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 280, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 281, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 281, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 282, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 282, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 283, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 283, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 284, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 284, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 285, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 285, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 286, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 286, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 287, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 287, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 288, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 288, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 289, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 289, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 290, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 290, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 291, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 291, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 292, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 292, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 293, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 293, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 294, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 294, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 295, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 295, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 296, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 296, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 297, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 297, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 298, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 298, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 299, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 299, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 300, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 300, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06415991 -0.03757305 -0.07242221  0.03411556  0.00940752 -0.00648086\n",
      "  -0.05285501 -0.04807792 -0.01358885 -0.0376904   0.02977375  0.00874143\n",
      "   0.03396539  0.04553774 -0.0152841  -0.04706017 -0.04003979 -0.0308201 ]]\n",
      "  Discrete Action: 13\n",
      "Step 301, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 301, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 302, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 302, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 303, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 303, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 304, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 304, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 305, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 305, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 306, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 306, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 307, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 307, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 308, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 308, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 309, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 309, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 310, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 310, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 311, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 311, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 312, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 312, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 313, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 313, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 314, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 314, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 315, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 315, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 316, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 316, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 317, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 317, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 318, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 318, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 319, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 319, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 320, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 320, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 321, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 321, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 322, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 322, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 323, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 323, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 324, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 324, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 325, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 325, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 326, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 326, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 327, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 327, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 328, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 328, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 329, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 329, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 330, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 330, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 331, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 331, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 332, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 332, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 333, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 333, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 334, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 334, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 335, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 335, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 336, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 336, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 337, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 337, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 338, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 338, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 339, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 339, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 340, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 340, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 341, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 341, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 342, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 342, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 343, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 343, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 344, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 344, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 345, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 345, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 346, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 346, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 347, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 347, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 348, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 348, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 349, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 349, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 350, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 350, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 351, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 351, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 352, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 352, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 353, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 353, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 354, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 354, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 355, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 355, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 356, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 356, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 357, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 357, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 358, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 358, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 359, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 359, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 360, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 360, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06436845 -0.03705295 -0.07201928  0.03449237  0.00913875 -0.00614489\n",
      "  -0.05327714 -0.0479825  -0.01362188 -0.03734765  0.02982511  0.00893753\n",
      "   0.03347765  0.04575063 -0.01520511 -0.04758352 -0.03965015 -0.0302423 ]]\n",
      "  Discrete Action: 13\n",
      "Step 361, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 361, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 362, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 362, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 363, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 363, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 364, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 364, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 365, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 365, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 366, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 366, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 367, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 367, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 368, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 368, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 369, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 369, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 370, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 370, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 371, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 371, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 372, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 372, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 373, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 373, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 374, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 374, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 375, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 375, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 376, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 376, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 377, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 377, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 378, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 378, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 379, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 379, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 380, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 380, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 381, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 381, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 382, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 382, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 383, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 383, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 384, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 384, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 385, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 385, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 386, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 386, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 387, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 387, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 388, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 388, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 389, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 389, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 390, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 390, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 391, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 391, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 392, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 392, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 393, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 393, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 394, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 394, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 395, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 395, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 396, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 396, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 397, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 397, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 398, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 398, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 399, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 399, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 400, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 400, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 401, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 401, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 402, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 402, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 403, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 403, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 404, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 404, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 405, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 405, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 406, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 406, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 407, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 407, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 408, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 408, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 409, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 409, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 410, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 410, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 411, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 411, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 412, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 412, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 413, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 413, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 414, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 414, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 415, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 415, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 416, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 416, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 417, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 417, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 418, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 418, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 419, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 419, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 420, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 420, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06365313 -0.03773335 -0.07215104  0.03449026  0.00964119 -0.00638079\n",
      "  -0.05235417 -0.04818838 -0.01269241 -0.03772682  0.02921183  0.00855088\n",
      "   0.03346278  0.04618715 -0.01574564 -0.04718002 -0.03962369 -0.03183856]]\n",
      "  Discrete Action: 13\n",
      "Step 421, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 421, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 422, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 422, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 423, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 423, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 424, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 424, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 425, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 425, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 426, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 426, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 427, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 427, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 428, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 428, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 429, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 429, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 430, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 430, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 431, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 431, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 432, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 432, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 433, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 433, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 434, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 434, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 435, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 435, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 436, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 436, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 437, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 437, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 438, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 438, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 439, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 439, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 440, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 440, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 441, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 441, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 442, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 442, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 443, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 443, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 444, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 444, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 445, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 445, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 446, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 446, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 447, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 447, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 448, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 448, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 449, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 449, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 450, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 450, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 451, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 451, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 452, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 452, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 453, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 453, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 454, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 454, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 455, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 455, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 456, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 456, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 457, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 457, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 458, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 458, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 459, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 459, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 460, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 460, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 461, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 461, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 462, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 462, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 463, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 463, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 464, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 464, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 465, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 465, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 466, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 466, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 467, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 467, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 468, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 468, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 469, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 469, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 470, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 470, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 471, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 471, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 472, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 472, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 473, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 473, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 474, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 474, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 475, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 475, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 476, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 476, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 477, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 477, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 478, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 478, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 479, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 479, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 480, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 480, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06375753 -0.03767995 -0.0721099   0.03460238  0.00964879 -0.00679033\n",
      "  -0.05269555 -0.04749436 -0.01298506 -0.03718877  0.029898    0.00796411\n",
      "   0.03254519  0.04709812 -0.01562197 -0.04681681 -0.04007915 -0.03114123]]\n",
      "  Discrete Action: 13\n",
      "Step 481, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 481, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 482, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 482, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 483, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 483, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 484, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 484, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 485, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 485, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 486, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 486, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 487, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 487, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 488, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 488, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 489, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 489, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 490, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 490, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 491, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 491, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 492, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 492, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 493, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 493, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 494, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 494, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 495, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 495, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 496, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 496, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 497, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 497, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 498, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 498, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 499, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 499, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 500, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 500, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 501, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 501, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 502, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 502, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 503, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 503, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 504, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 504, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 505, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 505, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 506, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 506, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 507, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 507, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 508, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 508, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 509, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 509, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 510, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 510, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 511, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 511, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 512, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 512, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 513, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 513, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 514, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 514, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 515, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 515, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 516, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 516, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 517, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 517, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 518, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 518, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 519, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 519, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 520, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 520, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 521, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 521, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 522, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 522, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 523, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 523, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 524, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 524, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 525, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 525, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 526, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 526, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 527, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 527, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 528, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 528, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 529, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 529, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 530, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 530, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 531, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 531, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 532, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 532, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 533, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 533, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 534, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 534, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 535, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 535, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 536, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 536, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 537, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 537, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 538, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 538, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 539, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 539, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 540, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 540, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353168 -0.03697115 -0.07319646  0.03506742  0.00906473 -0.00600059\n",
      "  -0.05310541 -0.0484166  -0.01361332 -0.03763664  0.02992408  0.00883156\n",
      "   0.0330093   0.04591866 -0.01572368 -0.0470766  -0.03878723 -0.03020842]]\n",
      "  Discrete Action: 13\n",
      "Step 541, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 541, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 542, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 542, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 543, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 543, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 544, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 544, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 545, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 545, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 546, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 546, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 547, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 547, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 548, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 548, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 549, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 549, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 550, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 550, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 551, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 551, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 552, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 552, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 553, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 553, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 554, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 554, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 555, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 555, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 556, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 556, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 557, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 557, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 558, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 558, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 559, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 559, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 560, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 560, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 561, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 561, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 562, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 562, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 563, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 563, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 564, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 564, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 565, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 565, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 566, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 566, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 567, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 567, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 568, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 568, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 569, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 569, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 570, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 570, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 571, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 571, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 572, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 572, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 573, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 573, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 574, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 574, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 575, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 575, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 576, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 576, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 577, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 577, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 578, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 578, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 579, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 579, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 580, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 580, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 581, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 581, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 582, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 582, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 583, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 583, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 584, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 584, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 585, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 585, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 586, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 586, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 587, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 587, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 588, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 588, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 589, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 589, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 590, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 590, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 591, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 591, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 592, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 592, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 593, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 593, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 594, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 594, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 595, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 595, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 596, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 596, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 597, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 597, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 598, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 598, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 599, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 599, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 600, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 600, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06430211 -0.03766244 -0.07175622  0.03352593  0.00956872 -0.00702376\n",
      "  -0.05250105 -0.04744577 -0.01298702 -0.03739289  0.02979421  0.00874796\n",
      "   0.0335578   0.04619751 -0.0151057  -0.04750457 -0.04050618 -0.03094845]]\n",
      "  Discrete Action: 13\n",
      "Step 601, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 601, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 602, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 602, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 603, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 603, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 604, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 604, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 605, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 605, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 606, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 606, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 607, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 607, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 608, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 608, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 609, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 609, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 610, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 610, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 611, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 611, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 612, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 612, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 613, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 613, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 614, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 614, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 615, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 615, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 616, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 616, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 617, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 617, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 618, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 618, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 619, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 619, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 620, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 620, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 621, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 621, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 622, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 622, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 623, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 623, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 624, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 624, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 625, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 625, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 626, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 626, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 627, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 627, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 628, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 628, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 629, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 629, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 630, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 630, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 631, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 631, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 632, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 632, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 633, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 633, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 634, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 634, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 635, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 635, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 636, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 636, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 637, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 637, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 638, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 638, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 639, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 639, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 640, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 640, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 641, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 641, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 642, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 642, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 643, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 643, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 644, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 644, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 645, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 645, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 646, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 646, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 647, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 647, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 648, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 648, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 649, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 649, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 650, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 650, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 651, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 651, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 652, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 652, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 653, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 653, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 654, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 654, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 655, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 655, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 656, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 656, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 657, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 657, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 658, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 658, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 659, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 659, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 660, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 660, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06353764 -0.03860209 -0.07117213  0.03382456  0.00830455 -0.006127\n",
      "  -0.05229209 -0.04864135 -0.01300999 -0.03801804  0.02955761  0.00899856\n",
      "   0.03391331  0.04549666 -0.01510685 -0.04764816 -0.04009403 -0.03186999]]\n",
      "  Discrete Action: 13\n",
      "Step 661, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 661, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 662, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 662, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 663, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 663, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 664, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 664, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 665, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 665, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 666, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 666, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 667, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 667, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 668, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 668, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 669, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 669, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 670, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 670, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 671, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 671, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 672, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 672, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 673, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 673, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 674, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 674, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 675, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 675, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 676, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 676, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 677, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 677, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 678, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 678, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 679, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 679, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 680, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 680, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 681, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 681, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 682, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 682, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 683, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 683, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 684, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 684, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 685, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 685, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 686, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 686, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 687, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 687, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 688, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 688, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 689, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 689, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 690, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 690, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 691, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 691, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 692, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 692, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 693, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 693, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 694, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 694, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 695, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 695, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 696, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 696, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 697, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 697, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 698, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 698, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 699, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 699, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 700, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 700, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 701, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 701, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 702, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 702, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 703, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 703, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 704, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 704, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 705, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 705, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 706, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 706, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 707, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 707, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 708, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 708, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 709, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 709, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 710, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 710, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 711, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 711, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 712, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 712, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 713, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 713, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 714, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 714, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 715, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 715, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 716, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 716, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 717, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 717, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 718, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 718, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 719, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 719, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 720, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 720, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06396773 -0.03847253 -0.070554    0.03360917  0.00852764 -0.00636848\n",
      "  -0.05270898 -0.04830254 -0.01302408 -0.03761112  0.0299998   0.00862682\n",
      "   0.03367332  0.04590296 -0.01496606 -0.04758181 -0.04068396 -0.03142695]]\n",
      "  Discrete Action: 13\n",
      "Step 721, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 721, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 722, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 722, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 723, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 723, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 724, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 724, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 725, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 725, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 726, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 726, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 727, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 727, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 728, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 728, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 729, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 729, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 730, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 730, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 731, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 731, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 732, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 732, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 733, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 733, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 734, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 734, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 735, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 735, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 736, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 736, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 737, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 737, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 738, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 738, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 739, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 739, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 740, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 740, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 741, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 741, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 742, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 742, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 743, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 743, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 744, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 744, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 745, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 745, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 746, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 746, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 747, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 747, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 748, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 748, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 749, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 749, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 750, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 750, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 751, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 751, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 752, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 752, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 753, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 753, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 754, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 754, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 755, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 755, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 756, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 756, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 757, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 757, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 758, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 758, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 759, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 759, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 760, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 760, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 761, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 761, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 762, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 762, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 763, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 763, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 764, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 764, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 765, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 765, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 766, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 766, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 767, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 767, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 768, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 768, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 769, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 769, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 770, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 770, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 771, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 771, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 772, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 772, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 773, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 773, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 774, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 774, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 775, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 775, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 776, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 776, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 777, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 777, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 778, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 778, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 779, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 779, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 780, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 780, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06282394 -0.03858694 -0.07136344  0.03494293  0.00824298 -0.00563036\n",
      "  -0.05314163 -0.04873797 -0.012909   -0.03760705  0.02897851  0.00810528\n",
      "   0.03275348  0.04586763 -0.01598038 -0.04726816 -0.04011846 -0.03196873]]\n",
      "  Discrete Action: 13\n",
      "Step 781, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 781, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 782, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 782, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 783, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 783, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 784, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 784, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 785, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 785, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 786, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 786, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 787, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 787, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 788, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 788, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 789, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 789, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 790, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 790, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 791, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 791, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 792, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 792, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 793, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 793, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 794, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 794, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 795, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 795, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 796, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 796, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 797, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 797, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 798, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 798, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 799, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 799, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 800, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 800, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 801, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 801, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 802, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 802, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 803, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 803, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 804, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 804, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 805, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 805, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 806, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 806, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 807, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 807, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 808, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 808, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 809, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 809, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 810, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 810, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 811, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 811, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 812, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 812, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 813, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 813, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 814, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 814, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 815, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 815, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 816, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 816, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 817, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 817, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 818, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 818, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 819, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 819, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 820, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 820, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 821, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 821, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 822, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 822, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 823, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 823, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 824, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 824, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 825, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 825, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 826, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 826, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 827, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 827, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 828, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 828, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 829, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 829, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 830, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 830, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 831, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 831, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 832, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 832, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 833, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 833, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 834, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 834, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 835, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 835, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 836, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 836, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 837, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 837, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 838, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 838, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 839, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 839, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 840, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 840, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06409636 -0.03818625 -0.07074749  0.0337441   0.00857566 -0.00614941\n",
      "  -0.05331983 -0.04868264 -0.01333957 -0.03783232  0.03035428  0.00843751\n",
      "   0.03383023  0.04556928 -0.0149627  -0.04739638 -0.04070622 -0.03105892]]\n",
      "  Discrete Action: 13\n",
      "Step 841, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 841, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 842, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 842, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 843, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 843, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 844, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 844, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 845, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 845, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 846, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 846, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 847, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 847, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 848, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 848, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 849, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 849, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 850, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 850, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 851, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 851, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 852, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 852, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 853, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 853, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 854, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 854, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 855, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 855, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 856, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 856, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 857, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 857, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 858, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 858, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 859, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 859, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 860, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 860, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 861, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 861, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 862, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 862, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 863, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 863, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 864, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 864, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 865, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 865, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 866, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 866, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 867, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 867, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 868, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 868, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 869, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 869, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 870, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 870, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 871, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 871, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 872, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 872, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 873, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 873, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 874, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 874, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 875, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 875, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 876, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 876, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 877, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 877, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 878, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 878, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 879, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 879, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 880, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 880, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 881, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 881, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 882, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 882, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 883, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 883, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 884, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 884, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 885, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 885, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 886, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 886, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 887, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 887, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 888, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 888, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 889, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 889, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 890, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 890, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 891, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 891, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 892, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 892, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 893, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 893, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 894, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 894, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 895, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 895, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 896, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 896, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 897, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 897, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 898, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 898, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 899, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 899, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 900, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 900, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06374989 -0.03822611 -0.0713918   0.03413206  0.00843731 -0.00584642\n",
      "  -0.05314672 -0.04903411 -0.01346898 -0.03821838  0.03002046  0.00872277\n",
      "   0.03410352  0.04509182 -0.01508765 -0.04734955 -0.04012338 -0.03135096]]\n",
      "  Discrete Action: 13\n",
      "Step 901, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 901, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 902, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 902, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 903, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 903, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 904, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 904, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 905, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 905, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 906, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 906, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 907, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 907, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 908, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 908, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 909, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 909, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 910, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 910, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 911, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 911, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 912, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 912, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 913, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 913, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 914, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 914, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 915, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 915, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 916, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 916, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 917, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 917, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 918, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 918, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 919, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 919, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 920, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 920, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 921, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 921, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 922, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 922, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 923, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 923, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 924, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 924, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 925, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 925, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 926, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 926, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 927, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 927, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 928, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 928, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 929, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 929, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 930, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 930, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 931, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 931, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 932, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 932, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 933, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 933, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 934, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 934, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 935, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 935, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 936, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 936, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 937, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 937, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 938, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 938, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 939, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 939, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 940, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 940, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 941, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 941, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 942, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 942, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 943, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 943, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 944, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 944, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 945, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 945, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 946, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 946, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 947, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 947, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 948, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 948, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 949, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 949, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 950, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 950, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 951, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 951, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 952, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 952, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 953, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 953, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 954, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 954, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 955, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 955, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 956, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 956, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 957, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 957, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 958, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 958, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 959, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 959, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 960, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 960, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06397635 -0.03767939 -0.0709565   0.03445626  0.00810938 -0.00558041\n",
      "  -0.05350241 -0.04885548 -0.01348765 -0.03772126  0.03003495  0.00888017\n",
      "   0.03364217  0.04545525 -0.01506142 -0.04772682 -0.03982882 -0.03069985]]\n",
      "  Discrete Action: 13\n",
      "Step 961, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 961, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 962, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 962, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 963, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 963, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 964, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 964, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 965, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 965, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 966, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 966, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 967, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 967, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 968, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 968, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 969, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 969, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 970, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 970, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 971, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 971, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 972, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 972, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 973, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 973, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 974, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 974, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 975, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 975, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 976, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 976, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 977, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 977, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 978, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 978, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 979, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 979, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 980, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 980, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 981, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 981, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 982, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 982, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 983, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 983, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 984, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 984, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 985, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 985, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 986, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 986, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 987, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 987, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 988, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 988, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 989, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 989, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 990, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 990, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 991, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 991, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 992, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 992, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 993, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 993, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 994, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 994, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 995, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 995, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 996, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 996, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 997, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 997, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 998, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 998, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 999, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 999, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 1000, Agent first_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Step 1000, Agent second_0\n",
      "  Observation (flattened): [0.         0.         0.         0.         0.43398693 0.43398693\n",
      " 0.43398693 0.43398693 0.43398693 0.43398693]...\n",
      "  Continuous Action: [[-0.06331105 -0.03850305 -0.07102028  0.03460797  0.00863864 -0.00574065\n",
      "  -0.05262638 -0.0491719  -0.01271197 -0.03816283  0.02933696  0.00841817\n",
      "   0.03351643  0.04578225 -0.01552299 -0.04736537 -0.03978872 -0.03238055]]\n",
      "  Discrete Action: 13\n",
      "Episode 1 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "Episode duration: 33.35 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1  # Render one episode for visualization\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    start_time = time.time()  # Track start time\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "\n",
    "            # Convert continuous action to discrete action\n",
    "            discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "            # Debug: Print observation and action for the agent\n",
    "            print(f\"Step {step_count + 1}, Agent {agent}\")\n",
    "            print(f\"  Observation (flattened): {obs_preprocessed.flatten().numpy()[:10]}...\")  # Print first 10 values\n",
    "            print(f\"  Continuous Action: {continuous_action}\")\n",
    "            print(f\"  Discrete Action: {discrete_action}\")\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards for each agent\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Render the environment\n",
    "        parallel_env.render()\n",
    "\n",
    "        # Update observations and step count\n",
    "        obs = next_obs\n",
    "        step_count += 1\n",
    "\n",
    "    end_time = time.time()  # Track end time\n",
    "    total_duration = end_time - start_time\n",
    "\n",
    "    # Print cumulative reward and episode duration\n",
    "    print(f\"Episode {episode + 1} completed\")\n",
    "    for agent, reward in episode_reward.items():\n",
    "        print(f\"  {agent} Reward: {reward}\")\n",
    "    print(f\"Episode duration: {total_duration:.2f} seconds\")\n",
    "\n",
    "# Close the rendering window after the episode\n",
    "parallel_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to encourage exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = boxing_v2.env(render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "env = resize_v1(env, 84, 84)\n",
    "env = dtype_v0(env, dtype=\"float32\")\n",
    "env = normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "parallel_env = aec_to_parallel(env)\n",
    "\n",
    "obs_dim = 84 * 84\n",
    "action_dim = parallel_env.action_space(\"first_0\").n\n",
    "n_agents = 2\n",
    "maddpg = MADDPG(obs_dim, action_dim, n_agents)\n",
    "replay_buffer = ReplayBuffer(100000, obs_dim, action_dim, n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 14\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 13\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 0\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 10\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 12\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 10\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 17\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 9\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 15\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 10\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 17\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 0\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 10\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 15\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 14\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 14\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 15\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 12\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 12\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 13\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 1\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 9\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 17\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 1\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 13\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 13\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 17\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 15\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 0\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 14\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 15\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 12\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 15\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 9\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 10\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 17\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 13\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 14\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 9\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 9\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 14\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 14\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 13\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 10\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 17\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 1/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -8.205703409186256\n",
      "Episode 1/6 completed in 12.92 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 12\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 10\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 17\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 10\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 14\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 1\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 1\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 15\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 15\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 9\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 1\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 0\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 13\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 13\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 10\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 9\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 12\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 9\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 0\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 17\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 15\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 1\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 1\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 12\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 17\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 0\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 12\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 14\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 12\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 13\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 15\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 9\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 13\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 1\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 15\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 17\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 10\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 1\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 0\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 14\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 17\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 13\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 1\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 14\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 2/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -8.195226030139324\n",
      "Episode 2/6 completed in 12.93 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 13\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 17\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 17\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 9\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 1\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 15\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 14\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 13\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 17\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 14\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 15\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 13\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 15\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 15\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 15\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 10\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 0\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 12\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 9\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 14\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 9\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 9\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 12\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 10\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 17\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 12\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 12\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 9\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 14\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 17\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 17\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 14\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 12\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 14\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 1\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 12\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 15\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 13\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 15\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 12\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 9\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 14\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 12\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 14\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 3/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -8.041733008552075\n",
      "Episode 3/6 completed in 12.80 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 13\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 17\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 0\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 9\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 14\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 0\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 9\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 14\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 9\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 15\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 17\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 10\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 9\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 1\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 10\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 9\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 14\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 14\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 1\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 14\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 13\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 15\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 0\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 12\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 17\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 9\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 9\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 17\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 0\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 17\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 10\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 15\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 12\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 1\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 13\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 17\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 9\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 1\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 13\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 1\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 12\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 17\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 15\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 9\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 12\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 4/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -7.8870563211138\n",
      "Episode 4/6 completed in 12.60 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 12\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 9\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 0\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 10\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 14\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 10\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 12\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 1\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 9\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 0\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 15\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 10\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 12\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 10\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 17\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 9\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 13\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 10\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 10\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 10\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 13\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 1\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 1\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 0\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 1\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 9\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 9\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 14\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 9\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 15\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 9\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 0\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 17\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 14\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 12\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 0\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 9\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 14\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 0\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 17\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 0\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 17\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 9\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 15\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 17\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 5/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -7.667909485305671\n",
      "Episode 5/6 completed in 12.72 seconds\n",
      "Step 1 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 13\n",
      "Step 1 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 2 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 1\n",
      "Step 2 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 3 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 9\n",
      "Step 3 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 4 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 4 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 5 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 5 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 6 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 10\n",
      "Step 6 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 7 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 0\n",
      "Step 7 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 8 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 10\n",
      "Step 8 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 9 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 12\n",
      "Step 9 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 10 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 10 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 11 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 13\n",
      "Step 11 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 12 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 1\n",
      "Step 12 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 13 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 15\n",
      "Step 13 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 14 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 14\n",
      "Step 14 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 15 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 0\n",
      "Step 15 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 16 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 16 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 17 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 15\n",
      "Step 17 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 18 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 13\n",
      "Step 18 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 19 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 0\n",
      "Step 19 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 20 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 0\n",
      "Step 20 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 21 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 12\n",
      "Step 21 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 22 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 1\n",
      "Step 22 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 23 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 13\n",
      "Step 23 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 24 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 10\n",
      "Step 24 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 25 Actions:\n",
      "  first_0: Action 10\n",
      "  second_0: Action 17\n",
      "Step 25 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 26 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 1\n",
      "Step 26 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 27 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 1\n",
      "Step 27 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 28 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 1\n",
      "Step 28 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 29 Actions:\n",
      "  first_0: Action 1\n",
      "  second_0: Action 0\n",
      "Step 29 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 30 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 15\n",
      "Step 30 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 31 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 17\n",
      "Step 31 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 32 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 13\n",
      "Step 32 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 33 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 13\n",
      "Step 33 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 34 Actions:\n",
      "  first_0: Action 17\n",
      "  second_0: Action 14\n",
      "Step 34 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 35 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 9\n",
      "Step 35 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 36 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 1\n",
      "Step 36 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 37 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 15\n",
      "Step 37 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 38 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 38 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 39 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 15\n",
      "Step 39 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 40 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 1\n",
      "Step 40 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 41 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 0\n",
      "Step 41 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 42 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 13\n",
      "Step 42 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 43 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 15\n",
      "Step 43 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 44 Actions:\n",
      "  first_0: Action 14\n",
      "  second_0: Action 1\n",
      "Step 44 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 45 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 14\n",
      "Step 45 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 46 Actions:\n",
      "  first_0: Action 9\n",
      "  second_0: Action 13\n",
      "Step 46 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 47 Actions:\n",
      "  first_0: Action 13\n",
      "  second_0: Action 14\n",
      "Step 47 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 48 Actions:\n",
      "  first_0: Action 12\n",
      "  second_0: Action 14\n",
      "Step 48 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 49 Actions:\n",
      "  first_0: Action 15\n",
      "  second_0: Action 17\n",
      "Step 49 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Step 50 Actions:\n",
      "  first_0: Action 0\n",
      "  second_0: Action 9\n",
      "Step 50 Rewards:\n",
      "  first_0: Reward 0\n",
      "  second_0: Reward 0\n",
      "Episode 6/6 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -7.494835680693242\n",
      "Episode 6/6 completed in 12.73 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from numpy.random import normal\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 6\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 50  # maybe 30-45 seconds per 100 steps\n",
    "\n",
    "# Track total loss and rewards per episode\n",
    "total_loss_per_episode = []\n",
    "agent1_rewards = []  # Rewards for the first agent\n",
    "agent2_rewards = []  # Rewards for the second agent\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Start timing the episode\n",
    "    start_time = time.time()\n",
    "\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    episode_policy_loss = 0  # Accumulate policy loss\n",
    "    episode_value_loss = 0  # Accumulate value loss\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "        \n",
    "            # # Convert continuous action to discrete action\n",
    "            # discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            # actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "            # Use Gaussian noise for exploration\n",
    "            exploration_scale = max(0.1, 0.5 - (episode / num_episodes))  # Cap max exploration at 0.5 (anneal noise)\n",
    "            exploration_noise = normal(0, exploration_scale, size=continuous_action.shape)\n",
    "            noisy_action = continuous_action + exploration_noise\n",
    "\n",
    "            # Convert to discrete action\n",
    "            discrete_action = np.argmax(noisy_action) # Use noisy action\n",
    "            actions[agent] = discrete_action\n",
    "\n",
    "        # Print actions for debugging\n",
    "        print(f\"Step {step_count + 1} Actions:\")\n",
    "        for agent, action in actions.items():\n",
    "            print(f\"  {agent}: Action {action}\")\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards for each agent\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # Print rewards for debugging\n",
    "        print(f\"Step {step_count + 1} Rewards:\")\n",
    "        for agent, reward in rewards.items():\n",
    "            print(f\"  {agent}: Reward {reward}\")\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            episode_policy_loss += policy_loss\n",
    "            episode_value_loss += value_loss\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Record total loss for this episode\n",
    "    total_loss = episode_policy_loss + episode_value_loss\n",
    "    total_loss_per_episode.append(total_loss)\n",
    "\n",
    "    # Separate rewards for each agent\n",
    "    agent1_reward = episode_reward[parallel_env.agents[0]]\n",
    "    agent2_reward = episode_reward[parallel_env.agents[1]]\n",
    "\n",
    "    # Append rewards to lists\n",
    "    agent1_rewards.append(agent1_reward)\n",
    "    agent2_rewards.append(agent2_reward)\n",
    "\n",
    "    # Print cumulative reward and total loss for the episode\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "    print(f\"  {parallel_env.agents[0]} Reward: {agent1_reward}\")\n",
    "    print(f\"  {parallel_env.agents[1]} Reward: {agent2_reward}\")\n",
    "    print(f\"  Total Loss: {total_loss}\")\n",
    "\n",
    "    # End timing the episode\n",
    "    end_time = time.time()\n",
    "    episode_duration = end_time - start_time\n",
    "\n",
    "    # Print the duration\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed in {episode_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't print actions this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = boxing_v2.env(render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "env = resize_v1(env, 84, 84)\n",
    "env = dtype_v0(env, dtype=\"float32\")\n",
    "env = normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "parallel_env = aec_to_parallel(env)\n",
    "\n",
    "obs_dim = 84 * 84\n",
    "action_dim = parallel_env.action_space(\"first_0\").n\n",
    "n_agents = 2\n",
    "maddpg = MADDPG(obs_dim, action_dim, n_agents)\n",
    "replay_buffer = ReplayBuffer(100000, obs_dim, action_dim, n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 22.048347901866236\n",
      "Episode 1/10 completed in 9.49 seconds\n",
      "Episode 2/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -5.493639854377463\n",
      "Episode 2/10 completed in 24.34 seconds\n",
      "Episode 3/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -6.244839262628492\n",
      "Episode 3/10 completed in 24.26 seconds\n",
      "Episode 4/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -5.6585875240767045\n",
      "Episode 4/10 completed in 24.63 seconds\n",
      "Episode 5/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -5.4077127632015625\n",
      "Episode 5/10 completed in 24.29 seconds\n",
      "Episode 6/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -5.185183964824709\n",
      "Episode 6/10 completed in 24.55 seconds\n",
      "Episode 7/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -4.972004757929962\n",
      "Episode 7/10 completed in 24.36 seconds\n",
      "Episode 8/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -4.770348158821928\n",
      "Episode 8/10 completed in 24.22 seconds\n",
      "Episode 9/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -4.57461678711239\n",
      "Episode 9/10 completed in 24.24 seconds\n",
      "Episode 10/10 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: -4.392311348544048\n",
      "Episode 10/10 completed in 24.27 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from numpy.random import normal\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 10\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 100  # maybe 30-45 seconds per 100 steps\n",
    "\n",
    "# Track total loss and rewards per episode\n",
    "total_loss_per_episode = []\n",
    "agent1_rewards = []  # Rewards for the first agent\n",
    "agent2_rewards = []  # Rewards for the second agent\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Start timing the episode\n",
    "    start_time = time.time()\n",
    "\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    episode_policy_loss = 0  # Accumulate policy loss\n",
    "    episode_value_loss = 0  # Accumulate value loss\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "        \n",
    "            # # Convert continuous action to discrete action\n",
    "            # discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            # actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "            # Use Gaussian noise for exploration\n",
    "            exploration_scale = max(0.1, 0.5 - (episode / num_episodes))  # Cap max exploration at 0.5 (anneal noise)\n",
    "            exploration_noise = normal(0, exploration_scale, size=continuous_action.shape)\n",
    "            noisy_action = continuous_action + exploration_noise\n",
    "\n",
    "            # Convert to discrete action\n",
    "            discrete_action = np.argmax(noisy_action) # Use noisy action\n",
    "            actions[agent] = discrete_action\n",
    "\n",
    "        # # Print actions for debugging\n",
    "        # print(f\"Step {step_count + 1} Actions:\")\n",
    "        # for agent, action in actions.items():\n",
    "        #     print(f\"  {agent}: Action {action}\")\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards for each agent\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # # Print rewards for debugging\n",
    "        # print(f\"Step {step_count + 1} Rewards:\")\n",
    "        # for agent, reward in rewards.items():\n",
    "        #     print(f\"  {agent}: Reward {reward}\")\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            episode_policy_loss += policy_loss\n",
    "            episode_value_loss += value_loss\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Record total loss for this episode\n",
    "    total_loss = episode_policy_loss + episode_value_loss\n",
    "    total_loss_per_episode.append(total_loss)\n",
    "\n",
    "    # Separate rewards for each agent\n",
    "    agent1_reward = episode_reward[parallel_env.agents[0]]\n",
    "    agent2_reward = episode_reward[parallel_env.agents[1]]\n",
    "\n",
    "    # Append rewards to lists\n",
    "    agent1_rewards.append(agent1_reward)\n",
    "    agent2_rewards.append(agent2_reward)\n",
    "\n",
    "    # Print cumulative reward and total loss for the episode\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "    print(f\"  {parallel_env.agents[0]} Reward: {agent1_reward}\")\n",
    "    print(f\"  {parallel_env.agents[1]} Reward: {agent2_reward}\")\n",
    "    print(f\"  Total Loss: {total_loss}\")\n",
    "\n",
    "    # End timing the episode\n",
    "    end_time = time.time()\n",
    "    episode_duration = end_time - start_time\n",
    "\n",
    "    # Print the duration\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed in {episode_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot total loss per episode\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_loss_per_episode, label='Total Loss', color='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss Per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot rewards for both agents\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(agent1_rewards, label=parallel_env.agents[0], color='green')\n",
    "plt.plot(agent2_rewards, label=parallel_env.agents[1], color='orange')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward Per Episode for Each Agent')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5 completed\n",
      "  first_0 Reward: -1.0\n",
      "  second_0 Reward: 1.0\n",
      "  Total Loss: 111.26843984736121\n",
      "Episode 1/5 completed in 236.11 seconds\n",
      "Episode 2/5 completed\n",
      "  first_0 Reward: -1.0\n",
      "  second_0 Reward: 1.0\n",
      "  Total Loss: 99.66088870690697\n",
      "Episode 2/5 completed in 247.33 seconds\n",
      "Episode 3/5 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 74.32515855293357\n",
      "Episode 3/5 completed in 242.56 seconds\n",
      "Episode 4/5 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 29.26389689889274\n",
      "Episode 4/5 completed in 244.52 seconds\n",
      "Episode 5/5 completed\n",
      "  first_0 Reward: 0.0\n",
      "  second_0 Reward: 0.0\n",
      "  Total Loss: 12.344461413360754\n",
      "Episode 5/5 completed in 244.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from numpy.random import normal\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 5\n",
    "batch_size = 64\n",
    "max_steps_per_episode = 1000  # maybe 30-45 seconds per 100 steps\n",
    "\n",
    "# Track total loss and rewards per episode\n",
    "total_loss_per_episode = []\n",
    "agent1_rewards = []  # Rewards for the first agent\n",
    "agent2_rewards = []  # Rewards for the second agent\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Start timing the episode\n",
    "    start_time = time.time()\n",
    "\n",
    "    obs = parallel_env.reset()\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]  # Extract observations if returned as a tuple\n",
    "\n",
    "    done = {agent: False for agent in parallel_env.agents}\n",
    "    episode_reward = defaultdict(float)  # Store cumulative reward for each agent\n",
    "\n",
    "    step_count = 0  # Initialize step counter\n",
    "    episode_policy_loss = 0  # Accumulate policy loss\n",
    "    episode_value_loss = 0  # Accumulate value loss\n",
    "\n",
    "    while not all(done.values()) and step_count < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observation\n",
    "            obs_preprocessed = torch.tensor(obs[agent], dtype=torch.float32)\n",
    "            if len(obs_preprocessed.shape) > 2:  # Ensure grayscale\n",
    "                obs_preprocessed = obs_preprocessed.mean(axis=-1)  # Convert RGB to grayscale\n",
    "            obs_preprocessed = obs_preprocessed.flatten().unsqueeze(0)  # Flatten and add batch dim\n",
    "\n",
    "            # Get continuous action from Actor\n",
    "            continuous_action = maddpg.agents[int(agent.split('_')[1])].actor(obs_preprocessed).detach().numpy()\n",
    "        \n",
    "            # # Convert continuous action to discrete action\n",
    "            # discrete_action = np.argmax(continuous_action)  # Take the action with the highest probability\n",
    "            # actions[agent] = discrete_action  # Store the discrete action for the agent\n",
    "\n",
    "            # Use Gaussian noise for exploration\n",
    "            exploration_scale = max(0.1, 0.5 - (episode / num_episodes))  # Cap max exploration at 0.5 (anneal noise)\n",
    "            exploration_noise = normal(0, exploration_scale, size=continuous_action.shape)\n",
    "            noisy_action = continuous_action + exploration_noise\n",
    "\n",
    "            # Convert to discrete action\n",
    "            discrete_action = np.argmax(noisy_action) # Use noisy action\n",
    "            actions[agent] = discrete_action\n",
    "\n",
    "        # # Print actions for debugging\n",
    "        # print(f\"Step {step_count + 1} Actions:\")\n",
    "        # for agent, action in actions.items():\n",
    "        #     print(f\"  {agent}: Action {action}\")\n",
    "\n",
    "        # One-hot encode actions for storage\n",
    "        actions_one_hot = np.zeros((len(parallel_env.agents), action_dim))\n",
    "        for idx, agent in enumerate(parallel_env.agents):\n",
    "            actions_one_hot[idx, actions[agent]] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "\n",
    "        if isinstance(step_output, tuple):  # Handle cases where step returns a tuple\n",
    "            next_obs, rewards, dones, truncations, infos = step_output\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_obs, rewards, dones, infos = step_output\n",
    "\n",
    "        # Accumulate rewards for each agent\n",
    "        for agent, reward in rewards.items():\n",
    "            episode_reward[agent] += reward\n",
    "\n",
    "        # # Print rewards for debugging\n",
    "        # print(f\"Step {step_count + 1} Rewards:\")\n",
    "        # for agent, reward in rewards.items():\n",
    "        #     print(f\"  {agent}: Reward {reward}\")\n",
    "\n",
    "        # Store data in the replay buffer\n",
    "        obs_array = []\n",
    "        next_obs_array = []\n",
    "\n",
    "        for agent in parallel_env.agents:\n",
    "            # Preprocess observations for storage\n",
    "            obs_processed = obs[agent].mean(axis=-1).flatten() if len(obs[agent].shape) > 2 else obs[agent].flatten()\n",
    "            next_obs_processed = next_obs[agent].mean(axis=-1).flatten() if len(next_obs[agent].shape) > 2 else next_obs[agent].flatten()\n",
    "            obs_array.append(obs_processed)\n",
    "            next_obs_array.append(next_obs_processed)\n",
    "\n",
    "        replay_buffer.store(\n",
    "            np.array(obs_array),\n",
    "            actions_one_hot,  # Use one-hot encoded actions\n",
    "            np.array([rewards[agent] for agent in parallel_env.agents]),\n",
    "            np.array(next_obs_array),\n",
    "            np.array([dones[agent] for agent in parallel_env.agents]),\n",
    "        )\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if replay_buffer.size >= batch_size:\n",
    "            policy_loss, value_loss = maddpg.update(replay_buffer, batch_size)\n",
    "            episode_policy_loss += policy_loss\n",
    "            episode_value_loss += value_loss\n",
    "\n",
    "        step_count += 1  # Increment step count\n",
    "\n",
    "    # Record total loss for this episode\n",
    "    total_loss = episode_policy_loss + episode_value_loss\n",
    "    total_loss_per_episode.append(total_loss)\n",
    "\n",
    "    # Separate rewards for each agent\n",
    "    agent1_reward = episode_reward[parallel_env.agents[0]]\n",
    "    agent2_reward = episode_reward[parallel_env.agents[1]]\n",
    "\n",
    "    # Append rewards to lists\n",
    "    agent1_rewards.append(agent1_reward)\n",
    "    agent2_rewards.append(agent2_reward)\n",
    "\n",
    "    # Print cumulative reward and total loss for the episode\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed\")\n",
    "    print(f\"  {parallel_env.agents[0]} Reward: {agent1_reward}\")\n",
    "    print(f\"  {parallel_env.agents[1]} Reward: {agent2_reward}\")\n",
    "    print(f\"  Total Loss: {total_loss}\")\n",
    "\n",
    "    # End timing the episode\n",
    "    end_time = time.time()\n",
    "    episode_duration = end_time - start_time\n",
    "\n",
    "    # Print the duration\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed in {episode_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/0AAAIjCAYAAABRfHuLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACKGUlEQVR4nOzdeVxUhf7G8WeGHRVxAXHB3dxyKU3DXBHBJUtTc80l07Js0Zar95amlVYuLV7LlutWmuaalSKokPuSZoupue8o5oKCwgDn9wcxv0hQUODMwOf9evGKOXPmzHP4MubjOXPGYhiGIQAAAAAAUOBYzQ4AAAAAAADyBqUfAAAAAIACitIPAAAAAEABRekHAAAAAKCAovQDAAAAAFBAUfoBAAAAACigKP0AAAAAABRQlH4AAAAAAAooSj8AAAAAAAUUpR8AkK+io6NlsVgUHR1tdhSHYLFY9Prrr5sdI9/Nnj1bFotFR48ezdfnze2fd3Jysl555RUFBgbKarWqS5cuubZtR5H+ml28eLHZUQAAt4HSDwCFgMViydZXdor4hAkTtHz58jzPnF4K079cXV1Vvnx5DRw4UKdOncrz53ckR48ezfCzcHFxUcWKFdW1a1ft3r07z58/vfRl9bVgwYI8z+CoZs6cqUmTJql79+6aM2eORowYkafP17p16yznUKtWrTx97tzw0UcfyWKxqGnTpmZHydRHH32k2bNnmx0DAHKVq9kBAAB574svvshwe+7cuYqMjLxhee3atW+5rQkTJqh79+75dkRz/PjxqlKliq5fv66tW7dq9uzZ2rhxo3777Td5enrmSwZH0bt3b3Xs2FEpKSnau3evPv74Y61atUpbt25Vw4YN8/z5n3vuOd133303LA8KCsrxth577DH16tVLHh4euRHNNOvWrVP58uX13nvv5dtzVqhQQRMnTrxhefHixfMtw+2aN2+eKleurO3bt+vgwYOqXr262ZEy+Oijj1S6dGkNHDjQ7CgAkGso/QBQCPTr1y/D7a1btyoyMvKG5Y6oQ4cOaty4sSTpiSeeUOnSpfXOO+9oxYoVevTRR01Od2vx8fEqUqRIrmzr3nvvzTCzBx54QA899JA+/vhjffLJJ3e07ezkbNGihbp3735Hz5POxcVFLi4uubItM507d06+vr65tr3U1FQlJSXd9B+0ihcv7hSv3X86cuSINm/erKVLl+rJJ5/UvHnzNHbsWLNjAUCBx+n9AABJaaXvxRdfVGBgoDw8PFSzZk1NnjxZhmHY17FYLIqPj9ecOXPspxSnHxE7duyYnn76adWsWVNeXl4qVaqUevTokevv2W7RooUk6dChQxmW79u3T927d1fJkiXl6empxo0ba8WKFfb7L126JBcXF3344Yf2ZefPn5fValWpUqUy7OewYcMUEBBgv71hwwb16NFDFStWlIeHhwIDAzVixAhdu3YtQ4aBAweqaNGiOnTokDp27KhixYqpb9++kqTExESNGDFCfn5+KlasmB566CGdPHnyjn4WwcHBktLKVLpt27apffv2Kl68uLy9vdWqVStt2rQpw+Nef/11WSwW/f777+rTp49KlCih5s2b31GWdBaLRcOHD9e8efNUs2ZNeXp6qlGjRlq/fn2G9TJ7T/+PP/6osLAwlS5dWl5eXqpSpYoef/zxDI/Lzu+plLOf96lTp/T444+rTJky8vDwUN26dTVz5syb7mf6Wy6ioqK0Z8+eG94ik92cf/951a1bVx4eHgoPD7/pc2dHTl6Ply5d0ogRI1S5cmV5eHioQoUK6t+/v86fP59hvdTUVL311luqUKGCPD091bZtWx08eDDbmebNm6cSJUqoU6dO6t69u+bNm5fpen/++acee+wx+fj4yNfXVwMGDNDPP/8si8Vyw6n3t3rdS///u7Zp0yaNHDlSfn5+KlKkiLp27arY2Fj7epUrV9aePXv0ww8/2OfZunXrbO8fADgqjvQDAGQYhh566CFFRUVp8ODBatiwoVavXq2XX35Zp06dsp+6/MUXX+iJJ55QkyZNNHToUElStWrVJEk7duzQ5s2b1atXL1WoUEFHjx7Vxx9/rNatW+v333+Xt7d3rmRNLy0lSpSwL9uzZ48eeOABlS9fXqNGjVKRIkX09ddfq0uXLlqyZIm6du0qX19f3X333Vq/fr2ee+45SdLGjRtlsVh04cIF/f7776pbt66ktJKf/o8LkrRo0SIlJCRo2LBhKlWqlLZv365p06bp5MmTWrRoUYZ8ycnJCgsLU/PmzTV58mT7fj/xxBP68ssv1adPHzVr1kzr1q1Tp06d7uhnkf4PH6VKlZKUdqp5hw4d1KhRI40dO1ZWq1WzZs1ScHCwNmzYoCZNmmR4fI8ePVSjRg1NmDDhhjKamStXrtxQBNOf32Kx2G//8MMPWrhwoZ577jl5eHjoo48+Uvv27bV9+3bdfffdmW773LlzCg0NlZ+fn0aNGiVfX18dPXpUS5cuta+T3d9TKfs/77Nnz+r++++3l28/Pz+tWrVKgwcPVlxcnF544YVM8/r5+emLL77QW2+9patXr9pPt69du3aOckppc/v66681fPhwlS5dWpUrV85yBpKUkpKS6Ry8vLzsZ2tk9/V49epVtWjRQnv37tXjjz+ue++9V+fPn9eKFSt08uRJlS5d2r79t99+W1arVS+99JIuX76sd999V3379tW2bdtumjfdvHnz9Mgjj8jd3V29e/fWxx9/rB07dmR4y0hqaqo6d+6s7du3a9iwYapVq5a++eYbDRgw4IbtZed1/3fPPvusSpQoobFjx+ro0aN6//33NXz4cC1cuFCS9P777+vZZ59V0aJF9Z///EeSVKZMmWztGwA4NAMAUOg888wzxt//F7B8+XJDkvHmm29mWK979+6GxWIxDh48aF9WpEgRY8CAATdsMyEh4YZlW7ZsMSQZc+fOtS+LiooyJBlRUVE3zThr1ixDkrFmzRojNjbWOHHihLF48WLDz8/P8PDwME6cOGFft23btka9evWM69ev25elpqYazZo1M2rUqJFhv8uUKWO/PXLkSKNly5aGv7+/8fHHHxuGYRh//vmnYbFYjA8++OCm+zZx4kTDYrEYx44dsy8bMGCAIckYNWpUhnV3795tSDKefvrpDMv79OljSDLGjh1705/FkSNHDEnGuHHjjNjYWCMmJsaIjo427rnnHkOSsWTJEiM1NdWoUaOGERYWZqSmpmbIXqVKFaNdu3b2ZWPHjjUkGb17977p86ZLn1lWX2fOnLGvm77sxx9/tC87duyY4enpaXTt2tW+LH2+R44cMQzDMJYtW2ZIMnbs2JFljuz+nubk5z148GCjbNmyxvnz5zOs26tXL6N48eKZzv7vWrVqZdStW/e2chpG2s/LarUae/bsuenz/P35sprDk08+aV8vu6/HMWPGGJKMpUuX3rB++u9R+vxr165tJCYm2u//4IMPDEnGr7/+esvcP/74oyHJiIyMtG+7QoUKxvPPP59hvSVLlhiSjPfff9++LCUlxQgODjYkGbNmzbIvz+7rPv13LSQkJMNrY8SIEYaLi4tx6dIl+7K6desarVq1uuX+AIAz4fR+AIBWrlwpFxcX+xHwdC+++KIMw9CqVatuuQ0vLy/79zabTX/++aeqV68uX19f7dq167azhYSEyM/PT4GBgerevbuKFCmiFStWqEKFCpKkCxcuaN26dXr00UftR6LPnz+vP//8U2FhYTpw4ID9av8tWrTQ2bNntX//fklpR/RbtmypFi1aaMOGDZLSjv4bhpHhSP/f9y0+Pl7nz59Xs2bNZBiGfvrppxsyDxs2LMPtlStXStINP9+sjiJnZezYsfLz81NAQIBat26tQ4cO6Z133tEjjzyi3bt368CBA+rTp4/+/PNP+88hPj5ebdu21fr165Wampphe0899VSOnn/MmDGKjIy84atkyZIZ1gsKClKjRo3stytWrKiHH35Yq1evVkpKSqbbTn9f/HfffSebzZbpOtn9Pc3uz9swDC1ZskSdO3eWYRj2n9n58+cVFhamy5cv39bvbk5fT61atVKdOnWyvf3KlStnOoe/7192X49LlixRgwYNbjgqLinD2RuSNGjQILm7u9tvp79GDh8+fMvM8+bNU5kyZdSmTRv7tnv27KkFCxZk+J0IDw+Xm5ubhgwZYl9mtVr1zDPPZNheTl736YYOHZphn1q0aKGUlBQdO3bslvkBwJlxej8AQMeOHVO5cuVUrFixDMvTr+afnb8UX7t2TRMnTtSsWbN06tSpDKeLX758+bazTZ8+XXfddZcuX76smTNnav369Rmu+H7w4EEZhqHXXntNr732WqbbOHfunMqXL28vKRs2bFCFChX0008/6c0335Sfn58mT55sv8/Hx0cNGjSwP/748eMaM2aMVqxYoYsXL2bY9j/3zdXV1f4PEumOHTsmq9VqfytEupo1a+boZzF06FD16NFDVqtVvr6+9veAS9KBAwckKdPToP+e9e9vi6hSpUqOnr9evXoKCQm55Xo1atS4Ydldd92lhIQExcbGZrheQrpWrVqpW7duGjdunN577z21bt1aXbp0UZ8+fez7mN3f0+z+vGNjY3Xp0iV9+umn+vTTTzPdl3Pnzt1yf/8pp6+nnM6hSJEit5xDdl+Phw4dUrdu3bL1vBUrVsxwO/136Z+viX9KSUnRggUL1KZNmwzXn2jatKmmTJmitWvXKjQ0VFLaz6Zs2bI3vB3on1f5z8nr/k7zA4Czo/QDAHLFs88+q1mzZumFF15QUFCQihcvLovFol69et1whDknmjRpYr96f5cuXdS8eXP16dNH+/fvV9GiRe3bfumllxQWFpbpNtILQ7ly5VSlShWtX79elStXlmEYCgoKkp+fn55//nkdO3ZMGzZsULNmzWS1pp0Ml5KSonbt2unChQv617/+pVq1aqlIkSI6deqUBg4ceMO+eXh42B+b22rUqJFl2UvPMWnSpCw/vq9o0aIZbv/9aLDZLBaLFi9erK1bt+rbb7/V6tWr9fjjj2vKlCnaunXrDdlzQ/rPrF+/fln+Y0n9+vVz/Xn/KS/mkBevx6w+bcG4xfUg1q1bpzNnzmjBggVasGDBDffPmzfPXvqzKyev+3S3mx8AnB2lHwCgSpUqac2aNbpy5UqGo5P79u2z35/un6f8plu8eLEGDBigKVOm2Jddv35dly5dyrWcLi4umjhxotq0aaP//ve/GjVqlKpWrSpJcnNzy9ZR6BYtWmj9+vWqUqWKGjZsqGLFiqlBgwYqXry4wsPDtWvXLo0bN86+/q+//qo//vhDc+bMUf/+/e3LIyMjs527UqVKSk1N1aFDhzIcbU5/m0FuSD+q7ePjk62fQ15KP+vg7/744w95e3vLz8/vpo+9//77df/99+utt97S/Pnz1bdvXy1YsEBPPPFEtn9Ps/vzTr+yf0pKSq7+zHLyesor2X09VqtWTb/99lueZpk3b578/f01ffr0G+5bunSpli1bphkzZsjLy0uVKlVSVFSUEhISMhzt/+enBOT0dZ9dWf35BgDOjPf0AwDUsWNHpaSk6L///W+G5e+9954sFos6dOhgX1akSJFMi7yLi8sNR8ymTZuW5Xu4b1fr1q3VpEkTvf/++7p+/br8/f3VunVrffLJJzpz5swN6//9I7mktNJ/9OhRLVy40H66v9VqVbNmzTR16lTZbLYM7+dPPzr4930zDEMffPBBtjOn//z+/nGBUtrVwnNLo0aNVK1aNU2ePFlXr1694f5//hzy0pYtWzK8b/zEiRP65ptvFBoamuXR1osXL97w+5N+xkJiYqKk7P+eZvfn7eLiom7dumnJkiWZFt/b/Znl5PWUV7L7euzWrZt+/vlnLVu27IZt5MYR8GvXrmnp0qV68MEH1b179xu+hg8fritXrtg/Zi8sLEw2m02fffaZfRupqak3/INBTl/32ZXVn28A4Mw40g8AUOfOndWmTRv95z//0dGjR9WgQQNFRETom2++0QsvvJDhvdGNGjXSmjVrNHXqVPvp8k2bNtWDDz6oL774QsWLF1edOnW0ZcsWrVmzxv5xcrnp5ZdfVo8ePTR79mw99dRTmj59upo3b6569eppyJAhqlq1qs6ePastW7bo5MmT+vnnn+2PTS/0+/fv14QJE+zLW7ZsqVWrVsnDwyPDR4jVqlVL1apV00svvaRTp07Jx8dHS5YsydH7gBs2bKjevXvro48+0uXLl9WsWTOtXbs2R59xfitWq1Wff/65OnTooLp162rQoEEqX768Tp06paioKPn4+Ojbb7+9o+fYsGGDrl+/fsPy+vXrZzgN/u6771ZYWFiGj+yTlOEMin+aM2eOPvroI3Xt2lXVqlXTlStX9Nlnn8nHx0cdO3aUlP3f05z8vN9++21FRUWpadOmGjJkiOrUqaMLFy5o165dWrNmjS5cuJDjn1NOXk+34/Lly/ryyy8zva9fv36SlO3X48svv6zFixerR48eevzxx9WoUSNduHBBK1as0IwZMzJc2+J2rFixQleuXNFDDz2U6f3333+//Pz8NG/ePPXs2VNdunRRkyZN9OKLL+rgwYOqVauWVqxYYZ/D34/E5+R1n12NGjXSxx9/rDfffFPVq1eXv7+/goODb2/nAcBR5O+HBQAAHME/P7LPMAzjypUrxogRI4xy5coZbm5uRo0aNYxJkyZl+IgrwzCMffv2GS1btjS8vLwMSfaP77t48aIxaNAgo3Tp0kbRokWNsLAwY9++fUalSpUyfMRfTj+yL7OPcEtJSTGqVatmVKtWzUhOTjYMwzAOHTpk9O/f3wgICDDc3NyM8uXLGw8++KCxePHiGx7v7+9vSDLOnj1rX7Zx40ZDktGiRYsb1v/999+NkJAQo2jRokbp0qWNIUOGGD///PMNHyE2YMAAo0iRIpnuz7Vr14znnnvOKFWqlFGkSBGjc+fOxokTJ3L0kX2TJk266XqGYRg//fST8cgjjxilSpUyPDw8jEqVKhmPPvqosXbtWvs66R/ZFxsbe8vtGcatP7Lv7/klGc8884zx5ZdfGjVq1DA8PDyMe+6554Z5//Mj+3bt2mX07t3bqFixouHh4WH4+/sbDz74YIaP/jOM7P+e5uTnffbsWeOZZ54xAgMDDTc3NyMgIMBo27at8emnn97yZ5PZR/blJGf6zyu7bvaRfX9/TWf39WgYaR9TOXz4cKN8+fKGu7u7UaFCBWPAgAH2jzFMn/+iRYsyPC799/Lvr4F/6ty5s+Hp6WnEx8dnuc7AgQMNNzc3+/PFxsYaffr0MYoVK2YUL17cGDhwoLFp0yZDkrFgwYIMj83O6z6rP0sy+7MoJibG6NSpk1GsWDFDEh/fB6BAsBgGVy8BAAC5w2Kx6Jlnnrnh1HbgTixfvlxdu3bVxo0b9cADD5gdBwCcCu/pBwAAgMO4du1ahtspKSmaNm2afHx8dO+995qUCgCcF+/pBwAAgMN49tlnde3aNQUFBSkxMVFLly7V5s2bNWHCBIf6mEkAcBaUfgAAADiM4OBgTZkyRd99952uX7+u6tWra9q0aRo+fLjZ0QDAKfGefgAAAAAACije0w8AAAAAQAFF6QcAAAAAoIDiPf25IDU1VadPn1axYsVksVjMjgMAAAAAKOAMw9CVK1dUrlw5Wa1ZH8+n9OeC06dPKzAw0OwYAAAAAIBC5sSJE6pQoUKW91P6c0GxYsUkpf2wfXx8TE6TNZvNpoiICIWGhsrNzc3sOMgCc3J8zMg5MCfnwJwcHzNyDszJOTAnx+dMM4qLi1NgYKC9j2aF0p8L0k/p9/HxcfjS7+3tLR8fH4f/BS7MmJPjY0bOgTk5B+bk+JiRc2BOzoE5OT5nnNGt3mLOhfwAAAAAACigKP0AAAAAABRQlH4AAAAAAAoo3tMPAAAAAAVQSkqKbDab2TGcis1mk6urq65fv66UlBRTs7i4uMjV1fWOPxae0g8AAAAABczVq1d18uRJGYZhdhSnYhiGAgICdOLEiTsu27nB29tbZcuWlbu7+21vg9IPAAAAAAVISkqKTp48KW9vb/n5+TlEeXUWqampunr1qooWLSqr1bx3wxuGoaSkJMXGxurIkSOqUaPGbeeh9AMAAABAAWKz2WQYhvz8/OTl5WV2HKeSmpqqpKQkeXp6mlr6JcnLy0tubm46duyYPdPt4EJ+AAAAAFAAcYTf+eXGPzxQ+gEAAAAAKKAo/QAAAAAAFFCUfgAAAACA6QzD0NChQ1WyZElZLBb5+vrqhRdeMDuW06P0AwAAAABMFx4ertmzZ+u7777TmTNn9Mcff+iNN964o21aLBYtX748R4/ZuHGjGjduLA8PD1WvXl2zZ8++owxmo/QDAAAAAEx36NAhlS1bVs2aNVNAQID8/f1VrFixLNdPSkrK9QxHjhxRz5491bp1a+3evVsvvPCCnnjiCa1evTrXnyu/8JF9AAAAAFCAGYahBFuCKc/t7eadrU8RGDhwoObMmSMp7eh8pUqVVLlyZTVs2FDvv/++JKly5coaPHiwDhw4oOXLl+uRRx7Rp59+qpEjR2rJkiW6ePGiypQpo6eeekqjR49W5cqVJUldu3aVJFWqVElHjx69aY5PPvlEFStW1OTJk2W1WlW7dm1t3LhR7733nsLCwm7752Ampyr969ev16RJk7Rz506dOXNGy5YtU5cuXW76mOjoaI0cOVJ79uxRYGCgXn31VQ0cODDDOtOnT9ekSZMUExOjBg0aaNq0aWrSpEne7QgAAAAA5JMEW4KKTixqynNfHX1VRdyL3HK9Dz74QNWqVdOnn36qHTt2yMXFRT169LhhvcmTJ2vMmDEaO3asJOnDDz/UihUr9PXXX6tixYo6ceKETpw4IUnasWOH/P39NWvWLLVv314uLi63zLF161a1bt06w7KwsDCnvraAU53eHx8frwYNGmj69OnZWv/IkSPq1KmT2rRpk+WpGQsXLtTIkSM1duxY7dq1Sw0aNFBYWJjOnTuXV7sBAAAAAPib4sWLq1ixYnJxcVFAQID8/PwyXS84OFgvvviiqlWrpmrVqun48eOqUaOGmjdvrkqVKql58+bq3bu3JNm34evre9Nt/l1MTMwN65UpU0ZxcXG6du3aHe6lOZzqSH+HDh3UoUOHbK8/Y8YMValSRVOmTJGkTE/NmDp1qoYMGaJBgwbZH/P9999r5syZGjVqVO7vhFmSE2Q5+Z3KJu+S5WSi5OpUoy9ULMnJzMnBWZKTVSLlqGRk/88jAAAAs3i7eevq6KumPXduaty4cYbbAwcOVLt27VSzZk21b99eDz74oEJDQ3P1OZ1dgW4UW7ZsUUhISIZlfz81IykpSTt37tTo0aPt91utVoWEhGjLli1ZbjcxMVGJiYn223FxcZIkm80mm82Wi3uQixLOyG1LTzWRpKx3DQ7AVWJODs5VUktJtt2nZWv4rpSN96kh/6X/eeywfy5DEnNyBszIOTAn55Bfc7LZbDIMQ6mpqUpNTZUkebl65elzZsUwDBmGke11Jdkzpy/7+21vb+8Mtxs2bKhDhw5p1apVWrt2rR599FG1bdtWixYtsq/z95/DrZQpU0axsbEZnvfMmTPy8fGRh4dHtreTW1JTU2UYhmw22w1vT8ju71GBLv0xMTEqU6ZMhmV/PzXj4sWLSklJyXSdffv2ZbndiRMnaty4cTcsj4iIkLd37v5LVm7xMC7pPmtts2MABYJFqSqZul9uBz/QwaNHtcd9IMXfgUVGRpodAdnAnBwfM3IOzMk55PWcXF1dFRAQoKtXr+bJFe7zyvXr15Wammo/qJqcnKykpCT77dTUVF2/ft1+++/Szwrv0KGDunfvrmPHjqlEiRJyc3PT1atXM31MZu69915FRkbqypUr9mWrVq3Sfffdl+1t5KakpCRdu3ZN69evV3Jycob7EhKyd3HGAl3688ro0aM1cuRI++24uDgFBgYqNDRUPj4+Jia7OZuthyIjI9WuXTu5ubmZHQdZsNlszMnB2Ww2/bxypBokfaLqyd+oSpXKSm3AEX9Hw2vJOTAnx8eMnANzcg75Nafr16/rxIkTKlq0qDw9PfPseXKbp6enrFarvVO5urrK3d3dfttqtcrT0zND53rvvfcUEBCge+65R1arVStXrlRAQIACAwNltVpVuXJl+xngHh4eKlGixE0zPPvss/r888/15ptv6vHHH1dUVJSWL1+ub7/91pSud/36dXl5eally5Y3zDK7/whRoEt/QECAzp49m2HZ2bNn5ePjIy8vL7m4uMjFxSXTdQICArLcroeHhzw8PG5Y7ubm5hR/yDpLzsKOOTm2o24ddPfd9eWy6xm5HPhALlZJ975H8XdAvJacA3NyfMzIOTAn55DXc0pJSZHFYpHVapXV6jzXbk//aL+/Z07fj6xu+/j4aPLkyTpw4IBcXFx03333aeXKlXL969pYU6ZM0ciRI/X555+rfPnyt/zIvqpVq2rhwoV67bXXNG3aNFWoUEGff/55jq4tl5usVqssFkumvzPZ/R0q0KU/KChIK1euzLAsMjJSQUFBkiR3d3c1atRIa9eutX/0X2pqqtauXavhw4fnd1wATia12hC5uLpK25+U9n8gGYbU6H2KPwAAwG144YUXMnw0XnR0dIb7MyvsQ4YM0ZAhQ7LcZufOndW5c+cc5WjevLl27tzpVP9gcjNOtRdXr17V7t27tXv3bklpH8m3e/duHT9+XFLaaff9+/e3r//UU0/p8OHDeuWVV7Rv3z599NFH+vrrrzVixAj7OiNHjtRnn32mOXPmaO/evRo2bJji4+PtV/MHgJuqPlRq8lna9398KO18Lq38AwAAAA7AqY70//jjj2rTpo39dvr76gcMGKDZs2frzJkz9n8AkKQqVaro+++/14gRI/TBBx/YT81I/7g+SerZs6diY2M1ZswYxcTEqGHDhgoPD7/h4n4AkKXqT6Qd3d82RPrjv2mlv/E0jvgDAAA4mLp16+rYsWOZ3vfJJ5+od+/e+Zwo7zlV6W/duvVNP+5h9uzZmT7mp59+uul2hw8fzun8AO5MtcGSLNK2J6QD0yWlSo3/K1mc6oQqAACAAm3lypVZftRdQT3w61SlHwAcWrXH00r+1selAx+nHfG/bzrFHwAAwEFUqlTppvenpqbmU5L8w99EASA3VR0o3T9LkkU6OEPaMUwyCt7/PAAAAOAcKP0AkNuqDpCC5iit+H8qbX+K4g8AAABTcHo/AOSFKo9JskhbB0iHPpNkSE0+4VR/AAAA5Cv+9gkAeaVKPynoi7Sif+jztKv7c8QfAAAA+YjSDwB5qXIfKejLtOJ/eKa0bbCUmmJ2KgAAABQSlH4AyGuVe0vN5ksWF+nwbIo/AABAAREdHS2LxaJLly6ZHSVLlH4AyA+Vev5/8T8yR9r2OMUfAACgkPnll1/UokULeXp6KjAwUO+++26ePyelHwDyS6VHpQe++qv4z5W2DqT4AwAAFBJxcXEKDQ1VpUqVtHPnTk2aNEmvv/66Pv300zx9Xko/AOSnij2kBxZKFlfp6JdpV/en+AMAgLxkGFJyvDlfhpGjqIsXL1a9evXk5eWlUqVKKSQkRPHx8ZKkzz//XLVr15anp6dq1aqljz76KMNjT548qd69e6tkyZIqUqSIGjdurG3bttnv//jjj1WtWjW5u7urZs2a+uKLLzI83mKx6PPPP1e/fv1UtGhR1ahRQytWrMiwzsqVK3XXXXfJy8tLbdq00dGjR7O9b/PmzVNSUpJmzpypunXrqlevXnruuec0derUHP2McoqP7AOA/Faxm2RZKG3sKR2dl/Y/w6A5kpU/kgEAQB5ISZC+LmrOcz96VXItkq1Vz5w5o969e+vdd99V165ddeXKFW3YsEGGYWjevHkaM2aM/vvf/+qee+7RTz/9pCFDhqhIkSIaMGCArl69qlatWql8+fJasWKFAgICtGvXLqWmpn1y0rJly/T888/r/fffV0hIiL777jsNGjRIFSpUUJs2bewZ3njjDY0dO1ZTp07V9OnT1bdvXx07dkwlS5bUiRMn9Mgjj+iZZ57R0KFD9eOPP+rFF1/M9o9iy5Ytatmypdzd3e3LwsLC9M477+jixYsqUaJEtreVE/wNEwDMEPiI1PxraeOj0rH5klLTPt6P4g8AAAqpM2fOKDk5WY888ogqVaokSapXr54kaezYsZoyZYoeeeQRSVKVKlX0+++/65NPPtGAAQM0f/58xcbGaseOHSpZsqQkqXr16vZtT548WQMHDtTTTz8tSRo5cqS2bt2qyZMnZyj9AwYMUPfu3eXj46MJEyboww8/1Pbt29W+fXv7mQJTpkyRJNWsWVO//vqr3nnnnWztX0xMjKpUqZJhWZkyZez3UfoBoKAJ7Cq1WCxt7CEdW5B2xL/ZlxR/AACQu1y80464m/Xc2dSgQQO1bdtW9erVU1hYmEJDQ9W9e3e5u7vr0KFDGjx4sIYMGWJfPzk5WcWLF5ck7d69W/fcc4+98P/T3r17NXTo0AzLHnjgAX3wwQcZlqX/I4MkFSlSRD4+Pjp37px9G02bNs2wflBQULb3zyz8zRIAzFThYan5Ymljd+n4QkmpUrN5ktXN7GQAAKCgsFiyfYq9mVxcXBQZGanNmzcrIiJC06ZN03/+8x99++23kqTPPvvshtLt4uIiSfLy8sqVDG5uGf8OZrFY7G8RuFMBAQE6e/ZshmXptwMCAnLlOTLDhfwAwGwVHpKaL0kr+scXSZv6SKk2s1MBAADkO4vFogceeEDjxo3TTz/9JHd3d23atEnlypXT4cOHVb169Qxf6afL169fX7t379aFCxcy3W7t2rW1adOmDMs2bdqkOnXqZDtb7dq1tX379gzLtm7dmu3HBwUFaf369bLZ/v/veZGRkapZs2aendovUfoBwDFU6Cy1WCpZ3aUTi6VNvSn+AACgUNm2bZsmTJigH3/8UcePH9fSpUsVGxur2rVra9y4cZo4caI+/PBD/fHHH/r11181a9Ys+5Xve/furYCAAHXp0kWbNm3S4cOHtWTJEm3ZskWS9PLLL2v27Nn6+OOPdeDAAU2dOlVLly7VSy+9lO18Tz31lA4cOKCXX35Z+/fv1/z58zV79uxsP75Pnz5yd3fX4MGDtWfPHi1cuFAffPCBRo4cmaOfU05R+gHAUZR/UGqx7K/ivyTt6v4pSWanAgAAyBc+Pj5av369OnbsqLvuukuvvvqqpkyZog4dOuiJJ57Q559/rlmzZqlevXpq1aqVZs+ebT/S7+7uroiICPn7+6tjx46qV6+e3n77bfvp/126dNEHH3ygyZMnq27duvrkk080a9YstW7dOtv5KlasqCVLlmj58uVq0KCBZsyYoQkTJmT78cWLF1dERISOHDmiRo0a6cUXX9SYMWNuuNZAbuM9/QDgSMp3lFoul9Z3lU4ukzb1lB5YKLm43/KhAAAAzqx27doKDw/P8v4+ffqoT58+Wd5fqVIlLV68OMv7hw0bpmHDhmV5v2EYSk1NVVxcnH3ZpUuXMqzz4IMP6sEHH8ywbNCgQVlu85/q16+vDRs2ZHv93MCRfgBwNOU6pBV/q4d0crm06VGO+AMAAOC2UPoBwBGVay+1/Oav4v9N2tX9UxLNTgUAAIAsdOjQQUWLFs30KydvA8htnN4PAI6qXJjUaoW0/mHp1LfShu5Si8WSi4fZyQAAAPAPn3/+ua5du5bpfSVLlsznNP+P0g8AjqxsqNTqW+mHztLp76QN3aQWSyj+AAAADqZ8+fJmR8gUp/cDgKMLCJFafSe5eEmnv5c2PCKlXDc7FQAAcHCGYZgdAXcoN2ZI6QcAZxDQ9m/Ff2Xa1f0p/gAAIBPpH1OXlMSFgJ1dQkKCJMnNze22t8Hp/QDgLAKCpdbfS9GdpDPh0vouaVf5d/E0OxkAAHAgrq6u8vb2VmxsrNzc3GS1cqw3u1JTU5WUlKTr16+b+nMzDEMJCQk6d+6cfH197f+Qczso/QDgTMq0kVqv/Kv4r5Z+eDit+Lt6mZ0MAAA4CIvForJly+rIkSM6duyY2XGcimEYunbtmry8vGSxWMyOI19fXwUEBNzRNij9AOBsyrSW2qySojtKMRHS+ofSPt7P1dvsZAAAwEG4u7urRo0anOKfQzabTevXr1fLli3v6JT63ODm5nZHR/jTUfoBwBn5t5Rar5KiO0gxa6QfHkr7eD+KPwAA+IvVapWnJ28DzAkXFxclJyfL09PT9NKfW3hzBwA4K/8WUutwybWodHZt2sf6JSeYnQoAAAAOhNIPAM7Mv7nUJr34r5N+eFBKjjc7FQAAABwEpR8AnJ3fA1Kb1ZJrMelslBRN8QcAAEAaSj8AFAR+zf6/+J+LTrvIn+2q2akAAABgMko/ABQUfkFScITk5iOdW0/xBwAAAKUfAAqU0vdLbf4q/rEb0q7ub7tidioAAACYhNIPAAVN6aZSm0jJrbgUu5HiDwAAUIhR+gGgICrdRApeI7n5SrGbpKj2ki3O7FQAAADIZ5R+ACioSjWW2v5V/M9vpvgDAAAUQpR+ACjISjaS2q6V3EtI57dI68KkpMtmpwIAAEA+ofQDQEFX8l4peK3kXlL6c6sUFSolXTI7FQAAAPIBpR8ACoOS9/x1xL+k9Od2aR3FHwAAoDCg9ANAYVGiodR2neRRSrqwQ1rXTkq6aHYqAAAA5CGnK/3Tp09X5cqV5enpqaZNm2r79u1Zrtu6dWtZLJYbvjp16mRfZ+DAgTfc3759+/zYFQDIfyUaSMHrJI/S0oUfKf4AAAAFnFOV/oULF2rkyJEaO3asdu3apQYNGigsLEznzp3LdP2lS5fqzJkz9q/ffvtNLi4u6tGjR4b12rdvn2G9r776Kj92BwDMUaL+X0f8S0sXdkprQ6TEC2anAgAAQB5wqtI/depUDRkyRIMGDVKdOnU0Y8YMeXt7a+bMmZmuX7JkSQUEBNi/IiMj5e3tfUPp9/DwyLBeiRIl8mN3AMA8vvWktlGSh590cZe0juIPAABQELmaHSC7kpKStHPnTo0ePdq+zGq1KiQkRFu2bMnWNv73v/+pV69eKlKkSIbl0dHR8vf3V4kSJRQcHKw333xTpUqVynI7iYmJSkxMtN+Oi0v73GubzSabzZaT3cpX6dkcOSOYkzMoMDMqUlNqFSHXH8JkufiTjLXBSm4Znvae/wKgwMypgGNOjo8ZOQfm5ByYk+NzphllN6PFMAwjj7PkitOnT6t8+fLavHmzgoKC7MtfeeUV/fDDD9q2bdtNH799+3Y1bdpU27ZtU5MmTezLFyxYIG9vb1WpUkWHDh3Sv//9bxUtWlRbtmyRi4tLptt6/fXXNW7cuBuWz58/X97e3re5hwBgjmKpJ9Ts2qvy1GVdtlbWZs/xSrL4mB0LAAAAN5GQkKA+ffro8uXL8vHJ+u9uhab0P/nkk9qyZYt++eWXm653+PBhVatWTWvWrFHbtm0zXSezI/2BgYE6f/78TX/YZrPZbIqMjFS7du3k5uZmdhxkgTk5vgI5o7jf5RodJkviWRnF6ym51eq09/w7sQI5pwKIOTk+ZuQcmJNzYE6Oz5lmFBcXp9KlS9+y9DvN6f2lS5eWi4uLzp49m2H52bNnFRAQcNPHxsfHa8GCBRo/fvwtn6dq1aoqXbq0Dh48mGXp9/DwkIeHxw3L3dzcHP4XQ3KenIUdc3J8BWpGpRpIIdHS2jayXP5VbuvDpOC1kqef2cnuWIGaUwHGnBwfM3IOzMk5MCfH5wwzym4+p7mQn7u7uxo1aqS1a9fal6Wmpmrt2rUZjvxnZtGiRUpMTFS/fv1u+TwnT57Un3/+qbJly95xZgBwKsVrpV3czzNAuvSrtDZYup75p6MAAADAOThN6ZekkSNH6rPPPtOcOXO0d+9eDRs2TPHx8Ro0aJAkqX///hku9Jfuf//7n7p06XLDxfmuXr2ql19+WVu3btXRo0e1du1aPfzww6pevbrCwsLyZZ8AwKEUr5V2xN+rrHT5N4o/AACAk3Oa0/slqWfPnoqNjdWYMWMUExOjhg0bKjw8XGXKlJEkHT9+XFZrxn/H2L9/vzZu3KiIiIgbtufi4qJffvlFc+bM0aVLl1SuXDmFhobqjTfeyPT0fQAoFHxqSm2jpbVtpMt70v4bvE7yKmN2MgAAAOSQU5V+SRo+fLiGDx+e6X3R0dE3LKtZs6ayulahl5eXVq9enZvxAKBg8Lnrb8X/97T/tl0ned38GioAAABwLE51ej8AIB/51Eg71d+7ghS3N634XztjdioAAADkAKUfAJC1YtXTjvh7B0px+yj+AAAATobSDwC4uWLV/jriHyjF7ZfWtJYSTpudCgAAANlA6QcA3FrRqn8V/4rSlT+kta2lhFNmpwIAAMAtUPoBANmTXvyLVJKuHPjriP9Js1MBAADgJij9AIDsK1ol7T3+RSpLVw9S/AEAABwcpR8AkDNFK/91xL+ydPVQWvGPP2FuJgAAAGSK0g8AyLkilaSQH6QiVdKK/9rWUvxxs1MBAADgHyj9AIDbU6RiWvEvWlW6evivI/7HzE4FAACAv6H0AwBuX5HAtPf4F60mxR9JK/5Xj5qbCQAAAHaUfgDAnSkSmPYe/6LVpfijaaf6U/wBAAAcAqUfAHDnvCukFf9iNdJO8V/TSrp6xOxUAAAAhR6lHwCQO7zLS22jpGJ3SQnH/zrV/7DZqQAAAAo1Sj8AIPekF3+fmv9f/K8cMjsVAABAoUXpBwDkLu9yfxX/WlLCibT3+F85aHYqAACAQonSDwDIfV5l/1b8T/51xJ/iDwAAkN8o/QCAvOEVkPZxfsXrSNdOpV3cL+6A2akAAAAKFUo/ACDveJWRgtf9VfxPS2tbSXF/mJ0KAACg0KD0AwDylleZtFP9i9eVrp1Je49/3H6zUwEAABQKlH4AQN7z9E8r/r710or/mtbS5X1mpwIAACjwKP0AgPzh6ScFr00r/tdj0o74X95rdioAAIACjdIPAMg/nn5p7/H3rS9dPyutbSNd/t3sVAAAAAUWpR8AkL88S0tt10klGv5/8b+0x+xUAAAABRKlHwCQ/zxKScFrpBL3SNfP/VX8fzM7FQAAQIFD6QcAmOPvxT8xVlobLF361exUAAAABQqlHwBgHo+SacW/ZKP/L/4XfzE7FQAAQIFB6QcAmMujpBQcKZVsLCWel9YFSxd/NjsVAABAgUDpBwCYz73EX8X/PinxT2ldW+nibrNTAQAAOD1KPwDAMbj7SsERUqkmacV/bVvpwk9mpwIAAHBqlH4AgONw95XaREilmkpJF9KO+F/YZXYqAAAAp0XpBwA4FvfiUpvVUqn7paSL0roQ6cJOs1MBAAA4JUo/AMDxuBeXgldLpYPSiv/aEOnPH81OBQAA4HQo/QAAx+Tmk3bEv3QzyXYp7Yj/nzvMTgUAAOBUKP0AAMflVkxqEy75PSDZLkvr2knnt5udCgAAwGlQ+gEAjs2tmNR6leTXIq34R7WTzm8zOxUAAIBToPQDAByfWzGp9UrJv6Vki/vriP9Ws1MBAAA4PEo/AMA5uBWVWn0v+beSkq9I60Kl2C1mpwIAAHBolH4AgPNwKyq1/l7yb51W/KNCpdhNZqcCAABwWJR+AIBzcS2SVvzLtJGSr0pR7aVzG81OBQAA4JAo/QAA5+PqLbX6TioTnFb8o9tL5zaYnQoAAMDhUPoBAM7J1Vtq9a0UECIlx0vRHaRz681OBQAA4FAo/QAA5+XqLbVcIQW0Syv+UR2ksz+YnQoAAMBhOF3pnz59uipXrixPT081bdpU27dvz3Ld2bNny2KxZPjy9PTMsI5hGBozZozKli0rLy8vhYSE6MCBA3m9GwCA3OLqJbX8RgoIlVISpOiO0tlos1MBAAA4BKcq/QsXLtTIkSM1duxY7dq1Sw0aNFBYWJjOnTuX5WN8fHx05swZ+9exY8cy3P/uu+/qww8/1IwZM7Rt2zYVKVJEYWFhun79el7vDgAgt7h6Sa2+kcq2txd/y7kos1MBAACYzqlK/9SpUzVkyBANGjRIderU0YwZM+Tt7a2ZM2dm+RiLxaKAgAD7V5kyZez3GYah999/X6+++qoefvhh1a9fX3PnztXp06e1fPnyfNgjAECucfGUWi6TynaQUq7JZWMXlU752exUAAAApnI1O0B2JSUlaefOnRo9erR9mdVqVUhIiLZs2ZLl465evapKlSopNTVV9957ryZMmKC6detKko4cOaKYmBiFhITY1y9evLiaNm2qLVu2qFevXpluMzExUYmJifbbcXFxkiSbzSabzXZH+5mX0rM5ckYwJ2fAjByZixS0UC6be8oas0r3p7wl26l7pfJhZgdDFng9OT5m5ByYk3NgTo7PmWaU3YxOU/rPnz+vlJSUDEfqJalMmTLat29fpo+pWbOmZs6cqfr16+vy5cuaPHmymjVrpj179qhChQqKiYmxb+Of20y/LzMTJ07UuHHjblgeEREhb2/vnO5avouMjDQ7ArKBOTk+ZuS4rMbjus8lVgEpP0qbu2mb578V69LQ7Fi4CV5Pjo8ZOQfm5ByYk+NzhhklJCRkaz2nKf23IygoSEFBQfbbzZo1U+3atfXJJ5/ojTfeuO3tjh49WiNHjrTfjouLU2BgoEJDQ+Xj43NHmfOSzWZTZGSk2rVrJzc3N7PjIAvMyfExI+dgu95WZ1aGqWzKDgUlTVTKA0tlBLQzOxb+gdeT42NGzoE5OQfm5PicaUbpZ5zfitOU/tKlS8vFxUVnz57NsPzs2bMKCAjI1jbc3Nx0zz336ODBg5Jkf9zZs2dVtmzZDNts2LBhltvx8PCQh4dHptt39F8MyXlyFnbMyfExI0dXVD96vKJOvnNkPf2dXDc9knaV/3Kc6u+IeD05PmbkHJiTc2BOjs8ZZpTdfE5zIT93d3c1atRIa9eutS9LTU3V2rVrMxzNv5mUlBT9+uuv9oJfpUoVBQQEZNhmXFyctm3blu1tAgAcV6rFTSlBC6QKD0upidL6h6XTq8yOBQAAkG+cpvRL0siRI/XZZ59pzpw52rt3r4YNG6b4+HgNGjRIktS/f/8MF/obP368IiIidPjwYe3atUv9+vXTsWPH9MQTT0hKu7L/Cy+8oDfffFMrVqzQr7/+qv79+6tcuXLq0qWLGbsIAMhtVnfpga+lCl3/Kv5dpFMrzU4FAACQL5zm9H5J6tmzp2JjYzVmzBjFxMSoYcOGCg8Pt1+I7/jx47Ja///fMS5evKghQ4YoJiZGJUqUUKNGjbR582bVqVPHvs4rr7yi+Ph4DR06VJcuXVLz5s0VHh4uT0/PfN8/AEAecXGXmi+UNvWSTiyVNnSVWiyVyncyOxkAAECecqrSL0nDhw/X8OHDM70vOjo6w+333ntP77333k23Z7FYNH78eI0fPz63IgIAHJHVTXpggbSpt3RiSVrxb75EqtDZ7GQAAAB5xqlO7wcA4I5Y3aQHvpIq9pBSbdLGbtLJFWanAgAAyDOUfgBA4WJ1k5rNkyo++lfx7y6d/MbsVAAAAHmC0g8AKHzSi3+lXmnFf0N36cQys1MBAADkOko/AKBwsrpKQV9IlXpLRrK08dG0i/wBAAAUIJR+AEDhZXWVguZKlfr8Vfx7SseXmJ0KAAAg11D6AQCFW3rxr9wvrfhv6ikdX2R2KgAAgFzhdB/ZBwBArrO6SPfPlmSRjn6R9rF+hiFVetTsZAAAAHeE0g8AgPRX8Z8lWSzSkbnS5j6SDKlST7OTAQAA3DZO7wcAIJ3VRWo6U6o6UDJS0or/0a/MTgUAAHDbONIPAMDfWV2kpv+TZJEOz5K29JNkSJX7mJ0MAAAgxzjSDwDAP1msUtPPpWqDJSNV2vKYdGSe2akAAAByjNIPAEBmLFapyadStSfSiv/W/tKRL8xOBQAAkCOUfgAAsmKxSk0+kaoP/euI/wDp8FyzUwEAAGQbpR8AgJuxWKX7PpaqPynJkLYOlA7PMTsVAABAtlD6AQC4FYtVuu8jqcYwpRX/QdKhWWanAgAAuCVKPwAA2WGxSo2nSzWelmRI2wZLh2aanQoAAOCmKP0AAGSXxSI1/q9013DZi//Bz81OBQAAkCVKPwAAOWGxSI0+lO56Lu329iHSwU/NzQQAAJAFSj8AADllsUiN3pdqPp92e/uT0oFPTI0EAACQGUo/AAC3w2KR7n1PqvlC2u0dT0kHZpgaCQAA4J8o/QAA3C6LRbp3qlRrZNrtHcOkPz4yNxMAAMDfUPoBALgTFot0z2Sp9ktpt398RvpjurmZAAAA/kLpBwDgTlksUsN3pdovp93+cbi0f5q5mQAAAETpBwAgd1gsUsN3pDr/Sru98zlp3wfmZgIAAIUepR8AgNxisUgNJkp1Rqfd3vWCtO99MxMBAIBCjtIPAEBuslikBm9Jdf+ddnvXCGnfe+ZmAgAAhRalHwCA3GaxSPXflOq+mnZ710hp7xRzMwEAgEKJ0g8AQF6wWKT646W7x6Td/ukl6fdJ5mYCAACFDqUfAIC8YrFI9cdJd49Nu737Fen3d83NBAAAChVKPwAAea3+61K919O+3/0vac/bZqYBAACFCKUfAID8UG+sVG982vc/j5b2TDA3DwAAKBQo/QAA5Jd6r0n130j7/uf/SL+9ZW4eAABQ4FH6AQDIT3e/mvaRfpL0y6vSr2+YmwcAABRolH4AAPJb3X9LDSamff/rGOnXcebmAQAABRalHwAAM9QdJTX864J+v74u/fK6iWEAAEBBRekHAMAsdf4lNfzrI/x+Gyf9MlYyDHMzAQCAAoXSDwCAmeq8LN0zOe3738ZLv4yh+AMAgFxD6QcAwGy1X5TumZL2/Z43pV9eo/gDAIBcQekHAMAR1B4p3fte2vd73kr7SD+KPwAAuEOUfgAAHEWtF6R730/7/veJ0s+jKf4AAOCOUPoBAHAktZ6XGn2Y9v3v70i7R1H8AQDAbXO60j99+nRVrlxZnp6eatq0qbZv357lup999platGihEiVKqESJEgoJCblh/YEDB8pisWT4at++fV7vBgAAWav5rNT4v2nf731X2v0KxR8AANwWpyr9Cxcu1MiRIzV27Fjt2rVLDRo0UFhYmM6dO5fp+tHR0erdu7eioqK0ZcsWBQYGKjQ0VKdOncqwXvv27XXmzBn711dffZUfuwMAQNbuekZqPD3t+72TpZ9epvgDAIAcc6rSP3XqVA0ZMkSDBg1SnTp1NGPGDHl7e2vmzJmZrj9v3jw9/fTTatiwoWrVqqXPP/9cqampWrt2bYb1PDw8FBAQYP8qUaJEfuwOAAA3d9fT0n0fpX2/b4q060WKPwAAyBFXswNkV1JSknbu3KnRo0fbl1mtVoWEhGjLli3Z2kZCQoJsNptKliyZYXl0dLT8/f1VokQJBQcH680331SpUqWy3E5iYqISExPtt+Pi4iRJNptNNpstJ7uVr9KzOXJGMCdnwIycQ4GZU+UnZE0x5LLrGWn/e0pJTVZqg8mSxWJ2slxRYOZUgDEj58CcnANzcnzONKPsZrQYhnMcMjh9+rTKly+vzZs3KygoyL78lVde0Q8//KBt27bdchtPP/20Vq9erT179sjT01OStGDBAnl7e6tKlSo6dOiQ/v3vf6to0aLasmWLXFxcMt3O66+/rnHjxt2wfP78+fL29r7NPQQAIGuVbKvVMOljSdIh1wf1m/vgAlP8AQBAziUkJKhPnz66fPmyfHx8slzPaY7036m3335bCxYsUHR0tL3wS1KvXr3s39erV0/169dXtWrVFB0drbZt22a6rdGjR2vkyJH223FxcfbrBdzsh202m82myMhItWvXTm5ubmbHQRaYk+NjRs6h4M2po5IPN5DrzqdULfk7Va5cUakN33P64l/w5lTwMCPnwJycA3NyfM40o/Qzzm/FaUp/6dKl5eLiorNnz2ZYfvbsWQUEBNz0sZMnT9bbb7+tNWvWqH79+jddt2rVqipdurQOHjyYZen38PCQh4fHDcvd3Nwc/hdDcp6chR1zcnzMyDkUqDnVfFJydZO2PSGXgx/JxWKRGk9z+uIvFbA5FVDMyDkwJ+fAnByfM8wou/mc5kJ+7u7uatSoUYaL8KVflO/vp/v/07vvvqs33nhD4eHhaty48S2f5+TJk/rzzz9VtmzZXMkNAECuqva41PR/kizSgenSj89IRqrZqQAAgINymtIvSSNHjtRnn32mOXPmaO/evRo2bJji4+M1aNAgSVL//v0zXOjvnXfe0WuvvaaZM2eqcuXKiomJUUxMjK5evSpJunr1ql5++WVt3bpVR48e1dq1a/Xwww+revXqCgsLM2UfAQC4pWqDpPtnKq34fyztoPgDAIDMOc3p/ZLUs2dPxcbGasyYMYqJiVHDhg0VHh6uMmXKSJKOHz8uq/X//x3j448/VlJSkrp3755hO2PHjtXrr78uFxcX/fLLL5ozZ44uXbqkcuXKKTQ0VG+88Uamp+8DAOAwqg6UZJW2DpQOzpCUKt33sWRxqn/PBwAAecypSr8kDR8+XMOHD8/0vujo6Ay3jx49etNteXl5afXq1bmUDACAfFa1f9r7+bcMkA5+KhmG1GQGxR8AANjxtwIAAJxZlcekoLlpRf/QZ9L2oZzqDwAA7Cj9AAA4uyr9pKAv/ir+/5O2DaH4AwAASZR+AAAKhsp9pKAv04r/4ZnStsFSaorZqQAAgMko/QAAFBSVe0vN5ksWF+nwbIo/AADI3oX8Ro4cme0NTp069bbDAACAO1SppySLtLmPdGRO2mn+98+SrC5mJwMAACbIVun/6aefMtzetWuXkpOTVbNmTUnSH3/8IRcXFzVq1Cj3EwIAgJyp9GjaVf039ZaOfiHJkO6fTfEHAKAQylbpj4qKsn8/depUFStWTHPmzFGJEiUkSRcvXtSgQYPUokWLvEkJAABypmIPSVZpUy/p6JdpR/yD5khWp/u0XgAAcAdy/J7+KVOmaOLEifbCL0klSpTQm2++qSlTpuRqOAAAcAcqdpOaL5QsrtKx+dKW/lJqstmpAABAPspx6Y+Li1NsbOwNy2NjY3XlypVcCQUAAHJJ4CNS86//Kv5fSVseo/gDAFCI5Lj0d+3aVYMGDdLSpUt18uRJnTx5UkuWLNHgwYP1yCOP5EVGAABwJwK7Si0WS1Y36dgCaXNfij8AAIVEjt/YN2PGDL300kvq06ePbDZb2kZcXTV48GBNmjQp1wMCAIBcUOFhqfliaWN36fjXkgyp2by0fwgAAAAFVo5Kf0pKin788Ue99dZbmjRpkg4dOiRJqlatmooUKZInAQEAQC6p8JDUfIm0sZt0fJFkGNID8yn+AAAUYDk6vd/FxUWhoaG6dOmSihQpovr166t+/foUfgAAnEWFzlKLpZLVXTqxOO3q/qk2s1MBAIA8kuP39N999906fPhwXmQBAAD5ofyDUotlfxX/pdLGnlJKktmpAABAHshx6X/zzTf10ksv6bvvvtOZM2cUFxeX4QsAADiB8h2llsslq4d0cpm0ieIPAEBBlOML+XXs2FGS9NBDD8lisdiXG4Yhi8WilJSU3EsHAADyTrkOacV/fRfp5HJpYw+p+SLJxd3kYAAAILfkuPRHRUXlRQ4AAGCGcu2llt9I6x+WTq1Iu7p/80WSi4fZyQAAQC7Icelv1apVXuQAAABmKRcmtVrxV/H/VtrQXWqxmOIPAEABkOPSny4hIUHHjx9XUlLG9//Vr1//jkMBAIB8VjZUavWt9ENn6fR30oZHpBZLJBdPs5MBAIA7kOPSHxsbq0GDBmnVqlWZ3s97+gEAcFIBIVKr7/4q/iul9Y9ILZdS/AEAcGI5vnr/Cy+8oEuXLmnbtm3y8vJSeHi45syZoxo1amjFihV5kREAAOSXgLZpxd/FSzqzSlrfVUq5bnYqAABwm3Jc+tetW6epU6eqcePGslqtqlSpkvr166d3331XEydOzIuMAAAgPwUES61XSi7e0plw6YeHpeRrZqcCAAC3IcelPz4+Xv7+/pKkEiVKKDY2VpJUr1497dq1K3fTAQAAc5Rp/f/FPyYi7SJ/FH8AAJxOjkt/zZo1tX//fklSgwYN9Mknn+jUqVOaMWOGypYtm+sBAQCAScq0ktqsklyLSDGR0vqHpOQEs1MBAIAcyHHpf/7553XmzBlJ0tixY7Vq1SpVrFhRH374oSZMmJDrAQEAgIn8W0qt04v/GukHij8AAM4kx1fv79evn/37Ro0a6dixY9q3b58qVqyo0qVL52o4AADgAPxbSK3DpegO0tm1aVf3b/Wt5OptdjIAAHALOT7Sf/jw4Qy3vb29de+991L4AQAoyPybS23CJdei0tl10g8PSsnxZqcCAAC3kOPSX716dVWsWFGPPfaY/ve//+ngwYN5kQsAADgavwekNqsl12LS2SgpuhPFHwAAB5fj0n/ixAlNnDhRXl5eevfdd3XXXXepQoUK6tu3rz7//PO8yAgAAByFX7P/L/7nfpCiO0q2q2anAgAAWchx6S9fvrz69u2rTz/9VPv379f+/fsVEhKir7/+Wk8++WReZAQAAI7EL0gKjpDcfKRz6yn+AAA4sByX/oSEBEVEROjf//63mjVrpvr16+vnn3/W8OHDtXTp0rzICAAAHE3p+6U2kZJbcSl2Q9pF/mxXzE4FAAD+IcdX7/f19VWJEiXUt29fjRo1Si1atFCJEiXyIhsAAHBkpZtIwZHSunZS7Ma04t96leRWzOxkAADgLzk+0t+xY0elpKRowYIFWrBggRYtWqQ//vgjL7IBAABHV+o+KXiN5OYrxW6SotpLtjizUwEAgL/kuPQvX75c58+fV3h4uIKCghQREaEWLVrY3+sPAAAKmVKNpbZrJPcS0vnN0rowKemy2akAAIBuo/Snq1evnh544AEFBQXpvvvu07lz57Rw4cLczAYAAJxFyUZpR/zdS0h/bpWiKP4AADiCHJf+qVOn6qGHHlKpUqXUtGlTffXVV7rrrru0ZMkSxcbG5kVGAADgDEreKwWvldxLSn9uk6JCpaRLZqcCAKBQy/GF/L766iu1atVKQ4cOVYsWLVS8ePG8yAUAAJxRyXuktmultW2lP7dL60LTPt7P3dfsZAAAFEo5Lv07duzIixwAAKCgKNFQartOWtdWurAj7er+wRFpp/4DAIB8dVvv6d+wYYP69eunoKAgnTp1SpL0xRdfaOPGjbkaDgAAOKkSDaTgdZJHaenCj2nFP+mi2akAACh0clz6lyxZorCwMHl5eemnn35SYmKiJOny5cuaMGFCrgcEAABOqkT9tCP+HqWlCzultSFS4gWzUwEAUKjkuPS/+eabmjFjhj777DO5ubnZlz/wwAPatWtXroYDAABOzree1DZK8vCTLu6S1lH8AQDITzku/fv371fLli1vWF68eHFdunQpNzIBAICCxPfutOLv6S9d/Cntvf6Jf5qdCgCAQiHHpT8gIEAHDx68YfnGjRtVtWrVXAl1M9OnT1flypXl6emppk2bavv27Tddf9GiRapVq5Y8PT1Vr149rVy5MsP9hmFozJgxKlu2rLy8vBQSEqIDBw7k5S4AAFD4+Nb9q/iXkS7uTru6//XzZqcCAKDAy3HpHzJkiJ5//nlt27ZNFotFp0+f1rx58/TSSy9p2LBheZHRbuHChRo5cqTGjh2rXbt2qUGDBgoLC9O5c+cyXX/z5s3q3bu3Bg8erJ9++kldunRRly5d9Ntvv9nXeffdd/Xhhx9qxowZ2rZtm4oUKaKwsDBdv349T/cFAIBCp3id/y/+l37+64g/xR8AgLyU44/sGzVqlFJTU9W2bVslJCSoZcuW8vDw0EsvvaRnn302LzLaTZ06VUOGDNGgQYMkSTNmzND333+vmTNnatSoUTes/8EHH6h9+/Z6+eWXJUlvvPGGIiMj9d///lczZsyQYRh6//339eqrr+rhhx+WJM2dO1dlypTR8uXL1atXrzzdHwAACp3itaW20dLaNtKlX5S4prmuxNXX9m3r5eLiYnY6ZCIlJUVxlw8zIwfHnJwDc3J86TM6fspL1SqHmh0nV+S49FssFv3nP//Ryy+/rIMHD+rq1auqU6eOihYtqmvXrsnLyysvciopKUk7d+7U6NGj7cusVqtCQkK0ZcuWTB+zZcsWjRw5MsOysLAwLV++XJJ05MgRxcTEKCQkxH5/8eLF1bRpU23ZsiXL0p+YmGj/1AJJiouLkyTZbDbZbLbb2r/8kJ7NkTOCOTkDZuQcmJMD864mo1WEEiLul2/CYfVzPSwdNzsUbuYBVzEjJ8CcnANzcnwPuErr9lpUsXwbs6PcVHb/jpPj0p/O3d1dderUkZRWgqdOnap3331XMTExt7vJmzp//rxSUlJUpkyZDMvLlCmjffv2ZfqYmJiYTNdPz5j+35utk5mJEydq3LhxNyyPiIiQt7f3rXfGZJGRkWZHQDYwJ8fHjJwDc3I8qUaqPj7xsY5eSdA7paWKbnlzwAAAgNt1Kjb1huvBOZqEhIRsrZft0p+YmKjXX39dkZGRcnd31yuvvKIuXbpo1qxZ+s9//iMXFxeNGDHitgM7k9GjR2c4gyAuLk6BgYEKDQ2Vj4+PicluzmazKTIyUu3atcvwcYtwLMzJ8TEj58CcHFOqkaqnVj6lyAuRslqsunjvZ7KeKsWcHBivJefAnJwDc3J86TPq5gQzSj/j/FayXfrHjBmjTz75RCEhIdq8ebN69OihQYMGaevWrZo6dap69OiRp+9LKV26tFxcXHT27NkMy8+ePauAgIBMHxMQEHDT9dP/e/bsWZUtWzbDOg0bNswyi4eHhzw8PG5Y7ubm5vC/GJLz5CzsmJPjY0bOgTk5jlQjVU+ueFKzf54tq8WqL7p+oR61emjlqZXMyQkwI+fAnJwDc3J8zjCj7ObL9tX7Fy1apLlz52rx4sWKiIhQSkqKkpOT9fPPP6tXr155fiEKd3d3NWrUSGvXrrUvS01N1dq1axUUFJTpY4KCgjKsL6Wd5pm+fpUqVRQQEJBhnbi4OG3bti3LbQIAgJxLSU3R4BWDNWv3LFktVs17ZJ761OtjdiwAAAq8bB/pP3nypBo1aiRJuvvuu+Xh4aERI0bIYrHkWbh/GjlypAYMGKDGjRurSZMmev/99xUfH2+/mn///v1Vvnx5TZw4UZL0/PPPq1WrVpoyZYo6deqkBQsW6Mcff9Snn34qKe2ihC+88ILefPNN1ahRQ1WqVNFrr72mcuXKqUuXLvm2XwAAFGQpqSl6fMXjmvvzXLlYXDTvkXnqeXdPs2MBAFAoZLv0p6SkyN3d/f8f6OqqokWL5kmorPTs2VOxsbEaM2aMYmJi1LBhQ4WHh9svxHf8+HFZrf9/8kKzZs00f/58vfrqq/r3v/+tGjVqaPny5br77rvt67zyyiuKj4/X0KFDdenSJTVv3lzh4eHy9PTM130DAKAgSklN0aBvBumLX76Qi8VFX3X7Sj3q9jA7FgAAhUa2S79hGBo4cKD9vezXr1/XU089pSJFimRYb+nSpbmb8B+GDx+u4cOHZ3pfdHT0Dct69OihHj2y/suFxWLR+PHjNX78+NyKCAAAlFb4BywfoHm/zpOLxUULui9Q9zrdzY4FAEChku3SP2DAgAy3+/Xrl+thAABAwZCcmqwBywdo/q/z5Wp11YJuC9StTjezYwEAUOhku/TPmjUrL3MAAIACIjk1WY8te0wLflsgV6urvu7+tbrW7mp2LAAACqVsl34AAIBbSU5NVr+l/bRwz0K5Wl21qMcidanVxexYAAAUWpR+AACQK5JTk9V3aV99vedruVndtKjHIj1c62GzYwEAUKhR+gEAwB2zpdjUd2lfLfp9kdysblry6BJ1rtnZ7FgAABR6lH4AAHBHbCk29V7SW0v2LpG7i7uWPLpED971oNmxAACAKP0AAOAO2FJs6rWkl5buXSp3F3ctfXSpOt3VyexYAADgL9kq/StWrMj2Bh966KHbDgMAAJxHUkqSei3upWX7lsndxV3Lei5TxxodzY4FAAD+Jlulv0uXLtnamMViUUpKyp3kAQAATiApJUmPLnpU3+z/Rh4uHlrea7naV29vdiwAAPAP2Sr9qampeZ0DAAA4iaSUJPVY1EMr9q+Qh4uHvun1jcKqh5kdCwAAZIL39AMAgGxLTE5Uj0U99O0f38rT1VPf9PpGodVCzY4FAACycFulPz4+Xj/88IOOHz+upKSkDPc999xzuRIMAAA4lsTkRHX7upu+P/C9PF09taLXCrWr1s7sWAAA4CZyXPp/+ukndezYUQkJCYqPj1fJkiV1/vx5eXt7y9/fn9IPAEABdD35urp93U0rD6yUl6uXvu39rdpWbWt2LAAAcAvWnD5gxIgR6ty5sy5evCgvLy9t3bpVx44dU6NGjTR58uS8yAgAAEx0Pfm6Hln4iL3wf9fnOwo/AABOIself/fu3XrxxRdltVrl4uKixMREBQYG6t1339W///3vvMgIAABMcj35uros6KJVB1fJy9VL3/f5XsFVgs2OBQAAsinHpd/NzU1Wa9rD/P39dfz4cUlS8eLFdeLEidxNBwAATHPNdk0PL3hYqw+tlrebt1b2Xak2VdqYHQsAAORAjt/Tf88992jHjh2qUaOGWrVqpTFjxuj8+fP64osvdPfdd+dFRgAAkM/SC3/k4ci0wt9npVpVbmV2LAAAkEM5PtI/YcIElS1bVpL01ltvqUSJEho2bJhiY2P1ySef5HpAAACQvxJsCXpowUOKPBypIm5FtKrvKgo/AABOKsdH+hs3bmz/3t/fX+Hh4bkaCAAAmCfBlqDOX3XWuiPr7IW/RaUWZscCAAC3KcdH+oODg3Xp0qUblsfFxSk4mAv7AADgrOKT4vXg/Ae17sg6FXUvqvB+4RR+AACcXI6P9EdHRyspKemG5devX9eGDRtyJRQAAMhf8UnxevCrBxV9NFrF3IspvF+4mgU2MzsWAAC4Q9ku/b/88ov9+99//10xMTH22ykpKQoPD1f58uVzNx0AAMhz8Unx6jS/k3449oOKuRfT6n6rFRQYZHYsAACQC7Jd+hs2bCiLxSKLxZLpafxeXl6aNm1aroYDAAB562rSVXWa30nrj62Xj4ePVvdbrfsr3G92LAAAkEuyXfqPHDkiwzBUtWpVbd++XX5+fvb73N3d5e/vLxcXlzwJCQAAct+VxCvqOL+jNh7fKB8PH0X0i1DTCk3NjgUAAHJRtkt/pUqVJEmpqal5FgYAAOSPK4lX1GFeB206sUnFPYor4rEINSnfxOxYAAAgl+X4Qn6SdOjQIb3//vvau3evJKlOnTp6/vnnVa1atVwNBwAAcl9cYpw6zOugzSc2q7hHcUU+Fqn7yt9ndiwAAJAHcvyRfatXr1adOnW0fft21a9fX/Xr19e2bdtUt25dRUZG5kVGAACQS+IS49T+y/bafGKzfD19tab/Ggo/AAAFWI6P9I8aNUojRozQ22+/fcPyf/3rX2rXrl2uhQMAALnn8vXLaj+vvbae3KoSniUU+VikGpVrZHYsAACQh3J8pH/v3r0aPHjwDcsff/xx/f7777kSCgAA5K7L1y8r7Mswe+Ff038NhR8AgEIgx6Xfz89Pu3fvvmH57t275e/vnxuZAABALrp0/ZJCvwzVtlPbVNKrpNb2X6t7y95rdiwAAJAPsn16//jx4/XSSy9pyJAhGjp0qA4fPqxmzZpJkjZt2qR33nlHI0eOzLOgAAAg5y5dv6TQL0K14/QOe+FvGNDQ7FgAACCfZLv0jxs3Tk899ZRee+01FStWTFOmTNHo0aMlSeXKldPrr7+u5557Ls+CAgCAnLl47aJCvwzVj6d/VCmvUlrbf60aBDQwOxYAAMhH2S79hmFIkiwWi0aMGKERI0boypUrkqRixYrlTToAAHBbLly7oHZftNOuM7tU2ru01vZfq/pl6psdCwAA5LMcXb3fYrFkuE3ZBwDA8Vy4dkEhc0P0U8xPKu1dWuv6r1O9MvXMjgUAAEyQo9J/11133VD8/+nChQt3FAgAANy+PxP+VMgXIdods1t+3n5aN2Cd7va/2+xYAADAJDkq/ePGjVPx4sXzKgsAALgD5xPOK2RuiH4++7P8i/hrXf91qutf1+xYAADARDkq/b169eJj+QAAcEDnE86r7dy2+uXsLypTpIzWDVinOn51zI4FAABMlu3Sf6vT+gEAgDli42PVdm5b/XruV5UpUkZRA6JU26+22bEAAIADyPHV+wEAgOM4F39Obee21W/nflNA0QBFDYhSrdK1zI4FAAAcRLZLf2pqal7mAAAAOXQu/pyC5wRrT+welS1aVlEDolSzdE2zYwEAAAeSo/f0AwAAx3D26lkFzw3W77G/q1yxcooaEKW7St1ldiwAAOBgrGYHyK4LFy6ob9++8vHxka+vrwYPHqyrV6/edP1nn31WNWvWlJeXlypWrKjnnntOly9fzrCexWK54WvBggV5vTsAANy2mKsxajOnjX6P/V3li5VX9IBoCj8AAMiU0xzp79u3r86cOaPIyEjZbDYNGjRIQ4cO1fz58zNd//Tp0zp9+rQmT56sOnXq6NixY3rqqad0+vRpLV68OMO6s2bNUvv27e23fX1983JXAAC4bemFf9/5fWmFf2C0qpesbnYsAADgoJyi9O/du1fh4eHasWOHGjduLEmaNm2aOnbsqMmTJ6tcuXI3PObuu+/WkiVL7LerVaumt956S/369VNycrJcXf9/1319fRUQEJD3OwIAwB04c+WMgucGa9/5fargU0FRA6Io/AAA4KacovRv2bJFvr6+9sIvSSEhIbJardq2bZu6du2are1cvnxZPj4+GQq/JD3zzDN64oknVLVqVT311FMaNGjQTT+iMDExUYmJifbbcXFxkiSbzSabzZaTXctX6dkcOSOYkzNgRs6hoM3p9JXTajevnQ5cOKBAn0BF9I1QpWKVnH7/CtqcCiJm5ByYk3NgTo7PmWaU3YxOUfpjYmLk7++fYZmrq6tKliypmJiYbG3j/PnzeuONNzR06NAMy8ePH6/g4GB5e3srIiJCTz/9tK5evarnnnsuy21NnDhR48aNu2F5RESEvL29s5XHTJGRkWZHQDYwJ8fHjJxDQZjTBdsFvXrwVZ1OPC0/Nz/9p/x/tH/Lfu3XfrOj5ZqCMKeCjhk5B+bkHJiT43OGGSUkJGRrPVNL/6hRo/TOO+/cdJ29e/fe8fPExcWpU6dOqlOnjl5//fUM97322mv27++55x7Fx8dr0qRJNy39o0eP1siRIzNsPzAwUKGhofLx8bnjvHnFZrMpMjJS7dq1k5ubm9lxkAXm5PiYkXMoKHM6deWU2s1rp9OJp1WpeCVF9I1QFd8qZsfKNQVlTgUZM3IOzMk5MCfH50wzSj/j/FZMLf0vvviiBg4ceNN1qlatqoCAAJ07dy7D8uTkZF24cOGW78W/cuWK2rdvr2LFimnZsmW3HFzTpk31xhtvKDExUR4eHpmu4+Hhkel9bm5uDv+LITlPzsKOOTk+ZuQcnHlOJ+NOqt28djp44aAqFa+k6IHRquxb2exYecKZ51RYMCPnwJycA3NyfM4wo+zmM7X0+/n5yc/P75brBQUF6dKlS9q5c6caNWokSVq3bp1SU1PVtGnTLB8XFxensLAweXh4aMWKFfL09Lzlc+3evVslSpTIsvADAJAfTlw+oTZz2ujQxUOq7FtZ0QOiVcm3ktmxAACAk3GK9/TXrl1b7du315AhQzRjxgzZbDYNHz5cvXr1sl+5/9SpU2rbtq3mzp2rJk2aKC4uTqGhoUpISNCXX36puLg4++kPfn5+cnFx0bfffquzZ8/q/vvvl6enpyIjIzVhwgS99NJLZu4uAKCQO375uNrMaaPDFw+rim8VRQ2IovADAIDb4hSlX5LmzZun4cOHq23btrJarerWrZs+/PBD+/02m0379++3X8xg165d2rZtmySpevWMH2d05MgRVa5cWW5ubpo+fbpGjBghwzBUvXp1TZ06VUOGDMm/HQMA4G+OXTqmNnPa6MilI6paoqqiBkSpYvGKZscCAABOymlKf8mSJTV//vws769cubIMw7Dfbt26dYbbmWnfvr3at2+faxkBALgTxy4dU+s5rXX00lFVK1FNUQOiFFg80OxYAADAiTlN6QcAoCA7eumo2sxpYy/80QOjVcGngtmxAACAk6P0AwBgsiMXj6jNnDY6dvmYapSsoagBUSrvU97sWAAAoACwmh0AAIDC7PDFw2o9pzWFHwAA5AmO9AMAYJLDFw+r9ezWOhF3QneVuktRA6JUrlg5s2MBAIAChNIPAIAJDl04pNZzWutk3EnVLFVTUQOiVLZYWbNjAQCAAobSDwBAPjt44aBaz26tU1dOqVbpWooaEKWAogFmxwIAAAUQ7+kHACAfHfjzgL3w1y5dm8IPAADyFEf6AQDIJ3/8+YfazGmj01dOq45fHa3rv05lipYxOxYAACjAKP0AAOSD/ef3q82cNjpz9Yzq+tXVugHr5F/E3+xYAACggOP0fgAA8ti+8/vshf9u/7sp/AAAIN9wpB8AgDy0N3avgucGK+ZqjOr519Pa/mvlV8TP7FgAAKCQoPQDAJBHfo/9XcFzgnU2/qzql6mvtf3XqrR3abNjAQCAQoTT+wEAyAO/x/6uNnPa6Gz8WTUo04DCDwAATMGRfgAActlv535T8JxgxSbEqmFAQ615bI1KeZcyOxYAACiEKP0AAOSivxf+ewLu0Zr+a1TSq6TZsQAAQCHF6f0AAOSSX8/+qjZz2ig2IVb3lr2Xwg8AAExH6QcAIBf8HPOz2sxpo/MJ59WobCOteYzCDwAAzMfp/QAA3KGfY35W27lt9ee1P9W4XGNFPhYpX09fs2MBAABwpB8AgDuxO2a3gucG689rf+q+cvdR+AEAgEOh9AMAcJt2ndml4DnBunDtgpqWb0rhBwAADofSDwDAbdh1ZpdC5obo4vWLur/C/Vrdb7WKexY3OxYAAEAGvKcfAIAc2nl6p0K+CNGl65cUVCFI4f3C5ePhY3YsAACAG3CkHwCAHNhxaoe98DcLbEbhBwAADo0j/QAAZNP2U9sV+kWoLide1gOBD2hV31Uq5lHM7FgAAABZ4kg/AADZsO3kNrX7op0uJ15W84rNKfwAAMApUPoBALiFrSe3KvTLUMUlxqlFxRYUfgAA4DQ4vR8AgJvYcmKLwr4M05WkK2pZqaW+7/O9iroXNTsWAABAtnCkHwCALGw+sdle+FtXbq2VfVZS+AEAgFOh9AMAkIlNxzfZC3+bym30Xe/vVMS9iNmxAAAAcoTT+wEA+IeNxzeq/ZftFW+LV3CVYH3b+1t5u3mbHQsAACDHONIPAMDfbDi2wV7421ZpS+EHAABOjdIPAMBffjj6gzrM66B4W7zaVW1H4QcAAE6P0g8AgKToo9HqOL+j4m3xCq0Wqm96fSMvNy+zYwEAANwRSj8AoNCLOhKlTvM7KcGWoLBqYVreczmFHwAAFAhcyA8AUKitO7JOD85/UNeSr6l99fZa1nOZPF09zY4FAACQKzjSDwAotNYeXqtO8zvpWvI1dazRkcIPAAAKHEo/AKBQWnN4jR786kFdT76uTjU6aemjSyn8AACgwKH0AwAKnYhDEer8VWddT76uB+96UEseXSIPVw+zYwEAAOQ6Sj8AoFBZfXC1HvrqIV1Pvq7Od3XW4h6LKfwAAKDAovQDAAqN8IPhenjBw0pMSdTDNR/W4kcp/AAAoGCj9AMACoWVB1baC3+XWl30dY+v5e7ibnYsAACAPEXpBwAUeN//8b26LuyqpJQkda3VVV93p/ADAIDCwWlK/4ULF9S3b1/5+PjI19dXgwcP1tWrV2/6mNatW8tisWT4euqppzKsc/z4cXXq1Ene3t7y9/fXyy+/rOTk5LzcFQBAPvr+wPd65OtHlJSSpG61u2lh94Vyc3EzOxYAAEC+cDU7QHb17dtXZ86cUWRkpGw2mwYNGqShQ4dq/vz5N33ckCFDNH78ePttb29v+/cpKSnq1KmTAgICtHnzZp05c0b9+/eXm5ubJkyYkGf7AgDIH9svb9ekJZNkS7Wpe53umv/IfAo/AAAoVJyi9O/du1fh4eHasWOHGjduLEmaNm2aOnbsqMmTJ6tcuXJZPtbb21sBAQGZ3hcREaHff/9da9asUZkyZdSwYUO98cYb+te//qXXX39d7u6c+gkAzmrFHyv07tF3lWwkq0edHpr3yDwKPwAAKHScovRv2bJFvr6+9sIvSSEhIbJardq2bZu6du2a5WPnzZunL7/8UgEBAercubNee+01+9H+LVu2qF69eipTpox9/bCwMA0bNkx79uzRPffck+k2ExMTlZiYaL8dFxcnSbLZbLLZbHe0r3kpPZsjZwRzcgbMyPF9s/8b9VnWR8lGsrrX6q45D82RUiVbKjNzNLyeHB8zcg7MyTkwJ8fnTDPKbkanKP0xMTHy9/fPsMzV1VUlS5ZUTExMlo/r06ePKlWqpHLlyumXX37Rv/71L+3fv19Lly61b/fvhV+S/fbNtjtx4kSNGzfuhuUREREZ3j7gqCIjI82OgGxgTo6PGTmmrZe2atLRSUpRilr4tlBvj96KCI8wOxZugdeT42NGzoE5OQfm5PicYUYJCQnZWs/U0j9q1Ci98847N11n7969t739oUOH2r+vV6+eypYtq7Zt2+rQoUOqVq3abW939OjRGjlypP12XFycAgMDFRoaKh8fn9vebl6z2WyKjIxUu3bt5ObGKa6Oijk5PmbkuJbuW6rJyycrRSl6tPaj6uneU+1D2zMnB8bryfExI+fAnJwDc3J8zjSj9DPOb8XU0v/iiy9q4MCBN12natWqCggI0Llz5zIsT05O1oULF7J8v35mmjZtKkk6ePCgqlWrpoCAAG3fvj3DOmfPnpWkm27Xw8NDHh4eNyx3c3Nz+F8MyXlyFnbMyfExI8ey+PfF6rusr1KMFPWt11efdfpMEeERzMlJMCfHx4ycA3NyDszJ8TnDjLKbz9TS7+fnJz8/v1uuFxQUpEuXLmnnzp1q1KiRJGndunVKTU21F/ns2L17tySpbNmy9u2+9dZbOnfunP3tA5GRkfLx8VGdOnVyuDcAALMs2rNIvZf0VoqRosfqP6ZZD89Sakqq2bEAAABMZzU7QHbUrl1b7du315AhQ7R9+3Zt2rRJw4cPV69evexX7j916pRq1aplP3J/6NAhvfHGG9q5c6eOHj2qFStWqH///mrZsqXq168vSQoNDVWdOnX02GOP6eeff9bq1av16quv6plnnsn0SD4AwPEs/G2hvfD3b9Bfsx6eJReri9mxAAAAHIJTlH4p7Sr8tWrVUtu2bdWxY0c1b95cn376qf1+m82m/fv32y9m4O7urjVr1ig0NFS1atXSiy++qG7duunbb7+1P8bFxUXfffedXFxcFBQUpH79+ql///4aP358vu8fACDnFvy2QH2Xpp3SP6DBAM18aCaFHwAA4G+c4ur9klSyZEnNnz8/y/srV64swzDstwMDA/XDDz/ccruVKlXSypUrcyUjACD/zP91vh5b9phSjVQNajhIn3X+jMIPAADwD05zpB8AgHTzfplnL/yPN3xcnz/0OYUfAAAgE5R+AIBT+fKXL9V/eX+lGql64p4n9NlDn8lq4X9nAAAAmeFvSQAApzH357nqvyyt8A+5d4g+6fwJhR8AAOAm+JsSAMApzNk9RwOXD5QhQ082elIzHpxB4QcAALgF/rYEAHB4s3fP1qBvBsmQoacaPaWPOn1E4QcAAMgG/sYEAHBoM3+aqce/eVyGDD3d+GkKPwAAQA7wtyYAgMP6367/afCKwTJk6Jn7ntF/O/5XFovF7FgAAABOg9IPAHBIn+38TE98+4Qk6dkmz2pah2kUfgAAgByi9AMAHM6nOz/V0O+GSpKeb/q8Pmj/AYUfAADgNlD6AQAOZcaPM/Tkd09Kkl5o+oLeC3uPwg8AAHCbKP0AAIfx8Y6PNez7YZKkEfeP0NSwqRR+AACAO0DpBwA4hOnbp+vplU9Lkl4MelFTQqdQ+AEAAO4QpR8AYLr/bv+vhq8aLkl6udnLmtRuEoUfAAAgF1D6AQCm+nDbh3p21bOSpFeavaJ3Qt6h8AMAAOQSSj8AwDTvb31fz4c/L0ka9cAovR3yNoUfAAAgF1H6AQCmeG/LexqxeoQk6d/N/60JbSdQ+AEAAHIZpR8AkO+mbpmqkREjJUn/afEfvRn8JoUfAAAgD1D6AQD5avLmyXox4kVJ0mstX9Mbbd6g8AMAAOQRSj8AIN+8u+ldvRz5siRpbKuxGt9mPIUfAAAgD7maHQAAUDi8s/EdjVo7SpL0eqvXNbb1WJMTAQAAFHwc6QcA5LmJGybaC/+41uMo/AAAAPmEI/0AgDz11vq39GrUq5KkN9q8oVdbvmpyIgAAgMKDI/0AgDzz5vo37YX/zTZvUvgBAADyGUf6AQB5YvwP4zU2Ou00/gnBEzS6xWiTEwEAABQ+lH4AQK57Pfp1jfthnCTp7bZv61/N/2VyIgAAgMKJ0g8AyDWGYej16Nc1fv14SdI7Ie/olQdeMTkVAABA4UXpBwDkCsMwNDZ6rN5Y/4YkaVK7SXqp2UsmpwIAACjcKP0AgDtmGIZei3pNb214S5I0JXSKRgaNNDkVAAAAKP0AgDtiGIZeXfeqJmycIEmaGjpVI4JGmJwKAAAAEqUfAHAHDMPQv9f+W29veluS9H7Y+3r+/udNTgUAAIB0lH4AwG0xDEOj1ozSu5vflSR90P4DPdf0OZNTAQAA4O8o/QCAHDMMQ/9a8y9N2jxJkjStwzQNbzLc5FQAAAD4J0o/ACBHDMPQy5Eva8qWKZKk/3b4r55p8ozJqQAAAJAZSj8AINsMw9CLES/qva3vSZI+6viRht03zORUAAAAyAqlHwCQLYZhaOTqkXp/2/uSpI87faynGj9lbigAAADcFKUfAHBLhmHohfAX9OH2DyVJnzz4iYY2GmpyKgAAANwKpR8AcFOGYej58Oc1bfs0SdKnD36qIY2GmJwKAAAA2UHpBwBkyTAMPbvqWU3fMV0WWfRZ5880+N7BZscCAABANlH6AQCZSjVSNXzlcH3848eyyKLPH/pcj9/zuNmxAAAAkAOUfgDADVKNVD3z/TOasXOGLLJo5sMzNbDhQLNjAQAAIIco/QCADFKNVD39/dP6ZOcnssiiWQ/P0oCGA8yOBQAAgNtgNTtAdl24cEF9+/aVj4+PfH19NXjwYF29ejXL9Y8ePSqLxZLp16JFi+zrZXb/ggUL8mOXAMDhpBqpeuq7p+yFf06XORR+AAAAJ+Y0R/r79u2rM2fOKDIyUjabTYMGDdLQoUM1f/78TNcPDAzUmTNnMiz79NNPNWnSJHXo0CHD8lmzZql9+/b2276+vrmeHwAcXaqRqie/fVKf//S5rBar5nSZo371+5kdCwAAAHfAKUr/3r17FR4erh07dqhx48aSpGnTpqljx46aPHmyypUrd8NjXFxcFBAQkGHZsmXL9Oijj6po0aIZlvv6+t6wLgAUJqlGqoasGKKZu2fKarFqbpe56lu/r9mxAAAAcIecovRv2bJFvr6+9sIvSSEhIbJardq2bZu6du16y23s3LlTu3fv1vTp02+475lnntETTzyhqlWr6qmnntKgQYNksViy3FZiYqISExPtt+Pi4iRJNptNNpstJ7uWr9KzOXJGMCdnUNBmlGqk6snvn9ScX+bIarFq9kOz9WjtR51+/wranAoq5uT4mJFzYE7OgTk5PmeaUXYzOkXpj4mJkb+/f4Zlrq6uKlmypGJiYrK1jf/973+qXbu2mjVrlmH5+PHjFRwcLG9vb0VEROjpp5/W1atX9dxzz2W5rYkTJ2rcuHE3LI+IiJC3t3e28pgpMjLS7AjIBubk+ArCjFKMFE0/MV3rLqyTVVaNqDhCPsd8tPLYSrOj5ZqCMKfCgDk5PmbkHJiTc2BOjs8ZZpSQkJCt9Uwt/aNGjdI777xz03X27t17x89z7do1zZ8/X6+99toN9/192T333KP4+HhNmjTppqV/9OjRGjlypP12XFycAgMDFRoaKh8fnzvOm1dsNpsiIyPVrl07ubm5mR0HWWBOjq+gzCglNUVDvh+idRfWycXiojkPz9GjdR41O1auKShzKuiYk+NjRs6BOTkH5uT4nGlG6Wec34qppf/FF1/UwIEDb7pO1apVFRAQoHPnzmVYnpycrAsXLmTrvfiLFy9WQkKC+vfvf8t1mzZtqjfeeEOJiYny8PDIdB0PD49M73Nzc3P4XwzJeXIWdszJ8TnzjFJSUzT4u8H68tcv5WJx0VfdvlKPuj3MjpUnnHlOhQlzcnzMyDkwJ+fAnByfM8wou/lMLf1+fn7y8/O75XpBQUG6dOmSdu7cqUaNGkmS1q1bp9TUVDVt2vSWj//f//6nhx56KFvPtXv3bpUoUSLLwg8Azi4lNUUDlg/QvF/nycXiogXdF6h7ne5mxwIAAEAecIr39NeuXVvt27fXkCFDNGPGDNlsNg0fPly9evWyX7n/1KlTatu2rebOnasmTZrYH3vw4EGtX79eK1fe+P7Ub7/9VmfPntX9998vT09PRUZGasKECXrppZfybd8AID8lpyZrwPIBmv/rfLlaXbWg2wJ1q9PN7FgAAADII05R+iVp3rx5Gj58uNq2bSur1apu3brpww8/tN9vs9m0f//+Gy5mMHPmTFWoUEGhoaE3bNPNzU3Tp0/XiBEjZBiGqlevrqlTp2rIkCF5vj8AkN+SU5P12LLHtOC3BXK1uurr7l+ra+1bf/oJAAAAnJfTlP6SJUtq/vz5Wd5fuXJlGYZxw/IJEyZowoQJmT6mffv2at++fa5lBABHlZyarH5L+2nhnoVytbpqUY9F6lKri9mxAAAAkMecpvQDAG6PLcWmvkv7atHvi+RmddOiHov0cK2HzY4FAACAfEDpB4ACzJZiU5+lfbT498Vys7ppyaNL1LlmZ7NjAQAAIJ9Q+gGggLKl2NR7SW8t2btE7i7uWvLoEj1414NmxwIAAEA+ovQDQAGUlJKkXot7adm+ZXJ3cdfSR5eq012dzI4FAACAfEbpB4ACJiklST0X99Tyfcvl7uKuZT2XqWONjmbHAgAAgAko/QBQgCSlJOnRRY/qm/3fyMPFQ8t7LVf76nxKCQAAQGFF6QeAAiIxOVE9FvXQt398Kw8XD33T6xuFVQ8zOxYAAABMROkHgAIgMTlR3Rd113d/fCdPV0990+sbhVYLNTsWAAAATEbpBwAnl5icqG5fd9P3B76Xp6unVvRaoXbV2pkdCwAAAA6A0g8ATux68nV1+7qbVh5YKS9XL33b+1u1rdrW7FgAAABwEJR+AHBS15Ovq+vCrgo/GC4vVy991+c7BVcJNjsWAAAAHAilHwCc0PXk6+qyoItWH1otL1cvfd/ne7Wp0sbsWAAAAHAwlH4AcDLXbNfUZWEXRRyKkLebt77v871aV25tdiwAAAA4IEo/ADiRa7ZrenjBw4o8HClvN2+t7LNSrSq3MjsWAAAAHBSlHwCcRIItQQ8veFhrDq9REbciWtl3pVpWaml2LAAAADgwSj8AOIEEW4I6f9VZ646sU1H3olrVd5WaV2xudiwAAAA4OEo/ADi4+KR4df6qs6KORqmoe1GF9w3XAxUfMDsWAAAAnAClHwAcWHxSvB786kFFH41WMfdiCu8XrmaBzcyOBQAAACdB6QcAB3U16ao6ze+k9cfWq5h7Ma3ut1pBgUFmxwIAAIATofQDgAO6mnRVHed11IbjG+Tj4aPV/Vbr/gr3mx0LAAAATobSDwAO5kriFXWc31Ebj2+Uj4ePIvpFqGmFpmbHAgAAgBOi9AOAA7mSeEUd5nXQphObVNyjuCIei1CT8k3MjgUAAAAnRekHAAcRlxinDvM6aPOJzSruUVyRj0XqvvL3mR0LAAAATozSDwAOIC4xTu2/bK8tJ7fI19NXkY9FqnG5xmbHAgAAgJOj9AOAyS5fv6z289pr68mtKuFZQmv6r9G9Ze81OxYAAAAKAEo/AJjo8vXLCvsyTNtObaPwAwAAINdR+gHAJJeuX1LYl2Hafmq7SnqV1JrH1uiesveYHQsAAAAFCKUfAExw8dpFhX4Zqh9P/6hSXqW0tv9aNQhoYHYsAAAAFDCUfgDIZxevXVS7L9pp55mdFH4AAADkKUo/AOSjC9cuqN0X7bTrzC6V9i6ttf3Xqn6Z+mbHAgAAQAFF6QeAfHLh2gWFzA3RTzE/yc/bT+sGrNPd/nebHQsAAAAFGKUfAPLBnwl/KuSLEO2O2U3hBwAAQL6h9ANAHjufcF4hc0P089mf5V/EX+v6r1Nd/7pmxwIAAEAhQOkHgDx0PuG82s5tq1/O/qIyRcpo3YB1quNXx+xYAAAAKCQo/QCQR2LjY9V2blv9eu5XlSlSRlEDolTbr7bZsQAAAFCIUPoBIA+ciz+ntnPb6rdzvymgaICiBkSpVulaZscCAABAIUPpB4Bcdi7+nILnBGtP7B6VLVpWUQOiVLN0TbNjAQAAoBCi9ANALjp79ayC5wbr99jfVa5YOUUNiNJdpe4yOxYAAAAKKUo/AOSSmKsxCp4TrL3n96p8sfKKGhClGqVqmB0LAAAAhRilHwBywZkrZxQ8N1j7zu9TBZ8KihoQpeolq5sdCwAAAIUcpR8A7tCZK2fUZk4b7f9zvyr4VFD0gGhVK1nN7FgAAACArGYHAABndvrKabWe01r7/9yvQJ9ACj8AAAAcitOU/rfeekvNmjWTt7e3fH19s/UYwzA0ZswYlS1bVl5eXgoJCdGBAwcyrHPhwgX17dtXPj4+8vX11eDBg3X16tU82AMABc2pK6fUenZr/fHnH6pYvKKiB1L4AQAA4FicpvQnJSWpR48eGjZsWLYf8+677+rDDz/UjBkztG3bNhUpUkRhYWG6fv26fZ2+fftqz549ioyM1Hfffaf169dr6NChebELAAqQP5P+VLt57XTgwgFVKl5J0QOiVbVEVbNjAQAAABk4zXv6x40bJ0maPXt2ttY3DEPvv/++Xn31VT388MOSpLlz56pMmTJavny5evXqpb179yo8PFw7duxQ48aNJUnTpk1Tx44dNXnyZJUrVy5P9sUMCbYEfbfvO+26tEuJ+xLl6uo0oy90kpOTmZODS7Ql6tWDr+pM0pm0wj8wWpV9K5sdCwAAALhBgW0UR44cUUxMjEJCQuzLihcvrqZNm2rLli3q1auXtmzZIl9fX3vhl6SQkBBZrVZt27ZNXbt2zXTbiYmJSkxMtN+Oi4uTJNlsNtlstjzaoztz5vIZ9VzaM+3GUVOjILuOmh0At1KpeCWt6bdG5YuUd9jXfmGWPhNm49iYk+NjRs6BOTkH5uT4nGlG2c1YYEt/TEyMJKlMmTIZlpcpU8Z+X0xMjPz9/TPc7+rqqpIlS9rXyczEiRPtZx78XUREhLy9ve80ep64ZLuk2kVqmx0DKDBKupXUwHIDtWfTHu3RHrPj4CYiIyPNjoBsYE6Ojxk5B+bkHJiT43OGGSUkJGRrPVNL/6hRo/TOO+/cdJ29e/eqVq1a+ZQoe0aPHq2RI0fab8fFxSkwMFChoaHy8fExMdnN9bD1UGRkpNq1ayc3Nzez4yALNpuNOTk4ZuQcmJNzYE6Ojxk5B+bkHJiT43OmGaWfcX4rppb+F198UQMHDrzpOlWr3t6FsQICAiRJZ8+eVdmyZe3Lz549q4YNG9rXOXfuXIbHJScn68KFC/bHZ8bDw0MeHh43LHdzc3P4XwzJeXIWdszJ8TEj58CcnANzcnzMyDkwJ+fAnByfM8wou/lMLf1+fn7y8/PLk21XqVJFAQEBWrt2rb3kx8XFadu2bfZPAAgKCtKlS5e0c+dONWrUSJK0bt06paamqmnTpnmSCwAAAACA/OI0H9l3/Phx7d69W8ePH1dKSop2796t3bt36+rVq/Z1atWqpWXLlkmSLBaLXnjhBb355ptasWKFfv31V/Xv31/lypVTly5dJEm1a9dW+/btNWTIEG3fvl2bNm3S8OHD1atXrwJ15X4AAAAAQOHkNBfyGzNmjObMmWO/fc8990iSoqKi1Lp1a0nS/v37dfnyZfs6r7zyiuLj4zV06FBdunRJzZs3V3h4uDw9Pe3rzJs3T8OHD1fbtm1ltVrVrVs3ffjhh/mzUwAAAAAA5CGnKf2zZ8/W7Nmzb7qOYRgZblssFo0fP17jx4/P8jElS5bU/PnzcyMiAAAAAAAOxWlO7wcAAAAAADlD6QcAAAAAoICi9AMAAAAAUEBR+gEAAAAAKKAo/QAAAAAAFFCUfgAAAAAACihKPwAAAAAABRSlHwAAAACAAorSDwAAAABAAUXpBwAAAACggKL0AwAAAABQQFH6AQAAAAAooCj9AAAAAAAUUK5mBygIDMOQJMXFxZmc5OZsNpsSEhIUFxcnNzc3s+MgC8zJ8TEj58CcnANzcnzMyDkwJ+fAnByfM80ovX+m99GsUPpzwZUrVyRJgYGBJicBAAAAABQmV65cUfHixbO832Lc6p8FcEupqak6ffq0ihUrJovFYnacLMXFxSkwMFAnTpyQj4+P2XGQBebk+JiRc2BOzoE5OT5m5ByYk3NgTo7PmWZkGIauXLmicuXKyWrN+p37HOnPBVarVRUqVDA7Rrb5+Pg4/C8wmJMzYEbOgTk5B+bk+JiRc2BOzoE5OT5nmdHNjvCn40J+AAAAAAAUUJR+AAAAAAAKKEp/IeLh4aGxY8fKw8PD7Ci4Cebk+JiRc2BOzoE5OT5m5ByYk3NgTo6vIM6IC/kBAAAAAFBAcaQfAAAAAIACitIPAAAAAEABRekHAAAAAKCAovQDAAAAAFBAUfoLmOnTp6ty5cry9PRU06ZNtX379puuv2jRItWqVUuenp6qV6+eVq5cmU9JC7eczGn27NmyWCwZvjw9PfMxbeGzfv16de7cWeXKlZPFYtHy5ctv+Zjo6Gjde++98vDwUPXq1TV79uw8z1nY5XRO0dHRN7yWLBaLYmJi8idwITRx4kTdd999KlasmPz9/dWlSxft37//lo/j/03553ZmxP+X8t/HH3+s+vXry8fHRz4+PgoKCtKqVatu+hheR/kvp3PitWS+t99+WxaLRS+88MJN13P21xOlvwD5v/buPybq+o8D+PMA7zgsfgnBmeQw6AJcIBAG2lBR0VhFYysbsXPmTIMGWBqxFTn/wDbDWhnSD3BFdaUO29Qk0MJJUnRwduLl8gdkC6SWkR4N8+79/eM7P+uAM468++CH52P7bN7n8/p87nW89tp7r7v7nJ988gnWr1+PiooKdHR0IDExEdnZ2ejv7x81/uuvv8bjjz+OJ598Ep2dncjNzUVubi5OnDjh5cwnF3frBACBgYHo7e2Vtp6eHi9mPPnYbDYkJiZi+/btY4o/d+4ccnJysHDhQpjNZpSUlGD16tVobGz0cKaTm7t1uubUqVNO/XTbbbd5KENqaWlBYWEh2tra0NTUhL///htLly6FzWZzeQ7XJu8aT40ArkveNmPGDGzZsgUmkwnfffcdFi1ahIcffhhdXV2jxrOP5OFunQD2kpza29tRU1ODe+6557pxiugnQYqRlpYmCgsLpcd2u11Mnz5dVFZWjhr/6KOPipycHKd9c+fOFU899ZRH85zs3K1TXV2dCAoK8lJ2NBwA0dDQcN2YjRs3ioSEBKd9jz32mMjOzvZgZvRPY6nTl19+KQCIixcveiUnGqm/v18AEC0tLS5juDbJayw14ro0MYSEhIh333131GPso4njenViL8nn0qVLIjY2VjQ1NYnMzExRXFzsMlYJ/cRP+hXiypUrMJlMWLx4sbTPx8cHixcvxrFjx0Y959ixY07xAJCdne0ynv678dQJAC5fvoyZM2ciKirqX98xJu9jL91ckpKSoNPpsGTJErS2tsqdzqQyMDAAAAgNDXUZw36S11hqBHBdkpPdbofRaITNZkN6evqoMewj+Y2lTgB7SS6FhYXIyckZ0SejUUI/cehXiN9++w12ux0RERFO+yMiIlzer9rX1+dWPP1346mTXq9HbW0tPvvsM9TX18PhcCAjIwM///yzN1KmMXDVS3/++Sf++usvmbKi4XQ6HXbs2IE9e/Zgz549iIqKwoIFC9DR0SF3apOCw+FASUkJ5s2bh9mzZ7uM49okn7HWiOuSPCwWC2655RZoNBqsXbsWDQ0NiI+PHzWWfSQfd+rEXpKH0WhER0cHKisrxxSvhH7ykzsBIrq+9PR0p3eIMzIyEBcXh5qaGmzevFnGzIhuLnq9Hnq9XnqckZGBM2fOYNu2bfjggw9kzGxyKCwsxIkTJ3D06FG5UyEXxlojrkvy0Ov1MJvNGBgYwO7du2EwGNDS0uJyoCR5uFMn9pL3nT9/HsXFxWhqappUP5rIoV8hwsLC4OvriwsXLjjtv3DhAiIjI0c9JzIy0q14+u/GU6fhpkyZgjlz5uD06dOeSJHGwVUvBQYGQqvVypQVjUVaWhqHUC8oKirCvn37cOTIEcyYMeO6sVyb5OFOjYbjuuQdarUaMTExAICUlBS0t7fj9ddfR01NzYhY9pF83KnTcOwlzzOZTOjv70dycrK0z26348iRI3jzzTcxNDQEX19fp3OU0E/8er9CqNVqpKSk4NChQ9I+h8OBQ4cOubyPKD093SkeAJqamq573xH9N+Op03B2ux0WiwU6nc5TaZKb2Es3L7PZzF7yICEEioqK0NDQgMOHDyM6Ovpfz2E/edd4ajQc1yV5OBwODA0NjXqMfTRxXK9Ow7GXPC8rKwsWiwVms1naUlNTkZ+fD7PZPGLgBxTST3L/kiDdOEajUWg0GrFz505x8uRJsWbNGhEcHCz6+vqEEEIUFBSIsrIyKb61tVX4+fmJrVu3CqvVKioqKsSUKVOExWKR6yVMCu7WadOmTaKxsVGcOXNGmEwmsWLFCuHv7y+6urrkegmKd+nSJdHZ2Sk6OzsFAFFVVSU6OztFT0+PEEKIsrIyUVBQIMWfPXtWBAQEiA0bNgir1Sq2b98ufH19xcGDB+V6CZOCu3Xatm2b2Lt3r/jxxx+FxWIRxcXFwsfHRzQ3N8v1EhRv3bp1IigoSHz11Veit7dX2gYHB6UYrk3yGk+NuC55X1lZmWhpaRHnzp0T33//vSgrKxMqlUp88cUXQgj20UThbp3YSxPD8F/vV2I/cehXmDfeeEPccccdQq1Wi7S0NNHW1iYdy8zMFAaDwSn+008/FXfddZdQq9UiISFB7N+/38sZT07u1KmkpESKjYiIEA888IDo6OiQIevJ49p/7TZ8u1YXg8EgMjMzR5yTlJQk1Gq1mDVrlqirq/N63pONu3V65ZVXxJ133in8/f1FaGioWLBggTh8+LA8yU8So9UHgFN/cG2S13hqxHXJ+1atWiVmzpwp1Gq1CA8PF1lZWdIgKQT7aKJwt07spYlh+NCvxH5SCSGE975XQERERERERETewnv6iYiIiIiIiBSKQz8RERERERGRQnHoJyIiIiIiIlIoDv1ERERERERECsWhn4iIiIiIiEihOPQTERERERERKRSHfiIiIiIiIiKF4tBPREREREREpFAc+omIiMgjuru7oVKpYDabPfYcK1euRG5urseuT0REdLPj0E9ERESjWrlyJVQq1Yht2bJlYzo/KioKvb29mD17toczJSIiIlf85E6AiIiIJq5ly5ahrq7OaZ9GoxnTub6+voiMjPREWkRERDRG/KSfiIiIXNJoNIiMjHTaQkJCAAAqlQrV1dVYvnw5tFotZs2ahd27d0vnDv96/8WLF5Gfn4/w8HBotVrExsY6vaFgsViwaNEiaLVaTJs2DWvWrMHly5el43a7HevXr0dwcDCmTZuGjRs3QgjhlK/D4UBlZSWio6Oh1WqRmJjolBMREdFkw6GfiIiIxu3FF19EXl4ejh8/jvz8fKxYsQJWq9Vl7MmTJ/H555/DarWiuroaYWFhAACbzYbs7GyEhISgvb0du3btQnNzM4qKiqTzX331VezcuRO1tbU4evQofv/9dzQ0NDg9R2VlJd5//33s2LEDXV1dKC0txRNPPIGWlhbP/RGIiIgmMJUY/hY5EREREf5/T399fT38/f2d9peXl6O8vBwqlQpr165FdXW1dOy+++5DcnIy3nrrLXR3dyM6OhqdnZ1ISkrCQw89hLCwMNTW1o54rnfeeQfPP/88zp8/j6lTpwIADhw4gAcffBC//PILIiIiMH36dJSWlmLDhg0AgKtXryI6OhopKSnYu3cvhoaGEBoaiubmZqSnp0vXXr16NQYHB/HRRx954s9EREQ0ofGefiIiInJp4cKFTkM9AISGhkr//udwfe2xq1/rX7duHfLy8tDR0YGlS5ciNzcXGRkZAACr1YrExERp4AeAefPmweFw4NSpU/D390dvby/mzp0rHffz80Nqaqr0Ff/Tp09jcHAQS5YscXreK1euYM6cOe6/eCIiIgXg0E9EREQuTZ06FTExMTfkWsuXL0dPTw8OHDiApqYmZGVlobCwEFu3br0h1792///+/ftx++23Ox0b648PEhERKQ3v6SciIqJxa2trG/E4Li7OZXx4eDgMBgPq6+vx2muv4e233wYAxMXF4fjx47DZbFJsa2srfHx8oNfrERQUBJ1Oh2+++UY6fvXqVZhMJulxfHw8NBoNfvrpJ8TExDhtUVFRN+olExER3VT4ST8RERG5NDQ0hL6+Pqd9fn5+0g/w7dq1C6mpqZg/fz4+/PBDfPvtt3jvvfdGvdZLL72ElJQUJCQkYGhoCPv27ZPeIMjPz0dFRQUMBgNefvll/Prrr3jmmWdQUFCAiIgIAEBxcTG2bNmC2NhY3H333aiqqsIff/whXf/WW2/Fc889h9LSUjgcDsyfPx8DAwNobW1FYGAgDAaDB/5CREREExuHfiIiInLp4MGD0Ol0Tvv0ej1++OEHAMCmTZtgNBrx9NNPQ6fT4eOPP0Z8fPyo11Kr1XjhhRfQ3d0NrVaL+++/H0ajEQAQEBCAxsZGFBcX495770VAQADy8vJQVVUlnf/ss8+it7cXBoMBPj4+WLVqFR555BEMDAxIMZs3b0Z4eDgqKytx9uxZBAcHIzk5GeXl5Tf6T0NERHRT4K/3ExER0bioVCo0NDQgNzdX7lSIiIjIBd7TT0RERERERKRQHPqJiIiIiIiIFIr39BMREdG48A5BIiKiiY+f9BMREREREREpFId+IiIiIiIiIoXi0E9ERERERESkUBz6iYiIiIiIiBSKQz8RERERERGRQnHoJyIiIiIiIlIoDv1ERERERERECsWhn4iIiIiIiEih/gd3UWOYQcTzSwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot total loss per episode\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_loss_per_episode, label='Total Loss', color='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss Per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot rewards for both agents\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(agent1_rewards, label=parallel_env.agents[0], color='green')\n",
    "plt.plot(agent2_rewards, label=parallel_env.agents[1], color='orange')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward Per Episode for Each Agent')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 models saved:\n",
      "  Actor: maddpg_trained_models_Dec_14_test/agent_0_actor.pth\n",
      "  Critic: maddpg_trained_models_Dec_14_test/agent_0_critic.pth\n",
      "  Actor Target: maddpg_trained_models_Dec_14_test/agent_0_actor_target.pth\n",
      "  Critic Target: maddpg_trained_models_Dec_14_test/agent_0_critic_target.pth\n",
      "Agent 1 models saved:\n",
      "  Actor: maddpg_trained_models_Dec_14_test/agent_1_actor.pth\n",
      "  Critic: maddpg_trained_models_Dec_14_test/agent_1_critic.pth\n",
      "  Actor Target: maddpg_trained_models_Dec_14_test/agent_1_actor_target.pth\n",
      "  Critic Target: maddpg_trained_models_Dec_14_test/agent_1_critic_target.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save models\n",
    "save_dir = \"maddpg_trained_models_Dec_14_test\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save each agent's models\n",
    "for idx, agent in enumerate(maddpg.agents):\n",
    "    actor_path = os.path.join(save_dir, f\"agent_{idx}_actor.pth\")\n",
    "    critic_path = os.path.join(save_dir, f\"agent_{idx}_critic.pth\")\n",
    "    actor_target_path = os.path.join(save_dir, f\"agent_{idx}_actor_target.pth\")\n",
    "    critic_target_path = os.path.join(save_dir, f\"agent_{idx}_critic_target.pth\")\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(agent.actor.state_dict(), actor_path)\n",
    "    torch.save(agent.critic.state_dict(), critic_path)\n",
    "    torch.save(agent.actor_target.state_dict(), actor_target_path)\n",
    "    torch.save(agent.critic_target.state_dict(), critic_target_path)\n",
    "\n",
    "    print(f\"Agent {idx} models saved:\")\n",
    "    print(f\"  Actor: {actor_path}\")\n",
    "    print(f\"  Critic: {critic_path}\")\n",
    "    print(f\"  Actor Target: {actor_target_path}\")\n",
    "    print(f\"  Critic Target: {critic_target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 models loaded:\n",
      "  Actor: maddpg_trained_models_Dec_14_test/agent_0_actor.pth\n",
      "  Critic: maddpg_trained_models_Dec_14_test/agent_0_critic.pth\n",
      "  Actor Target: maddpg_trained_models_Dec_14_test/agent_0_actor_target.pth\n",
      "  Critic Target: maddpg_trained_models_Dec_14_test/agent_0_critic_target.pth\n",
      "Agent 1 models loaded:\n",
      "  Actor: maddpg_trained_models_Dec_14_test/agent_1_actor.pth\n",
      "  Critic: maddpg_trained_models_Dec_14_test/agent_1_critic.pth\n",
      "  Actor Target: maddpg_trained_models_Dec_14_test/agent_1_actor_target.pth\n",
      "  Critic Target: maddpg_trained_models_Dec_14_test/agent_1_critic_target.pth\n"
     ]
    }
   ],
   "source": [
    "# Directory to load models from\n",
    "load_dir = \"maddpg_trained_models_Dec_14_test\"\n",
    "\n",
    "# Load each agent's models\n",
    "for idx, agent in enumerate(maddpg.agents):\n",
    "    actor_path = os.path.join(load_dir, f\"agent_{idx}_actor.pth\")\n",
    "    critic_path = os.path.join(load_dir, f\"agent_{idx}_critic.pth\")\n",
    "    actor_target_path = os.path.join(load_dir, f\"agent_{idx}_actor_target.pth\")\n",
    "    critic_target_path = os.path.join(load_dir, f\"agent_{idx}_critic_target.pth\")\n",
    "    \n",
    "    # Load models\n",
    "    agent.actor.load_state_dict(torch.load(actor_path))\n",
    "    agent.critic.load_state_dict(torch.load(critic_path))\n",
    "    agent.actor_target.load_state_dict(torch.load(actor_target_path))\n",
    "    agent.critic_target.load_state_dict(torch.load(critic_target_path))\n",
    "\n",
    "    print(f\"Agent {idx} models loaded:\")\n",
    "    print(f\"  Actor: {actor_path}\")\n",
    "    print(f\"  Critic: {critic_path}\")\n",
    "    print(f\"  Actor Target: {actor_target_path}\")\n",
    "    print(f\"  Critic Target: {critic_target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs138_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
