{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.atari import boxing_v2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Boxing environment\n",
    "env = boxing_v2.env(render_mode=\"rgb_array\") \n",
    "env.reset(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple random policy for demonstration\n",
    "def random_policy(observation, action_space):\n",
    "    return action_space().sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for agent in env.agent_iter():\n",
    "#     observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "#     if termination or truncation:\n",
    "#         action = None\n",
    "#     else:\n",
    "#         action = env.action_space(agent).sample()\n",
    "#     env.step(action)\n",
    "#     env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import PPOAgent\n",
    "from buffers import RolloutBuffer\n",
    "from ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check torch installation\n",
    "# !python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the PPOAgent and Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test setup\n",
    "# def test_agent_and_buffer():\n",
    "#     # Initialize environment\n",
    "#     env = boxing_v2.env(render_mode=\"rgb_array\")\n",
    "#     env.reset()\n",
    "\n",
    "#     # Observation shape and action space\n",
    "#     obs_shape = (3, 210, 160)  # Example shape for Atari observations\n",
    "#     action_space = env.action_space(\"first_0\")\n",
    "\n",
    "#     # Initialize agent and buffer\n",
    "#     agent = PPOAgent(obs_shape, action_space)\n",
    "#     buffer = RolloutBuffer()\n",
    "\n",
    "#     # Test environment interaction\n",
    "#     print(\"Testing interaction with Boxing environment...\")\n",
    "#     for agent_name in env.agent_iter(10):  # Interact for 10 steps\n",
    "#         obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "#         if obs is not None:\n",
    "#             # Normalize the observation for the network\n",
    "#             obs_tensor = torch.tensor(obs / 255.0, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # (C, H, W)\n",
    "#             action_probs = agent.forward_policy(obs_tensor)\n",
    "#             action = torch.multinomial(action_probs, 1).item()  # Sample action\n",
    "#             log_prob = torch.log(action_probs.squeeze(0)[action])  # Compute log-prob\n",
    "\n",
    "#             # Store data in the buffer\n",
    "#             buffer.store(obs, action, log_prob.item(), reward, termination or truncation)\n",
    "\n",
    "#             # Print action probabilities and chosen action\n",
    "#             print(f\"Action Probabilities: {action_probs.detach().numpy()}\")\n",
    "#             print(f\"Chosen Action: {action}, Log Prob: {log_prob.item()}\")\n",
    "\n",
    "#         env.step(action if not termination and not truncation else None)\n",
    "\n",
    "#     # Check buffer contents\n",
    "#     print(\"\\nTesting RolloutBuffer contents...\")\n",
    "#     print(f\"Observations: {len(buffer.observations)}\")\n",
    "#     print(f\"Actions: {len(buffer.actions)}\")\n",
    "#     print(f\"Log Probs: {len(buffer.log_probs)}\")\n",
    "#     print(f\"Rewards: {len(buffer.rewards)}\")\n",
    "#     print(f\"Dones: {len(buffer.dones)}\")\n",
    "\n",
    "#     # Test forward pass through the value network\n",
    "#     print(\"\\nTesting value network...\")\n",
    "#     value = agent.forward_value(obs_tensor)\n",
    "#     print(f\"Value Estimate: {value.item()}\")\n",
    "\n",
    "#     # Clear the buffer\n",
    "#     buffer.clear()\n",
    "#     print(\"\\nRolloutBuffer cleared. Current size:\", len(buffer.observations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_agent_and_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from pettingzoo.atari import boxing_v2\n",
    "\n",
    "# # Test configuration\n",
    "# obs_shape = (3, 210, 160)  # Atari observation shape\n",
    "# action_space = type('', (), {'n': 6})()  # Mock action space with 6 actions\n",
    "# num_steps = 10  # Number of steps in the rollout\n",
    "\n",
    "# # Initialize PPO and buffer\n",
    "# ppo = PPO(obs_shape, action_space)\n",
    "# buffer = RolloutBuffer()\n",
    "\n",
    "# # Initialize environment\n",
    "# env = boxing_v2.env(render_mode=\"rgb_array\")\n",
    "# env.reset()\n",
    "\n",
    "# # Interaction loop\n",
    "# print(\"Starting environment interaction...\")\n",
    "# for step in range(num_steps):\n",
    "#     for agent_name in env.agent_iter():\n",
    "#         obs, reward, termination, truncation, info = env.last()\n",
    "#         if obs is not None:\n",
    "#             # Normalize observation and process it\n",
    "#             obs_tensor = torch.tensor(obs / 255.0, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # (C, H, W)\n",
    "#             action_probs = ppo.policy.forward_policy(obs_tensor)  # Get action probabilities\n",
    "#             action = torch.multinomial(action_probs, 1).item()  # Sample an action\n",
    "#             log_prob = torch.log(action_probs.squeeze(0)[action])  # Compute log-probability\n",
    "\n",
    "#             # Store step in the buffer\n",
    "#             buffer.store(obs, action, log_prob.item(), reward, termination or truncation)\n",
    "\n",
    "#             # Step in the environment\n",
    "#             env.step(action if not termination and not truncation else None)\n",
    "\n",
    "#         if termination or truncation:\n",
    "#             break  # Break the loop if the environment ends\n",
    "\n",
    "# # Compute returns and advantages\n",
    "# print(\"Computing returns and advantages...\")\n",
    "# buffer.compute_returns_and_advantages(ppo.policy, ppo.gamma, ppo.gae_lambda)\n",
    "\n",
    "# # Perform an update\n",
    "# print(\"Updating the PPO model...\")\n",
    "# ppo.update(buffer)\n",
    "\n",
    "# # Print buffer statistics\n",
    "# print(\"Buffer Summary:\")\n",
    "# print(f\"Observations: {len(buffer.observations)}\")\n",
    "# print(f\"Actions: {len(buffer.actions)}\")\n",
    "# print(f\"Log Probs: {len(buffer.log_probs)}\")\n",
    "# print(f\"Rewards: {len(buffer.rewards)}\")\n",
    "# print(f\"Returns: {len(buffer.returns)}\")\n",
    "# print(f\"Advantages: {len(buffer.advantages)}\")\n",
    "\n",
    "# # Clear the buffer for the next episode\n",
    "# buffer.clear()\n",
    "# print(\"Buffer cleared. Current size:\", len(buffer.observations))\n",
    "\n",
    "# # Test the forward pass after updates\n",
    "# dummy_obs = torch.randn(1, *obs_shape)  # Random observation\n",
    "# policy_output = ppo.policy.forward_policy(dummy_obs)\n",
    "# value_output = ppo.policy.forward_value(dummy_obs)\n",
    "\n",
    "# print(\"Policy Output Shape:\", policy_output.size())\n",
    "# print(\"Value Output Shape:\", value_output.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Wrapper for Pettingzoo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supersuit import pad_observations_v0, pad_action_space_v0, resize_v1, normalize_obs_v0, frame_stack_v1, dtype_v0\n",
    "from pettingzoo.utils import aec_to_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = boxing_v2.env()\n",
    "env = pad_observations_v0(env)\n",
    "env = pad_action_space_v0(env)\n",
    "env = resize_v1(env, 84, 84)  # Resize frames to 84x84\n",
    "env = dtype_v0(env, dtype=\"float32\")  # Convert observations to float32\n",
    "env = normalize_obs_v0(env, env_min=0, env_max=1)  # Normalize pixel values\n",
    "env = frame_stack_v1(env, 4)  # Stack 4 frames\n",
    "parallel_env = aec_to_parallel(env)  # Convert to parallel format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after conv layers: torch.Size([1, 64, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize PPO and RolloutBuffer\n",
    "obs_shape = (4, 84, 84)  # Stacked frames\n",
    "action_space = env.action_space(\"first_0\")  # Example action space for an agent\n",
    "ppo = PPO(obs_shape, action_space)\n",
    "buffer = RolloutBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: first_0, Raw Observation Shape: (84, 84, 12)\n",
      "Agent: first_0, Tensor Shape: torch.Size([1, 4, 84, 84])\n",
      "Agent: second_0, Raw Observation Shape: (84, 84, 12)\n",
      "Agent: second_0, Tensor Shape: torch.Size([1, 4, 84, 84])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected observation structure after environment step.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m     agent_observations \u001b[38;5;241m=\u001b[39m next_observations[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected observation structure after environment step.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Store data in the buffer for each agent\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent, obs \u001b[38;5;129;01min\u001b[39;00m agent_observations\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected observation structure after environment step."
     ]
    }
   ],
   "source": [
    "# Step 3: Training Loop\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1000  # Maximum steps to prevent infinite loops\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment\n",
    "    observations = parallel_env.reset()\n",
    "\n",
    "    # Check and extract nested observations (e.g., from Agent 0)\n",
    "    if isinstance(observations, tuple) and len(observations) > 0:\n",
    "        agent_observations = observations[0]  # Extract observations from the first tuple element\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected observation structure in parallel environment.\")\n",
    "\n",
    "    # Initialize done flags for each agent\n",
    "    done = {agent: False for agent in agent_observations.keys()}\n",
    "    step = 0\n",
    "\n",
    "    while not all(done.values()) and step < max_steps_per_episode:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "\n",
    "        # Process observations for each agent\n",
    "        for agent, obs in agent_observations.items():\n",
    "            # Print the shape for debugging\n",
    "            print(f\"Agent: {agent}, Raw Observation Shape: {obs.shape}\")\n",
    "\n",
    "            # Convert to tensor and permute dimensions to match PyTorch format\n",
    "            if obs.shape[-1] == 12:  # If the observation has 12 channels\n",
    "                obs = obs[..., :4]  # Select the first 4 channels to fix stacking issue\n",
    "\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # (C, H, W)\n",
    "            print(f\"Agent: {agent}, Tensor Shape: {obs_tensor.shape}\")\n",
    "\n",
    "            # Get action probabilities\n",
    "            action_probs = ppo.policy.forward_policy(obs_tensor)\n",
    "            action = torch.multinomial(action_probs, 1).item()  # Sample action\n",
    "            log_probs[agent] = torch.log(action_probs.squeeze(0)[action])  # Log probability\n",
    "            actions[agent] = action  # Store action for the agent\n",
    "\n",
    "        # Step the environment\n",
    "        step_output = parallel_env.step(actions)\n",
    "        if len(step_output) == 5:  # Handle truncations if included\n",
    "            next_observations, rewards, dones, truncations, infos = step_output\n",
    "            # Combine done and truncation flags\n",
    "            dones = {agent: dones[agent] or truncations[agent] for agent in dones}\n",
    "        else:\n",
    "            next_observations, rewards, dones, infos = step_output\n",
    "\n",
    "        # Extract next observations from Agent 0\n",
    "        if isinstance(next_observations, tuple) and len(next_observations) > 0:\n",
    "            agent_observations = next_observations[0]\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected observation structure after environment step.\")\n",
    "\n",
    "        # Store data in the buffer for each agent\n",
    "        for agent, obs in agent_observations.items():\n",
    "            buffer.store(obs, actions[agent], log_probs[agent].item(), rewards[agent], dones[agent])\n",
    "\n",
    "        # Update done flags\n",
    "        done = dones\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    # Compute Returns and Advantages\n",
    "    print(f\"Episode {episode + 1}: Computing returns and advantages...\")\n",
    "    buffer.compute_returns_and_advantages(ppo.policy, ppo.gamma, ppo.gae_lambda)\n",
    "\n",
    "    # Update PPO\n",
    "    print(f\"Episode {episode + 1}: Updating PPO model...\")\n",
    "    ppo.update(buffer)\n",
    "\n",
    "    # Clear buffer for the next episode\n",
    "    buffer.clear()\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug parallel environment\n",
    "# from pettingzoo.utils.conversions import aec_to_parallel\n",
    "\n",
    "# # Convert AEC environment to parallel format\n",
    "# parallel_env = aec_to_parallel(env)\n",
    "\n",
    "# # Verify the type\n",
    "# print(type(parallel_env))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset and inspect observations\n",
    "# observations = parallel_env.reset()\n",
    "# print(\"Observations type:\", type(observations))\n",
    "# print(\"Observations:\", observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for agent_idx, obs in enumerate(observations):\n",
    "#     print(f\"Agent {agent_idx}, Observation: {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### 4.Self-Play Setup -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PPO Training Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Advantage Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Logging and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_shape = (3, 210, 160)\n",
    "# action_space = type('', (), {'n': 6})()  # Mock action space with 6 actions\n",
    "\n",
    "# agent = PPOAgent(obs_shape, action_space)\n",
    "# dummy_obs = torch.randn(1, *obs_shape)  # Create a random input tensor\n",
    "# policy_output = agent.forward_policy(dummy_obs)\n",
    "# value_output = agent.forward_value(dummy_obs)\n",
    "\n",
    "# print(\"Policy Output Shape:\", policy_output.size())\n",
    "# print(\"Value Output Shape:\", value_output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS153",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
